{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pursuant-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-intent",
   "metadata": {},
   "source": [
    "### Reading the text files to generate a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "running-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.getcwd()+\"/dataset/WWW\")\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-injection",
   "metadata": {},
   "source": [
    "- Creating a dictionary to store all the values extracted from the text files\n",
    "- Each dictionary stores the key(document number) and text extracted from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "built-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = sorted(os.listdir(path+\"/doc\"))\n",
    "key_files = sorted(os.listdir(path+\"/keys\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "innocent-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the top 500 files\n",
    "text_files = text_files[:500]\n",
    "key_files = key_files[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-classics",
   "metadata": {},
   "source": [
    "### Extracting the texts from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chronic-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "boring-mercury",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : 10023569.txt\n",
      "Processing : 10030413.txt\n",
      "Processing : 10048650.txt\n",
      "Processing : 10057110.txt\n",
      "Processing : 10057112.txt\n",
      "Processing : 100585.txt\n",
      "Processing : 100717.txt\n",
      "Processing : 10071923.txt\n",
      "Processing : 10074391.txt\n",
      "Processing : 1008874.txt\n",
      "Processing : 10119.txt\n",
      "Processing : 101488.txt\n",
      "Processing : 10153001.txt\n",
      "Processing : 10172411.txt\n",
      "Processing : 10178588.txt\n",
      "Processing : 1021018.txt\n",
      "Processing : 10210475.txt\n",
      "Processing : 10239456.txt\n",
      "Processing : 1025710.txt\n",
      "Processing : 10260371.txt\n",
      "Processing : 102655.txt\n",
      "Processing : 10267504.txt\n",
      "Processing : 10272014.txt\n",
      "Processing : 10274995.txt\n",
      "Processing : 10278781.txt\n",
      "Processing : 1029161.txt\n",
      "Processing : 10323033.txt\n",
      "Processing : 1033807.txt\n",
      "Processing : 10348116.txt\n",
      "Processing : 10358404.txt\n",
      "Processing : 10365987.txt\n",
      "Processing : 10377441.txt\n",
      "Processing : 10378963.txt\n",
      "Processing : 10389546.txt\n",
      "Processing : 10414735.txt\n",
      "Processing : 10460295.txt\n",
      "Processing : 10460575.txt\n",
      "Processing : 10460577.txt\n",
      "Processing : 10461284.txt\n",
      "Processing : 10464296.txt\n",
      "Processing : 10477594.txt\n",
      "Processing : 104778.txt\n",
      "Processing : 1050874.txt\n",
      "Processing : 10514579.txt\n",
      "Processing : 10524156.txt\n",
      "Processing : 10525045.txt\n",
      "Processing : 10525233.txt\n",
      "Processing : 10563630.txt\n",
      "Processing : 10565631.txt\n",
      "Processing : 105768.txt\n",
      "Processing : 10581378.txt\n",
      "Processing : 10582583.txt\n",
      "Processing : 10582584.txt\n",
      "Processing : 10589971.txt\n",
      "Processing : 1059530.txt\n",
      "Processing : 10599039.txt\n",
      "Processing : 10603174.txt\n",
      "Processing : 10606830.txt\n",
      "Processing : 10631238.txt\n",
      "Processing : 10631834.txt\n",
      "Processing : 10633510.txt\n",
      "Processing : 10633621.txt\n",
      "Processing : 10643874.txt\n",
      "Processing : 10659466.txt\n",
      "Processing : 10754076.txt\n",
      "Processing : 1077174.txt\n",
      "Processing : 10772351.txt\n",
      "Processing : 10777275.txt\n",
      "Processing : 10795381.txt\n",
      "Processing : 10814222.txt\n",
      "Processing : 10844303.txt\n",
      "Processing : 1084549.txt\n",
      "Processing : 10901561.txt\n",
      "Processing : 10924972.txt\n",
      "Processing : 10946910.txt\n",
      "Processing : 10950133.txt\n",
      "Processing : 11006514.txt\n",
      "Processing : 11015761.txt\n",
      "Processing : 11026417.txt\n",
      "Processing : 11034828.txt\n",
      "Processing : 11052867.txt\n",
      "Processing : 1107868.txt\n",
      "Processing : 11081679.txt\n",
      "Processing : 11108002.txt\n",
      "Processing : 11134936.txt\n",
      "Processing : 11134938.txt\n",
      "Processing : 11204787.txt\n",
      "Processing : 11212270.txt\n",
      "Processing : 11227602.txt\n",
      "Processing : 11275612.txt\n",
      "Processing : 11275613.txt\n",
      "Processing : 11277694.txt\n",
      "Processing : 11338040.txt\n",
      "Processing : 11338703.txt\n",
      "Processing : 11394824.txt\n",
      "Processing : 11403481.txt\n",
      "Processing : 11418806.txt\n",
      "Processing : 1142421.txt\n",
      "Processing : 11485543.txt\n",
      "Processing : 11498123.txt\n",
      "Processing : 1150148.txt\n",
      "Processing : 11556066.txt\n",
      "Processing : 11570239.txt\n",
      "Processing : 11616857.txt\n",
      "Processing : 11619605.txt\n",
      "Processing : 11620085.txt\n",
      "Processing : 11624616.txt\n",
      "Processing : 11628441.txt\n",
      "Processing : 11631340.txt\n",
      "Processing : 11631791.txt\n",
      "Processing : 11633625.txt\n",
      "Processing : 11638887.txt\n",
      "Processing : 11639816.txt\n",
      "Processing : 11643476.txt\n",
      "Processing : 11657105.txt\n",
      "Processing : 11659004.txt\n",
      "Processing : 1167068.txt\n",
      "Processing : 1168781.txt\n",
      "Processing : 11701167.txt\n",
      "Processing : 11703421.txt\n",
      "Processing : 11708130.txt\n",
      "Processing : 11714006.txt\n",
      "Processing : 11749957.txt\n",
      "Processing : 11756683.txt\n",
      "Processing : 11759742.txt\n",
      "Processing : 11782911.txt\n",
      "Processing : 11785.txt\n",
      "Processing : 11788753.txt\n",
      "Processing : 11792499.txt\n",
      "Processing : 11796084.txt\n",
      "Processing : 11798933.txt\n",
      "Processing : 11806298.txt\n",
      "Processing : 11807460.txt\n",
      "Processing : 11811654.txt\n",
      "Processing : 11820735.txt\n",
      "Processing : 1182832.txt\n",
      "Processing : 11835370.txt\n",
      "Processing : 11836329.txt\n",
      "Processing : 11839803.txt\n",
      "Processing : 11840142.txt\n",
      "Processing : 11840967.txt\n",
      "Processing : 11841922.txt\n",
      "Processing : 11848994.txt\n",
      "Processing : 11850677.txt\n",
      "Processing : 11858468.txt\n",
      "Processing : 11859214.txt\n",
      "Processing : 11867634.txt\n",
      "Processing : 11876537.txt\n",
      "Processing : 11878695.txt\n",
      "Processing : 11881756.txt\n",
      "Processing : 11931844.txt\n",
      "Processing : 1194360.txt\n",
      "Processing : 1194809.txt\n",
      "Processing : 11981850.txt\n",
      "Processing : 12003689.txt\n",
      "Processing : 12008191.txt\n",
      "Processing : 12092784.txt\n",
      "Processing : 12102.txt\n",
      "Processing : 12127726.txt\n",
      "Processing : 12191750.txt\n",
      "Processing : 1225527.txt\n",
      "Processing : 1227358.txt\n",
      "Processing : 1229683.txt\n",
      "Processing : 1229684.txt\n",
      "Processing : 1247804.txt\n",
      "Processing : 1247888.txt\n",
      "Processing : 125048.txt\n",
      "Processing : 1267363.txt\n",
      "Processing : 1275176.txt\n",
      "Processing : 12995751.txt\n",
      "Processing : 13005831.txt\n",
      "Processing : 13009292.txt\n",
      "Processing : 13023730.txt\n",
      "Processing : 13058954.txt\n",
      "Processing : 13058990.txt\n",
      "Processing : 13059029.txt\n",
      "Processing : 13059042.txt\n",
      "Processing : 13059044.txt\n",
      "Processing : 13059085.txt\n",
      "Processing : 13059173.txt\n",
      "Processing : 13059270.txt\n",
      "Processing : 13059318.txt\n",
      "Processing : 13059382.txt\n",
      "Processing : 13059388.txt\n",
      "Processing : 13059401.txt\n",
      "Processing : 13059585.txt\n",
      "Processing : 13059663.txt\n",
      "Processing : 13059781.txt\n",
      "Processing : 13059898.txt\n",
      "Processing : 13059932.txt\n",
      "Processing : 13060051.txt\n",
      "Processing : 13060083.txt\n",
      "Processing : 13060159.txt\n",
      "Processing : 13060226.txt\n",
      "Processing : 13060384.txt\n",
      "Processing : 13060390.txt\n",
      "Processing : 13060588.txt\n",
      "Processing : 13060608.txt\n",
      "Processing : 13060664.txt\n",
      "Processing : 13060669.txt\n",
      "Processing : 13060679.txt\n",
      "Processing : 13060796.txt\n",
      "Processing : 13060825.txt\n",
      "Processing : 13060918.txt\n",
      "Processing : 13061041.txt\n",
      "Processing : 13061054.txt\n",
      "Processing : 13061186.txt\n",
      "Processing : 13061206.txt\n",
      "Processing : 13061384.txt\n",
      "Processing : 13061550.txt\n",
      "Processing : 13061552.txt\n",
      "Processing : 13061595.txt\n",
      "Processing : 13061601.txt\n",
      "Processing : 13061675.txt\n",
      "Processing : 13061698.txt\n",
      "Processing : 13061707.txt\n",
      "Processing : 13061722.txt\n",
      "Processing : 13061737.txt\n",
      "Processing : 13061804.txt\n",
      "Processing : 13061836.txt\n",
      "Processing : 13061839.txt\n",
      "Processing : 13061918.txt\n",
      "Processing : 13061924.txt\n",
      "Processing : 13061972.txt\n",
      "Processing : 13061989.txt\n",
      "Processing : 13062025.txt\n",
      "Processing : 13062026.txt\n",
      "Processing : 13062112.txt\n",
      "Processing : 13062121.txt\n",
      "Processing : 13062191.txt\n",
      "Processing : 13062261.txt\n",
      "Processing : 13062305.txt\n",
      "Processing : 13062337.txt\n",
      "Processing : 13062396.txt\n",
      "Processing : 13062406.txt\n",
      "Processing : 13062409.txt\n",
      "Processing : 13062436.txt\n",
      "Processing : 13062801.txt\n",
      "Processing : 13062845.txt\n",
      "Processing : 13062852.txt\n",
      "Processing : 13062911.txt\n",
      "Processing : 13062962.txt\n",
      "Processing : 13063216.txt\n",
      "Processing : 13063239.txt\n",
      "Processing : 13063418.txt\n",
      "Processing : 13063481.txt\n",
      "Processing : 13063484.txt\n",
      "Processing : 13063564.txt\n",
      "Processing : 13063596.txt\n",
      "Processing : 13063612.txt\n",
      "Processing : 13063679.txt\n",
      "Processing : 13063697.txt\n",
      "Processing : 13063699.txt\n",
      "Processing : 13063758.txt\n",
      "Processing : 13063830.txt\n",
      "Processing : 13063949.txt\n",
      "Processing : 13063954.txt\n",
      "Processing : 13064078.txt\n",
      "Processing : 13064160.txt\n",
      "Processing : 13064222.txt\n",
      "Processing : 13064271.txt\n",
      "Processing : 13064294.txt\n",
      "Processing : 13064296.txt\n",
      "Processing : 13064323.txt\n",
      "Processing : 13064431.txt\n",
      "Processing : 13064442.txt\n",
      "Processing : 13064535.txt\n",
      "Processing : 13064551.txt\n",
      "Processing : 13064602.txt\n",
      "Processing : 13064675.txt\n",
      "Processing : 13064722.txt\n",
      "Processing : 13064730.txt\n",
      "Processing : 13087531.txt\n",
      "Processing : 13089030.txt\n",
      "Processing : 13106832.txt\n",
      "Processing : 13109.txt\n",
      "Processing : 13152279.txt\n",
      "Processing : 13185533.txt\n",
      "Processing : 13190545.txt\n",
      "Processing : 13193305.txt\n",
      "Processing : 13240315.txt\n",
      "Processing : 13251515.txt\n",
      "Processing : 13386371.txt\n",
      "Processing : 13395437.txt\n",
      "Processing : 13403801.txt\n",
      "Processing : 13501869.txt\n",
      "Processing : 13502389.txt\n",
      "Processing : 13502390.txt\n",
      "Processing : 13502392.txt\n",
      "Processing : 13502500.txt\n",
      "Processing : 13502857.txt\n",
      "Processing : 13502912.txt\n",
      "Processing : 13503261.txt\n",
      "Processing : 13504156.txt\n",
      "Processing : 13505677.txt\n",
      "Processing : 13506151.txt\n",
      "Processing : 13507253.txt\n",
      "Processing : 13507543.txt\n",
      "Processing : 13507714.txt\n",
      "Processing : 13509697.txt\n",
      "Processing : 13510800.txt\n",
      "Processing : 13512950.txt\n",
      "Processing : 13513557.txt\n",
      "Processing : 13514543.txt\n",
      "Processing : 13514784.txt\n",
      "Processing : 13515810.txt\n",
      "Processing : 13516419.txt\n",
      "Processing : 13517074.txt\n",
      "Processing : 13517135.txt\n",
      "Processing : 13518145.txt\n",
      "Processing : 13518419.txt\n",
      "Processing : 13518634.txt\n",
      "Processing : 13518670.txt\n",
      "Processing : 13519640.txt\n",
      "Processing : 13520597.txt\n",
      "Processing : 13520879.txt\n",
      "Processing : 13521214.txt\n",
      "Processing : 13521570.txt\n",
      "Processing : 13522059.txt\n",
      "Processing : 13522731.txt\n",
      "Processing : 13523265.txt\n",
      "Processing : 13525236.txt\n",
      "Processing : 13526048.txt\n",
      "Processing : 13526508.txt\n",
      "Processing : 13526671.txt\n",
      "Processing : 13526814.txt\n",
      "Processing : 13526913.txt\n",
      "Processing : 13527964.txt\n",
      "Processing : 13528329.txt\n",
      "Processing : 13528523.txt\n",
      "Processing : 13528887.txt\n",
      "Processing : 13529859.txt\n",
      "Processing : 13530223.txt\n",
      "Processing : 13530633.txt\n",
      "Processing : 13530974.txt\n",
      "Processing : 13531284.txt\n",
      "Processing : 13532199.txt\n",
      "Processing : 13533362.txt\n",
      "Processing : 13533459.txt\n",
      "Processing : 13533717.txt\n",
      "Processing : 13533950.txt\n",
      "Processing : 13534577.txt\n",
      "Processing : 13534613.txt\n",
      "Processing : 13535450.txt\n",
      "Processing : 13536580.txt\n",
      "Processing : 13536686.txt\n",
      "Processing : 13536936.txt\n",
      "Processing : 13537427.txt\n",
      "Processing : 13537435.txt\n",
      "Processing : 13538141.txt\n",
      "Processing : 13538176.txt\n",
      "Processing : 13538488.txt\n",
      "Processing : 13539372.txt\n",
      "Processing : 13540032.txt\n",
      "Processing : 13540651.txt\n",
      "Processing : 13542825.txt\n",
      "Processing : 13543051.txt\n",
      "Processing : 13543053.txt\n",
      "Processing : 13543554.txt\n",
      "Processing : 13543822.txt\n",
      "Processing : 13544213.txt\n",
      "Processing : 13544630.txt\n",
      "Processing : 13545447.txt\n",
      "Processing : 13601457.txt\n",
      "Processing : 13621384.txt\n",
      "Processing : 13638975.txt\n",
      "Processing : 13714941.txt\n",
      "Processing : 13753027.txt\n",
      "Processing : 13756266.txt\n",
      "Processing : 13778833.txt\n",
      "Processing : 13785303.txt\n",
      "Processing : 13788626.txt\n",
      "Processing : 13788838.txt\n",
      "Processing : 13801324.txt\n",
      "Processing : 13819642.txt\n",
      "Processing : 13820155.txt\n",
      "Processing : 13833456.txt\n",
      "Processing : 13875907.txt\n",
      "Processing : 13896241.txt\n",
      "Processing : 13904998.txt\n",
      "Processing : 13982483.txt\n",
      "Processing : 14005107.txt\n",
      "Processing : 14005258.txt\n",
      "Processing : 14007948.txt\n",
      "Processing : 14026193.txt\n",
      "Processing : 14027370.txt\n",
      "Processing : 14045611.txt\n",
      "Processing : 14122983.txt\n",
      "Processing : 14123171.txt\n",
      "Processing : 14128558.txt\n",
      "Processing : 14146914.txt\n",
      "Processing : 14149355.txt\n",
      "Processing : 14169498.txt\n",
      "Processing : 14173486.txt\n",
      "Processing : 14185350.txt\n",
      "Processing : 14186402.txt\n",
      "Processing : 14200731.txt\n",
      "Processing : 14209920.txt\n",
      "Processing : 14237951.txt\n",
      "Processing : 14249.txt\n",
      "Processing : 14255423.txt\n",
      "Processing : 14265822.txt\n",
      "Processing : 14316716.txt\n",
      "Processing : 14316718.txt\n",
      "Processing : 14322308.txt\n",
      "Processing : 14347434.txt\n",
      "Processing : 14348154.txt\n",
      "Processing : 14353464.txt\n",
      "Processing : 14371347.txt\n",
      "Processing : 14372945.txt\n",
      "Processing : 14373644.txt\n",
      "Processing : 14373837.txt\n",
      "Processing : 14374135.txt\n",
      "Processing : 14375309.txt\n",
      "Processing : 14375496.txt\n",
      "Processing : 14375712.txt\n",
      "Processing : 14375718.txt\n",
      "Processing : 14376091.txt\n",
      "Processing : 14376543.txt\n",
      "Processing : 14377120.txt\n",
      "Processing : 14377272.txt\n",
      "Processing : 14377562.txt\n",
      "Processing : 14380234.txt\n",
      "Processing : 14380564.txt\n",
      "Processing : 14382235.txt\n",
      "Processing : 14383928.txt\n",
      "Processing : 14385094.txt\n",
      "Processing : 14386517.txt\n",
      "Processing : 14386992.txt\n",
      "Processing : 14387519.txt\n",
      "Processing : 14387989.txt\n",
      "Processing : 14388642.txt\n",
      "Processing : 14388703.txt\n",
      "Processing : 14388913.txt\n",
      "Processing : 14388926.txt\n",
      "Processing : 14389008.txt\n",
      "Processing : 14389159.txt\n",
      "Processing : 14389185.txt\n",
      "Processing : 14389220.txt\n",
      "Processing : 14389382.txt\n",
      "Processing : 14392043.txt\n",
      "Processing : 14392151.txt\n",
      "Processing : 14394291.txt\n",
      "Processing : 14394643.txt\n",
      "Processing : 14395405.txt\n",
      "Processing : 14395695.txt\n",
      "Processing : 14395704.txt\n",
      "Processing : 14396486.txt\n",
      "Processing : 14397937.txt\n",
      "Processing : 14426843.txt\n",
      "Processing : 14435710.txt\n",
      "Processing : 14445422.txt\n",
      "Processing : 14445546.txt\n",
      "Processing : 14449751.txt\n",
      "Processing : 14453157.txt\n",
      "Processing : 14453752.txt\n",
      "Processing : 14454508.txt\n",
      "Processing : 14475693.txt\n",
      "Processing : 1454933.txt\n",
      "Processing : 1605069.txt\n",
      "Processing : 163229.txt\n",
      "Processing : 16334.txt\n",
      "Processing : 16381.txt\n",
      "Processing : 16387.txt\n",
      "Processing : 1659738.txt\n",
      "Processing : 16706.txt\n",
      "Processing : 173979.txt\n",
      "Processing : 178204.txt\n",
      "Processing : 178963.txt\n",
      "Processing : 1810492.txt\n",
      "Processing : 183.txt\n",
      "Processing : 1833006.txt\n",
      "Processing : 1833066.txt\n",
      "Processing : 1839820.txt\n",
      "Processing : 1840610.txt\n",
      "Processing : 1845167.txt\n",
      "Processing : 1848435.txt\n",
      "Processing : 1880971.txt\n",
      "Processing : 1888132.txt\n",
      "Processing : 204579.txt\n",
      "Processing : 204586.txt\n",
      "Processing : 21580.txt\n",
      "Processing : 21587.txt\n",
      "Processing : 217637.txt\n",
      "Processing : 230498.txt\n",
      "Processing : 238458.txt\n",
      "Processing : 239996.txt\n",
      "Processing : 246813.txt\n",
      "Processing : 247234.txt\n",
      "Processing : 251906.txt\n",
      "Processing : 251909.txt\n",
      "Processing : 254542.txt\n",
      "Processing : 254899.txt\n",
      "Processing : 259989.txt\n",
      "Processing : 259993.txt\n",
      "Processing : 275063.txt\n",
      "Processing : 278400.txt\n",
      "Processing : 281466.txt\n",
      "Processing : 281471.txt\n",
      "Processing : 285729.txt\n"
     ]
    }
   ],
   "source": [
    "for file in text_files:\n",
    "    print(f'Processing : {file}')\n",
    "    f = open(path+'/doc/'+file)\n",
    "    data = f.read()\n",
    "    data = data.replace('\\n',' ')\n",
    "    file = int(file.split('.')[0])\n",
    "    text_data[file] = data\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fourth-boards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10023569: 'Autonomous resource provisioning for multi-service web applications Dynamic resource provisioning aims at maintaining the end-to-end response time of a web application within a pre-defined SLA . Although the topic has been well studied for monolithic applications , provisioning resources for applications composed of multiple services remains a challenge . When the SLA is violated , one must decide which service ( s ) should be reprovisioned for optimal effect . We propose to assign an SLA only to the front-end service . Other services are not given any particular response time objectives . Services are autonomously responsible for their own provisioning operations and collaboratively negotiate performance objectives with each other to decide the provisioning service ( s ) . We demonstrate through extensive experiments that our system can add\\\\/remove\\\\/shift both servers and caches within an entire multi-service application under varying workloads to meet the SLA target and improve resource utilization . ',\n",
       " 10030413: \"Collaborative filtering for orkut communities : discovery of user latent behavior Users of social networking services can connect with each other by forming communities for online interaction . Yet as the number of communities hosted by such websites grows over time , users have even greater need for effective community recommendations in order to meet more users . In this paper , we investigate two algorithms from very different domains and evaluate their effectiveness for personalized community recommendation . First is association rule mining ( ARM ) , which discovers associations between sets of communities that are shared across many users . Second is latent Dirichlet allocation ( LDA ) , which models user-community co-occurrences using latent aspects . In comparing LDA with ARM , we are interested in discovering whether modeling low-rank latent structure is more effective for recommendations than directly mining rules from the observed data . We experiment on an Orkut data set consisting of 492,104 users and 118,002 communities . Our empirical comparisons using the top-k recommendations metric show that LDA performs consistently better than ARM for the community recommendation task when recommending a list of 4 or more communities . However , for recommendation lists of up to 3 communities , ARM is still a bit better . We analyze examples of the latent information learned by LDA to explain this finding . To efficiently handle the large-scale data set , we parallelize LDA on distributed computers and demonstrate our parallel implementation 's scalability with varying numbers of machines . \",\n",
       " 10048650: 'A trust management framework for service-oriented environments Many reputation management systems have been developed under the assumption that each entity in the system will use a variant of the same scoring function . Much of the previous work in reputation management has focused on providing robustness and improving performance for a given reputation scheme . In this paper , we present a reputation-based trust management framework that supports the synthesis of trust-related feedback from many different entities while also providing each entity with the flexibility to apply different scoring functions over the same feedback data for customized trust evaluations . We also propose a novel scheme to cache trust values based on recent client activity . To evaluate our approach , we implemented our trust management service and tested it on a realistic application scenario in both LAN and WAN distributed environments . Our results indicate that our trust management service can effectively support multiple scoring functions with low overhead and high availability . ',\n",
       " 10057110: \"A client-server architecture for state-dependent dynamic visualizations on the web As sophisticated enterprise applications move to the Web , some advanced user experiences become difficult to migrate due to prohibitively high computation , memory , and bandwidth requirements . State-dependent visualizations of large-scale data sets are particularly difficult since a change in the client 's context necessitates a change in the displayed results . This paper describes a Web architecture where clients are served a session-specific image of the data , with this image divided into tiles dynamically generated by the server . This set of tiles is supplemented with a corpus of metadata describing the immediate vicinity of interest ; additional metadata is delivered as needed in a progressive fashion in support and anticipation of the user 's actions . We discuss how the design of this architecture was motivated by the goal of delivering a highly responsive user experience . As an example of a complete application built upon this architecture , we present OrgMaps , an interactive system for navigating hierarchical data , enabling fluid , low-latency navigation of trees of hundreds of thousands of nodes on standard Web browsers using only HTML and JavaScript . \",\n",
       " 10057112: 'Rapid development of spreadsheet-based web mashups The rapid growth of social networking sites and web communities have motivated web sites to expose their APIs to external developers who create mashups by assembling existing functionalities . Current APIs , however , aim toward developers with programming expertise ; they are not directly usable by wider class of users who do not have programming background , but would nevertheless like to build their own mashups . To address this need , we propose a spreadsheet-based Web mashups development framework , which enables users to develop mashups in the popular spreadsheet environment . First , we provide a mechanism that makes structured data first class values of spreadsheet cells . Second , we propose a new component model that can be used to develop fairly sophisticated mashups , involving joining data sources and keeping spreadsheet data up to date . Third , to simplify mashup development , we provide a collection of spreadsheet-based mashup patterns that captures common Web data access and spreadsheet presentation functionalities . Users can reuse and customize these patterns to build spreadsheet-based Web mashups instead of developing them from scratch . Fourth , we enable users to manipulate structured data presented on spreadsheet in a drag-and-drop fashion . Finally , we have developed and tested a proof-of-concept prototype to demonstrate the utility of the proposed framework . ',\n",
       " 100585: \"Information diffusion through blogspace We study the dynamics of information propagation in environments of low-overhead personal publishing , using a large collection of weblogs over time as our example domain . We characterize and model this collection at two levels . First , we present a macroscopic characterization of topic propagation through our corpus , formalizing the notion of long-running `` chatter '' topics consisting recursively of `` spike '' topics generated by outside world events , or more rarely , by resonances within the community . Second , we present a microscopic characterization of propagation from individual to individual , drawing on the theory of infectious diseases to model the flow . We propose , validate , and employ an algorithm to induce the underlying propagation network from a sequence of posts , and report on the results . \",\n",
       " 100717: 'BizCQ : using continual queries to cope with changes in business information exchange In this poster , we propose the framework of BizCQ , a system to apply Continual Queries ( 7 ) ( 8 ) on Web-based content to manage information exchanges between two business partners . In this poster , we describe ways to leverage previous research in Web monitoring techniques applied to the everyday problem of managing change within a business environment , and focus on the difficulties of managing changes that are caused by external parties in business-to-business ( B2B ) information exchanges . ',\n",
       " 10071923: \"Combining global optimization with local selection for efficient QoS-aware service composition The run-time binding of web services has been recently put forward in order to support rapid and dynamic web service compositions . With the growing number of alternative web services that provide the same functionality but differ in quality parameters , the service composition becomes a decision problem on which component services should be selected such that user 's end-to-end QoS requirements ( e.g. availability , response time ) and preferences ( e.g. price ) are satisfied . Although very efficient , local selection strategy fails short in handling global QoS requirements . Solutions based on global optimization , on the other hand , can handle global constraints , but their poor performance renders them inappropriate for applications with dynamic and real-time requirements . In this paper we address this problem and propose a solution that combines global optimization with local selection techniques to benefit from the advantages of both worlds . The proposed solution consists of two steps : first , we use mixed integer programming ( MIP ) to find the optimal decomposition of global QoS constraints into local constraints . Second , we use distributed local selection to find the best web services that satisfy these local constraints . The results of experimental evaluation indicate that our approach significantly outperforms existing solutions in terms of computation time while achieving close-to-optimal results . \",\n",
       " 10074391: 'Advertising keyword generation using active learning This paper proposes an efficient relevance feedback based interactive model for keyword generation in sponsored search advertising . We formulate the ranking of relevant terms as a supervised learning problem and suggest new terms for the seed by leveraging user relevance feedback information . Active learning is employed to select the most informative samples from a set of candidate terms for user labeling . Experiments show our approach improves the relevance of generated terms significantly with little user effort required . ',\n",
       " 1008874: 'Small world peer networks in distributed web search In ongoing research , a collaborative peer network application is being proposed to address the scalability limitations of centralized search engines . Here we introduce a local adaptive routing algorithm used to dynamically change the topology of the peer network based on a simple learning scheme driven by query response interactions among neighbors . We test the algorithm via simulations with 70 model users based on actual Web crawls . We find that the network topology rapidly converges from a random network to a small world network , with emerging clusters that match the user communities with shared interests . ',\n",
       " 10119: 'Simulation , verification and automated composition of web services Web services -- Web-accessible programs and devices - are a key application area for the Semantic Web . With the proliferation of Web services and the evolution towards the Semantic Web comes the opportunity to automate various Web services tasks . Our objective is to enable markup and automated reasoning technology to describe , simulate , compose , test , and verify compositions of Web services . We take as our starting point the DAML-S DAML+OIL ontology for describing the capabilities of Web services . We define the semantics for a relevant subset of DAML-S in terms of a first-order logical language . With the semantics in hand , we encode our service descriptions in a Petri Net formalism and provide decision procedures for Web service simulation , verification and composition . We also provide an analysis of the complexity of these tasks under different restrictions to the DAML-S composite services we can describe . Finally , we present an implementation of our analysis techniques . This implementation takes as input a DAML-S description of a Web service , automatically generates a Petri Net and performs the desired analysis . Such a tool has broad applicability both as a back end to existing manual Web service composition tools , and as a stand-alone tool for Web service developers . ',\n",
       " 101488: 'The interoperability of learning object repositories and services : standards , implementations and lessons learned Interoperability is one of the main issues in creating a networked system of repositories . The eduSource project in its holisticapproach to building a network of learning object repositories in Canada is implementing an open network for learning services . Itsopenness is supported by a communication protocol called theeduSource Communications Layer ( ECL ) which closely implements the IMS Digital Repository Interoperability ( DRI ) specification and architecture . The ECL in conjunction withconnection middleware enables any service providers to join thenetwork . EduSource is open to external initiatives as it explicitlysupports an extensible bridging mechanism between eduSource and other major initiatives . This paper discusses interoperability in general and then focuses on the design of ECL as animplementation of IMS DRI with supporting infrastructure andmiddleware . The eduSource implementation is in the mature stateof its development as being deployed in different settings withdifferent partners . Two applications used in evaluating ourapproach are described : a gateway for connecting betweeneduSource and the NSDL initiative , and a federated searchconnecting eduSource , EdNA and SMETE . ',\n",
       " 10153001: 'Social recommender systems The goal of this tutorial is to expose participants to the current research on social recommender systems ( i.e. , recommender systems for the social web ) . Participants will become familiar with state-of-the-art recommendation methods , their classifications according to various criteria , common evaluation methodologies , and potential applications that can utilize social recommender systems . Additionally , open issues and challenges in the field will be discussed . ',\n",
       " 10172411: \"A search-based method for forecasting ad impression in contextual advertising Contextual advertising ( also called content match ) refers to the placement of small textual ads within the content of a generic web page . It has become a significant source of revenue for publishers ranging from individual bloggers to major newspapers . At the same time it is an important way for advertisers to reach their intended audience . This reach depends on the total number of exposures of the ad ( impressions ) and its click-through-rate ( CTR ) that can be viewed as the probability of an end-user clicking on the ad when shown . These two orthogonal , critical factors are both difficult to estimate and even individually can still be very informative and useful in planning and budgeting advertising campaigns . In this paper , we address the problem of forecasting the number of impressions for new or changed ads in the system . Producing such forecasts , even within large margins of error , is quite challenging : 1 ) ad selection in contextual advertising is a complicated process based on tens or even hundreds of page and ad features ; 2 ) the publishers ' content and traffic vary over time ; and 3 ) the scale of the problem is daunting : over a course of a week it involves billions of impressions , hundreds of millions of distinct pages , hundreds of millions of ads , and varying bids of other competing advertisers . We tackle these complexities by simulating the presence of a given ad with its associated bid over weeks of historical data . We obtain an impression estimate by counting how many times the ad would have been displayed if it were in the system over that period of time . We estimate this count by an efficient two-level search algorithm over the distinct pages in the data set . Experimental results show that our approach can accurately forecast the expected number of impressions of contextual ads in real time . We also show how this method can be used in tools for bid selection and ad evaluation . \",\n",
       " 10178588: \"Latent space domain transfer between high dimensional overlapping distributions Transferring knowledge from one domain to another is challenging due to a number of reasons . Since both conditional and marginal distribution of the training data and test data are non-identical , model trained in one domain , when directly applied to a different domain , is usually low in accuracy . For many applications with large feature sets , such as text document , sequence data , medical data , image data of different resolutions , etc. two domains usually do not contain exactly the same features , thus introducing large numbers of `` missing values '' when considered over the union of features from both domains . In other words , its marginal distributions are at most overlapping . In the same time , these problems are usually high dimensional , such as , several thousands of features . Thus , the combination of high dimensionality and missing values make the relationship in conditional probabilities between two domains hard to measure and model . To address these challenges , we propose a framework that first brings the marginal distributions of two domains closer by `` filling up '' those missing values of disjoint features . Afterwards , it looks for those comparable sub-structures in the `` latent-space '' as mapped from the expanded feature vector , where both marginal and conditional distribution are similar . With these sub-structures in latent space , the proposed approach then find common concepts that are transferable across domains with high probability . During prediction , unlabeled instances are treated as `` queries '' , the mostly related labeled instances from out-domain are retrieved , and the classification is made by weighted voting using retrieved out-domain examples . We formally show that importing feature values across domains and latent semantic index can jointly make the distributions of two related domains easier to measure than in original feature space , the nearest neighbor method employed to retrieve related out domain examples is bounded in error when predicting in-domain examples . Software and datasets are available for download . \",\n",
       " 1021018: \"Investigating behavioral variability in web search Understanding the extent to which people 's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively . In this paper we describe a longitudinal log-based study that investigated variability in people . s interaction behavior when engaged in search-related activities on the Web . allWe analyze the search interactions of more than two thousand volunteer users over a five-month period , with the aim of characterizing differences in their interaction styles . allThe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users , and within and between the search queries they submit . allOur findings also suggest two classes of extreme user . navigators and explorers . whose search interaction is highly consistent or highly variable . Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone . \",\n",
       " 10210475: 'Efficient application placement in a dynamic hosting platform Web hosting providers are increasingly looking into dynamic hosting to reduce costs and improve the performance of their platforms . Instead of provisioning fixed resources to each customer , dynamic hosting maintains a variable number of application instances to satisfy current demand . While existing research in this area has mostly focused on the algorithms that decide on the number and location of application instances , we address the problem of efficient enactment of these decisions once they are made . We propose a new approach to application placement and experimentally show that it dramatically reduces the cost of application placement , which in turn improves the end-to-end agility of the hosting platform in reacting to demand changes . ',\n",
       " 10239456: 'Query-driven indexing for peer-to-peer text retrieval We describe a query-driven indexing framework for scalable text retrieval over structured P2P networks . To cope with the bandwidth consumption problem that has been identified as the major obstacle for full-text retrieval in P2P networks , we truncate posting lists associated with indexing features to a constant size storing only top-k ranked document references . To compensate for the loss of information caused by the truncation , we extend the set of indexing features with carefully chosen term sets . Indexing term sets are selected based on the query statistics extracted from query logs , thus we index only such combinations that are a ) frequently present in user queries and b ) non-redundant w.r.t the rest of the index . The distributed index is compact and efficient as it constantly evolves adapting to the current query popularity distribution . Moreover , it is possible to control the tradeoff between the storage\\\\/bandwidth requirements and the quality of query answering by tuning the indexing parameters . Our theoretical analysis and experimental results indicate that we can indeed achieve scalable P2P text retrieval for very large document collections and deliver good retrieval performance . ',\n",
       " 1025710: 'Making RDF presentable : integrated global and local semantic Web browsing This paper discusses generating document structure from annotated media repositories in a domain-independent manner . This approaches the vision of a universal RDF browser . We start by applying the search-and-browse paradigm established for the WWW to RDF presentation . Furthermore , this paper adds to this paradigm the clustering-based derivation of document structure from search returns , providing simple but domain-independent hypermedia generation from RDF stores . While such generated presentations hardly meet the standards of those written by humans , they provide quick access to media repositories when the required document has not yet been written . The resulting system allows a user to specify a topic for which it generates a hypermedia document providing guided navigation through virtually any RDF repository . The impact for content providers is that as soon as one adds new media items and their annotations to a repository , they become immediately available for automatic integration into subsequently requested presentations . ',\n",
       " 10260371: 'Network-aware forward caching This paper proposes and evaluates a Network Aware Forward Caching approach for determining the optimal deployment strategy of forward caches to a network . A key advantage of this approach is that we can reduce the network costs associated with forward caching to maximize the benefit obtained from their deployment . We show in our simulation that a 37 % increase to net benefits could be achieved over the standard method of full cache deployment to cache all POPs traffic . In addition , we show that this maximal point occurs when only 68 % of the total traffic is cached . Another contribution of this paper is the analysis we use to motivate and evaluate this problem . We characterize the Internet traffic of 100K subscribers of a US residential broadband provider . We use both layer 4 and layer 7 analysis to investigate the traffic volumes of the flows as well as study the general characteristics of the applications used . We show that HTTP is a dominant protocol and account for 68 % of the total downstream traffic and that 34 % of that traffic is multimedia . In addition , we show that multimedia content using HTTP exhibits a 83 % annualized growth rate and other HTTP traffic has a 53 % growth rate versus the 26 % over all annual growth rate of broadband traffic . This shows that HTTP traffic will become ever more dominent and increase the potential caching opportunities . Furthermore , we characterize the core backbone traffic of this broadband provider to measure the distance traveled by content and traffic . We find that CDN traffic is much more efficient than P2P content and that there is large skew in the Air Miles between POP in a typical network . Our findings show that there are many opportunties in broadband provider networks to optimize how traffic is delivered and cached . ',\n",
       " 102655: \"Dynamic assembly of learning objects This paper describes one solution to the problem of how to select sequence , and link Web resources into a coherent , focused organization for instruction that addresses a user 's immediate and focused learning need . A system is described that automatically generates individualized learning paths from a repository of XML Web resources . Each Web resource has an XML Learning Object Metadata ( LOM ) description consisting of General , Educational , and Classification metadata . Dynamic assembly of these learning objects is based on the relative match of the learning object content and metadata to the learner 's needs , preferences , context , and constraints . Learning objects are connected into coherent paths based on their LOM topic classifications and the proximity of these topics in a Resource Description Framework ( RDF ) graph . An instructional sequencing policy specifies how to arrange the objects on the path into a particular learning sequence . The system has been deployed and evaluated within a corporate setting . \",\n",
       " 10267504: 'Learning consensus opinion : mining data from a labeling game We consider the problem of identifying the consensus ranking for the results of a query , given preferences among those results from a set of individual users . Once consensus rankings are identified for a set of queries , these rankings can serve for both evaluation and training of retrieval and learning systems . We present a novel approach to collecting the individual user preferences over image-search results : we use a collaborative game in which players are rewarded for agreeing on which image result is best for a query . Our approach is distinct from other labeling games because we are able to elicit directly the preferences of interest with respect to image queries extracted from query logs . As a source of relevance judgments , this data provides a useful complement to click data . Furthermore , the data is free of positional biases and is collected by the game without the risk of frustrating users with non-relevant results ; this risk is prevalent in standard mechanisms for debiasing clicks . We describe data collected over 34 days from a deployed version of this game that amounts to about 18 million expressed preferences between pairs . Finally , we present several approaches to modeling this data in order to extract the consensus rankings from the preferences and better sort the search results for targeted queries . ',\n",
       " 10272014: 'Incorporating site-level knowledge to extract structured data from web forums Web forums have become an important data resource for many web applications , but extracting structured data from unstructured web forum pages is still a challenging task due to both complex page layout designs and unrestricted user created posts . In this paper , we study the problem of structured data extraction from various web forum sites . Our target is to find a solution as general as possible to extract structured data , such as post title , post author , post time , and post content from any forum site . In contrast to most existing information extraction methods , which only leverage the knowledge inside an individual page , we incorporate both page-level and site-level knowledge and employ Markov logic networks ( MLNs ) to effectively integrate all useful evidence by learning their importance automatically . Site-level knowledge includes ( 1 ) the linkages among different object pages , such as list pages and post pages , and ( 2 ) the interrelationships of pages belonging to the same object . The experimental results on 20 forums show a very encouraging information extraction performance , and demonstrate the ability of the proposed approach on various forums . We also show that the performance is limited if only page-level knowledge is used , while when incorporating the site-level knowledge both precision and recall can be significantly improved . ',\n",
       " 10274995: \"Towards context-aware search by learning a very large variable length hidden markov model from search logs Capturing the context of a user 's query from the previous queries and clicks in the same session may help understand the user 's information need . A context-aware approach to document re-ranking , query suggestion , and URL recommendation may improve users ' search experience substantially . In this paper , we propose a general approach to context-aware search . To capture contexts of queries , we learn a variable length Hidden Markov Model ( vlHMM ) from search sessions extracted from log data . Although the mathematical model is intuitive , how to learn a large vlHMM with millions of states from hundreds of millions of search sessions poses a grand challenge . We develop a strategy for parameter initialization in vlHMM learning which can greatly reduce the number of parameters to be estimated in practice . We also devise a method for distributed vlHMM learning under the map-reduce model . We test our approach on a real data set consisting of 1.8 billion queries , 2.6 billion clicks , and 840 million search sessions , and evaluate the effectiveness of the vlHMM learned from the real data on three search applications : document re-ranking , query suggestion , and URL recommendation . The experimental results show that our approach is both effective and efficient . \",\n",
       " 10278781: 'Object views : fine-grained sharing in browsers Browsers do not currently support the secure sharing of JavaScript objects between principals . We present this problem as the need for object views , which are consistent and controllable versions of objects . Multiple views can be made for the same object and customized for the recipients . We implement object views with a JavaScript library that wraps shared objects and interposes on all access attempts . The security challenge is to fully mediate access to objects shared through a view and prevent privilege escalation . We discuss how object views can be deployed in two settings : same-origin sharing with rewriting-based JavaScript isolation systems like Google Caja , and inter-origin sharing between browser frames over a message-passing channel . To facilitate simple document sharing , we build a policy system for declaratively defining policies for document object views . Notably , our document policy system makes it possible to hide elements without breaking document structure invariants . Developers can control the fine-grained behavior of object views with an aspect system that accepts programmatic policies . ',\n",
       " 1029161: \"Searching with numbers A large fraction of the useful web comprises of specification documents that largely consist of hattribute name , numeric valuei pairs embedded in text . Examples include product information , classified advertisements , resumes , etc. . The approach taken in the past to search these documents by first establishing correspondences between values and their names has achieved limited success because of the difficulty of extracting this information from free text . We propose a new approach that does not require this correspondence to be accurately established . Provided the data has `` low reflectivity '' , we can do effective search even if the values in the data have not been assigned attribute names and the user has omitted attribute names in the query . We give algorithms and indexing structures for implementing the search . We also show how hints ( i. e , imprecise , partial correspondences ) from automatic data extraction techniques can be incorporated into our approach for better accuracy on high reflectivity datasets . Finally , we validate our approach by showing that we get high precision in our answers on real datasets from a variety of domains . \",\n",
       " 10323033: 'Automatic web service composition with abstraction and refinement The behavioral description based Web Service Composition ( WSC ) problem aims at the automatic construction of a coordinator web service that controls a set of web services to reach a goal state . However , solving the WSC problem exactly with a realistic model is doubly-exponential in the number of variables in web service descriptions . In this paper , we propose a novel efficient approximation-based algorithm using automatic abstraction and refinement to dramatically reduce the number of variables needed to solve the problem . ',\n",
       " 1033807: 'A fault model and mutation testing of access control policies To increase confidence in the correctness of specified policies , policy developers can conduct policy testing by supplying typical test inputs ( requests ) and subsequently checking test outputs ( responses ) against expected ones . Unfortunately , manual testing is tedious and few tools exist for automated testing of access control policies . We present a fault model for access control policies and a framework to explore it . The framework includes mutation operators used to implement the fault model , mutant generation , equivalent-mutant detection , and mutant-killing determination . This framework allows us to investigate our fault model , evaluate coverage criteria for test generation and selection , and determine a relationship between structural coverage and fault-detection effectiveness . We have implemented the framework and applied it to various policies written in XACML . Our experimental results offer valuable insights into choosing mutation operators in mutation testing and choosing coverage criteria in test generation and selection . ',\n",
       " 10348116: 'Preserving XML queries during schema evolution In XML databases , new schema versions may be released as frequently as once every two weeks . This poster describes a taxonomy of changes for XML schema evolution . It examines the impact of those changes on schema validation and query evaluation . Based on that study , it proposes guidelines for XML schema evolution and for writing queries in such a way that they continue to operate as expected across evolving schemas . ',\n",
       " 10358404: 'Open user profiles for adaptive news systems : help or harm ? Over the last five years , a range of projects have focused on progressively more elaborated techniques for adaptive news delivery . However , the adaptation process in these systems has become more complicated and thus less transparent to the users . In this paper , we concentrate on the application of open user models in adding transparency and controllability to adaptive news systems . We present a personalized news system , YourNews , which allows users to view and edit their interest profiles , and report a user study on the system . Our results confirm that users prefer transparency and control in their systems , and generate more trust to such systems . However , similar to previous studies , our study demonstrate that this ability to edit user profiles may also harm the system . s performance and has to be used with caution . ',\n",
       " 10365987: 'Homepage live : automatic block tracing for web personalization The emergence of personalized homepage services , e.g. personalized Google Homepage and Microsoft Windows Live , has enabled Web users to select Web contents of interest and to aggregate them in a single Web page . The web contents are often predefined content blocks provided by the service providers . However , it involves intensive manual efforts to define the content blocks and maintain the information in it . In this paper , we propose a novel personalized homepage system , called . Homepage Live. , to allow end users to use drag-and-drop actions to collect their favorite Web content blocks from existing Web pages and organize them in a single page . Moreover , Homepage Live automatically traces the changes of blocks with the evolvement of the container pages by measuring the tree edit distance of the selected blocks . By exploiting the immutable elements of Web pages , the tracing algorithm performance is significantly improved . The experimental results demonstrate the effectiveness and efficiency of our algorithm . ',\n",
       " 10377441: 'SRing : a structured non dht p2p overlay supporting string range queries This paper presents SRing , a structured non DHT P2P overlay that efficiently supports exact and range queries on multiple attribute values . In SRing , all attribute values are interpreted as strings formed by a base alphabet and are published in the lexicographical order . Two virtual rings are built : N-ring is built in a skip-list way for range partition and queries ; D-ring is built in a small-world way for the construction of N-ring . A leave-and-join based load balancing method is used to balance range overload in the network with heterogeneous nodes . ',\n",
       " 10378963: 'Acquiring ontological knowledge from query logs We present a method for acquiring ontological knowledge using search query logs . We first use query logs to identify important contexts associated with terms belonging to a semantic category ; we then use these contexts to harvest new words belonging to this category . Our evaluation on selected categories indicates that the method works very well to help harvesting terms , achieving 85 % to 95 % accuracy in categorizing newly acquired terms . ',\n",
       " 10389546: 'Sync kit : a persistent client-side database caching toolkit for data intensive websites We introduce a client-server toolkit called Sync Kit that demonstrates how client-side database storage can improve the performance of data intensive websites . Sync Kit is designed to make use of the embedded relational database defined in the upcoming HTML5 standard to offload some data storage and processing from a web server onto the web browsers to which it serves content . Our toolkit provides various strategies for synchronizing relational database tables between the browser and the web server , along with a client-side template library so that portions web applications may be executed client-side . Unlike prior work in this area , Sync Kit persists both templates and data in the browser across web sessions , increasing the number of concurrent connections a server can handle by up to a factor of four versus that of a traditional server-only web stack and a factor of three versus a recent template caching approach . ',\n",
       " 10414735: 'Social search and discovery using a unified approach We explore new ways of improving a search engine using data from Web 2.0 applications such as blogs and social bookmarks . This data contains entities such as documents , people and tags , and relationships between them . We propose a simple yet effective method , based on faceted search , that treats all entities in a unified manner : returning all of them ( documents , people and tags ) on every search , and allowing all of them to be used as search terms . We describe an implementation of such a social search engine on the intranet of a large enterprise , and present large-scale experiments which verify the validity of our approach . ',\n",
       " 10460295: 'Where to adapt dynamic service compositions Peer services depend on one another to accomplish their tasks , and their structures may evolve . A service composition may be designed to replace its member services whenever the quality of the composite service fails to meet certain quality-of-service ( QoS ) requirements . Finding services and service invocation endpoints having the greatest impact on the quality are important to guide subsequent service adaptations . This paper proposes a technique that samples the QoS of composite services and continually analyzes them to identify artifacts for service adaptation . The preliminary results show that our technique has the potential to effectively find such artifacts in services . ',\n",
       " 10460575: \"Constructing travel itineraries from tagged geo-temporal breadcrumbs Vacation planning is a frequent laborious task which requires skilled interaction with a multitude of resources . This paper develops an end-to-end approach for constructing intra-city travel itineraries automatically by tapping a latent source reflecting geo-temporal breadcrumbs left by millions of tourists . In particular , the popular rich media sharing site , Flickr , allows photos to be stamped by the date and time of when they were taken , and be mapped to Points Of Interest ( POIs ) by latitude-longitude information as well as semantic metadata ( e.g. , tags ) that describe them . Our extensive user study on a `` crowd-sourcing '' marketplace ( Amazon Mechanical Turk ) , indicates that high quality itineraries can be automatically constructed from Flickr data , when compared against popular professionally generated bus tours . \",\n",
       " 10460577: \"Deducing trip related information from flickr Uploading tourist photos is a popular activity on photo sharing platforms . These photographs and their associated metadata ( tags , geo-tags , and temporal information ) should be useful for mining information about the sites visited . However , user-supplied metadata are often noisy and efficient filtering methods are needed before extracting useful knowledge . We focus here on exploiting temporal information , associated with tourist sites that appear in Flickr . From automatically filtered sets of geo-tagged photos , we deduce answers to questions like `` how long does it take to visit a tourist attraction ? '' or `` what can I visit in one day in this city ? '' Our method is evaluated and validated by comparing the automatically obtained visit duration times to manual estimations . \",\n",
       " 10461284: \"Visualizing differences in web search algorithms using the expected weighted hoeffding distance We introduce a new dissimilarity function for ranked lists , the expected weighted Hoeffding distance , that has several advantages over current dissimilarity measures for ranked search results . First , it is easily customized for users who pay varying degrees of attention to websites at different ranks . Second , unlike existing measures such as generalized Kendall 's tau , it is based on a true metric , preserving meaningful embeddings when visualization techniques like multi-dimensional scaling are applied . Third , our measure can effectively handle partial or missing rank information while retaining a probabilistic interpretation . Finally , the measure can be made computationally tractable and we give a highly efficient algorithm for computing it . We then apply our new metric with multi-dimensional scaling to visualize and explore relationships between the result sets from different search engines , showing how the weighted Hoeffding distance can distinguish important differences in search engine behavior that are not apparent with other rank-distance metrics . Such visualizations are highly effective at summarizing and analyzing insights on which search engines to use , what search strategies users can employ , and how search results evolve over time . We demonstrate our techniques using a collection of popular search engines , a representative set of queries , and frequently used query manipulation methods . \",\n",
       " 10464296: 'Dtwiki : a disconnection and intermittency tolerant wiki Wikis have proven to be a valuable tool for collaboration and content generation on the web . Simple semantics and ease-of-use make wiki systems well suited for meeting many emerging region needs in the areas of education , collaboration and local content generation . Despite their usefulness , current wiki software does not work well in the network environments found in emerging regions . For example , it is common to have long-lasting network partitions due to cost , power and poor connectivity . Network partitions make a traditional centralized wiki architecture unusable due to the unavailability of the central server . Existing solutions towards addressing connectivity problems include web-caching proxies and snapshot distribution . While proxies and snapshots allow wiki data to be read while disconnected , they prevent users from contributing updates back to the wiki . In this paper we detail the design and implementation of DTWiki , a wiki system which explicitly addresses the problem of operating a wiki system in an intermittent environment . The DTWiki system is able to cope with long-lasting partitions and bad connectivity while providing the functionality of popular wiki software such as MediaWiki and TWiki . ',\n",
       " 10477594: \"Externalities in online advertising Most models for online advertising assume that an advertiser 's value from winning an ad auction , which depends on the clickthrough rate or conversion rate of the advertisement , is independent of other advertisements served alongside it in the same session . This ignores an important ` externality effect ' : as the advertising audience has a limited attention span , a high-quality ad on a page can detract attention from other ads on the same page . That is , the utility to a winner in such an auction also depends on the set of other winners . In this paper , we introduce the problem of modeling externalities in online advertising , and study the winner determination problem in these models . Our models are based on choice models on the audience side . We show that in the most general case , the winner determination problem is hard even to approximate . However , we give an approximation algorithm for this problem with an approximation factor that is logarithmic in the ratio of the maximum to the minimum bid . Furthermore , we show that there are some interesting special cases , such as the case where the audience preferences are single peaked , where the problem can be solved exactly in polynomial time . For all these algorithms , we prove that the winner determination algorithm can be combined with VCG-style payments to yield truthful mechanisms . \",\n",
       " 104778: \"Impact of search engines on page popularity Recent studies show that a majority of Web page accesses are referred by search engines . In this paper we study the widespread use of Web search engines and its impact on the ecology of the Web . In particular , we study how much impact search engines have on the popularity evolution of Web pages . For example , given that search engines return currently popular '' pages at the top of search results , are we somehow penalizing newly created pages that are not very well known yet ? Are popular pages getting even more popular and new pages completely ignored ? We first show that this unfortunate trend indeed exists on the Web through an experimental study based on real Web data . We then analytically estimate how much longer it takes for a new page to attract a large number of Web users when search engines return only popular pages at the top of search results . Our result shows that search engines can have an immensely worrisome impact on the discovery of new Web pages . \",\n",
       " 1050874: 'A machine learning based approach for table detection on the web Table is a commonly used presentation scheme , especially for describing relational information . However , table understanding remains an open problem . In this paper , we consider the problem of table detection in web documents . Its potential applications include web mining , knowledge management , and web content summarization and delivery to narrow-bandwidth devices . We describe a machine learning based approach to classify each given table entity as either genuine or non-genuine . Various features reflecting the layout as well as content characteristics of tables are studied . In order to facilitate the training and evaluation of our table classifier , we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database . The database consists of 1,393 HTML files collected from hundreds of different web sites and contains 11,477 leaf TABLE elements , out of which 1,740 are genuine tables . Experiments were conducted using the cross validation method and an F-measure of 95.89 % was achieved . ',\n",
       " 10514579: 'Detecting Wikipedia vandalism with active learning and statistical language models This paper proposes an active learning approach using language model statistics to detect Wikipedia vandalism . Wikipedia is a popular and influential collaborative information system . The collaborative nature of authoring , as well as the high visibility of its content , have exposed Wikipedia articles to vandalism . Vandalism is defined as malicious editing intended to compromise the integrity of the content of articles . Extensive manual efforts are being made to combat vandalism and an automated approach to alleviate the laborious process is needed . This paper builds statistical language models , constructing distributions of words from the revision history of Wikipedia articles . As vandalism often involves the use of unexpected words to draw attention , the fitness ( or lack thereof ) of a new edit when compared with language models built from previous versions may well indicate that an edit is a vandalism instance . In addition , the paper adopts an active learning model to solve the problem of noisy and incomplete labeling of Wikipedia vandalism . The Wikipedia domain with its revision histories offers a novel context in which to explore the potential of language models in characterizing author intention . As the experimental results presented in the paper demonstrate , these models hold promise for vandalism detection . ',\n",
       " 10524156: 'sMash : semantic-based mashup navigation for data API network With the proliferation of data APIs , it is not uncommon that users who have no clear ideas about data APIs will encounter difficulties to build Mashups to satisfy their requirements . In this paper , we present a semantic-based mashup navigation system , sMash that makes mashup building easy by constructing and visualizing a real-life data API network . We build a sample network by gathering more than 300 popular APIs and find that the relationships between them are so complex that our system will play an important role in navigating users and give them inspiration to build interesting mashups easily . The system is accessible at : http:\\\\/\\\\/www.dart.zju.edu.cn\\\\/mashup . ',\n",
       " 10525045: 'Mashroom : end-user mashup programming using nested tables This paper presents an end-user-oriented programming environment called Mashroom . Major contributions herein include an end-user programming model with an expressive data structure as well as a set of formally-defined mashup operators . The data structure takes advantage of nested table , and maintains the intuitiveness while allowing users to express complex data objects . The mashup operators are visualized with contextual menu and formula bar and can be directly applied on the data . Experiments and case studies reveal that end users have little difficulty in effectively and efficiently using Mashroom to build mashup applications . ',\n",
       " 10525233: \"PAKE-based mutual HTTP authentication for preventing phishing attacks We developed a new Web authentication protocol with password-based mutual authentication which prevents various kinds of phishing attacks . This protocol provides a protection of user 's passwords against any phishers even if a dictionary attack is employed , and prevents phishers from imitating a false sense of successful authentication to users . The protocol is designed considering interoperability with many recent Web applications which requires many features which current HTTP authentication does not provide . The protocol is proposed as an Internet Draft submitted to IETF , and implemented in both server side ( as an Apache extension ) and client side ( as a Mozilla-based browser and an IE-based one ) . \",\n",
       " 10563630: 'Earthquake shakes Twitter users : real-time event detection by social sensors Twitter , a popular microblogging service , has received much attention recently . An important characteristic of Twitter is its real-time nature . For example , when an earthquake occurs , people make many Twitter posts ( tweets ) related to the earthquake , which enables detection of earthquake occurrence promptly , simply by observing the tweets . As described in this paper , we investigate the real-time interaction of events such as earthquakes in Twitter and propose an algorithm to monitor tweets and to detect a target event . To detect a target event , we devise a classifier of tweets based on features such as the keywords in a tweet , the number of words , and their context . Subsequently , we produce a probabilistic spatiotemporal model for the target event that can find the center and the trajectory of the event location . We consider each Twitter user as a sensor and apply Kalman filtering and particle filtering , which are widely used for location estimation in ubiquitous\\\\/pervasive computing . The particle filter works better than other comparable methods for estimating the centers of earthquakes and the trajectories of typhoons . As an application , we construct an earthquake reporting system in Japan . Because of the numerous earthquakes and the large number of Twitter users throughout the country , we can detect an earthquake with high probability ( 96 % of earthquakes of Japan Meteorological Agency ( JMA ) seismic intensity scale 3 or more are detected ) merely by monitoring tweets . Our system detects earthquakes promptly and sends e-mails to registered users . Notification is delivered much faster than the announcements that are broadcast by the JMA . ',\n",
       " 10565631: \"Mind the data skew : distributed inferencing by speeddating in elastic regions Semantic Web data exhibits very skewed frequency distributions among terms . Efficient large-scale distributed reasoning methods should maintain load-balance in the face of such highly skewed distribution of input data . We show that term-based partitioning , used by most distributed reasoning approaches , has limited scalability due to load-balancing problems . We address this problem with a method for data distribution based on clustering in elastic regions . Instead of as - signing data to fixed peers , data flows semi-randomly in the network . Data items `` speed-date '' while being temporarily collocated in the same peer . We introduce a bias in the routing to allow semantically clustered neighborhoods to emerge . Our approach is self-organising , efficient and does not require any central coordination . We have implemented this method on the MaRVIN platform and have performed experiments on large real-world datasets , using a cluster of up to 64 nodes . We compute the RDFS closure over different datasets and show that our clustering algorithm drastically reduces computation time , calculating the RDFS closure of 200 million triples in 7.2 minutes . \",\n",
       " 105768: 'Tv2web : generating and browsing web with multiple lod from video streams and their metadata We propose a method of automatically constructing Web content from video streams with metadata that we call TV2Web . The Web content includes thumbnails of video units and caption data generated from metadata . Users can watch TV ona normal Web browser . They can also manipulate Web content with zooming metaphors to seamlessly alter the level of detail ( LOD ) of the content being viewed . They can search for favorite scenes faster than with analog video equipment , and experience a new cross-media environment . We also developed a prototype of the TV2Web system and discuss its implementation . ',\n",
       " 10581378: \"Empirical comparison of algorithms for network community detection Detecting clusters or communities in large real-world graphs such as large social or information networks is a problem of considerable interest . In practice , one typically chooses an objective function that captures the intuition of a network cluster as set of nodes with better internal connectivity than external connectivity , and then one applies approximation algorithms or heuristics to extract sets of nodes that are related to the objective function and that `` look like '' good communities for the application of interest . In this paper , we explore a range of network community detection methods in order to compare them and to understand their relative performance and the systematic biases in the clusters they identify . We evaluate several common objective functions that are used to formalize the notion of a network community , and we examine several different classes of approximation algorithms that aim to optimize such objective functions . In addition , rather than simply fixing an objective and asking for an approximation to the best cluster of any size , we consider a size-resolved version of the optimization problem . Considering community quality as a function of its size provides a much finer lens with which to examine community detection algorithms , since objective functions and approximation algorithms often have non-obvious size-dependent behavior . \",\n",
       " 10582583: \"What is disputed on the web ? We present a method for automatically acquiring of a corpus of disputed claims from the web . We consider a factual claim to be disputed if a page on the web suggests both that the claim is false and also that other people say it is true . Our tool extracts disputed claims by searching the web for patterns such as `` falsely claimed that X '' and then using a statistical classifier to select text that appears to be making a disputed claim . We argue that such a corpus of disputed claims is useful for a wide range of applications related to information credibility on the web , and we report what our current corpus reveals about what is being disputed on the web . \",\n",
       " 10582584: 'Highlighting disputed claims on the web We describe Dispute Finder , a browser extension that alerts a user when information they read online is disputed by a source that they might trust . Dispute Finder examines the text on the page that the user is browsing and highlights any phrases that resemble known disputed claims . If a user clicks on a highlighted phrase then Dispute Finder shows them a list of articles that support other points of view . Dispute Finder builds a database of known disputed claims by crawling web sites that already maintain lists of disputed claims , and by allowing users to enter claims that they believe are disputed . Dispute Finder identifies snippets that make known disputed claims by running a simple textual entailment algorithm inside the browser extension , referring to a cached local copy of the claim database . In this paper , we explain the design of Dispute Finder , and the trade-offs between the various design decisions that we explored . ',\n",
       " 10589971: 'Towards robust trust establishment in web-based social networks with socialtrust We propose the SocialTrust framework for tamper-resilient trust establishment in online social networks . Two of the salient features of SocialTrust are its dynamic revision of trust by ( i ) distinguishing relationship quality from trust ; and ( ii ) incorporating a personalized feedback mechanism for adapting as the social network evolves . ',\n",
       " 1059530: \"Addressing the testing challenge with a web-based e-assessment system that tutors as it assesses Secondary teachers across the country are being asked to use formative assessment data to inform their classroom instruction . At the same time , critics of No Child Left Behind are calling the bill `` No Child Left Untestedö emphasizing the negative side of assessment , in that every hour spent assessing students is an hour lost from instruction . Or does it have to be ? What if we better integrated assessment into the classroom , and we allowed students to learn during the test ? Maybe we could even provide tutoring on the steps of solving problems . Our hypothesis is that we can achieve more accurate assessment by not only using data on whether students get test items right or wrong , but by also using data on the effort required for students to learn how to solve a test item . We provide evidence for this hypothesis using data collected with our E-ASSISTment system by more than 600 students over the course of the 2004-2005 school year . We also show that we can track student knowledge over time using modern longitudinal data analysis techniques . In a separate paper ( 9 ) , we report on the ASSISTment system 's architecture and scalability , while this paper is focused on how we can reliably assess student learning . \",\n",
       " 10599039: \"Collective privacy management in social networks Social Networking is one of the major technological phenomena of the Web 2.0 , with hundreds of millions of people participating . Social networks enable a form of self expression for users , and help them to socialize and share content with other users . In spite of the fact that content sharing represents one of the prominent features of existing Social Network sites , Social Networks yet do not support any mechanism for collaborative management of privacy settings for shared content . In this paper , we model the problem of collaborative enforcement of privacy policies on shared data by using game theory . In particular , we propose a solution that offers automated ways to share images based on an extended notion of content ownership . Building upon the Clarke-Tax mechanism , we describe a simple mechanism that promotes truthfulness , and that rewards users who promote co-ownership . We integrate our design with inference techniques that free the users from the burden of manually selecting privacy preferences for each picture . To the best of our knowledge this is the first time such a protection mechanism for Social Networking has been proposed . In the paper , we also show a proof-of-concept application , which we implemented in the context of Facebook , one of today 's most popular social networks . We show that supporting these type of solutions is not also feasible , but can be implemented through a minimal increase in overhead to end-users . \",\n",
       " 10603174: 'Parallel crawling for online social networks Given a huge online social network , how do we retrieve information from it through crawling ? Even better , how do we improve the crawling performance by using parallel crawlers that work independently ? In this paper , we present the framework of parallel crawlers for online social networks , utilizing a centralized queue . To show how this works in practice , we describe our implementation of the crawlers for an online auction website . The crawlers work independently , therefore the failing of one crawler does not affect the others at all . The framework ensures that no redundant crawling would occur . Using the crawlers that we built , we visited a total of approximately 11 million auction users , about 66,000 of which were completely crawled . ',\n",
       " 10606830: \"Privacy wizards for social networking sites Privacy is an enormous problem in online social networking sites . While sites such as Facebook allow users fine-grained control over who can see their profiles , it is difficult for average users to specify this kind of detailed policy . In this paper , we propose a template for the design of a social networking privacy wizard . The intuition for the design comes from the observation that real users conceive their privacy preferences ( which friends should be able to see which information ) based on an implicit set of rules . Thus , with a limited amount of user input , it is usually possible to build a machine learning model that concisely describes a particular user 's preferences , and then use this model to configure the user 's privacy settings automatically . As an instance of this general framework , we have built a wizard based on an active learning paradigm called uncertainty sampling . The wizard iteratively asks the user to assign privacy `` labels '' to selected ( `` informative '' ) friends , and it uses this input to construct a classifier , which can in turn be used to automatically assign privileges to the rest of the user 's ( unlabeled ) friends . To evaluate our approach , we collected detailed privacy preference data from 45 real Facebook users . Our study revealed two important things . First , real users tend to conceive their privacy preferences in terms of communities , which can easily be extracted from a social network graph using existing techniques . Second , our active learning wizard , using communities as features , is able to recommend high-accuracy privacy settings using less user input than existing policy-specification tools . \",\n",
       " 10631238: \"Detection and analysis of drive-by-download attacks and malicious JavaScript code JavaScript is a browser scripting language that allows developers to create sophisticated client-side interfaces for web applications . However , JavaScript code is also used to carry out attacks against the user 's browser and its extensions . These attacks usually result in the download of additional malware that takes complete control of the victim 's platform , and are , therefore , called `` drive-by downloads . '' Unfortunately , the dynamic nature of the JavaScript language and its tight integration with the browser make it difficult to detect and block malicious JavaScript code . This paper presents a novel approach to the detection and analysis of malicious JavaScript code . Our approach combines anomaly detection with emulation to automatically identify malicious JavaScript code and to support its analysis . We developed a system that uses a number of features and machine-learning techniques to establish the characteristics of normal JavaScript code . Then , during detection , the system is able to identify anomalous JavaScript code by emulating its behavior and comparing it to the established profiles . In addition to identifying malicious code , the system is able to support the analysis of obfuscated code and to generate detection signatures for signature-based systems . The system has been made publicly available and has been used by thousands of analysts . \",\n",
       " 10631834: 'Probabilistic question recommendation for question answering communities User-Interactive Question Answering ( QA ) communities such as Yahoo ! Answers are growing in popularity . However , as these QA sites always have thousands of new questions posted daily , it is difficult for users to find the questions that are of interest to them . Consequently , this may delay the answering of the new questions . This gives rise to question recommendation techniques that help users locate interesting questions . In this paper , we adopt the Probabilistic Latent Semantic Analysis ( PLSA ) model for question recommendation and propose a novel metric to evaluate the performance of our approach . The experimental results show our recommendation approach is effective . ',\n",
       " 10633510: 'Tag-oriented document summarization Social annotations on a Web document are highly generalized description of topics contained in that page . Their tagged frequency indicates the user attentions with various degrees . This makes annotations a good resource for summarizing multiple topics in a Web page . In this paper , we present a tag-oriented Web document summarization approach by using both document content and the tags annotated on that document . To improve summarization performance , a new tag ranking algorithm named EigenTag is proposed in this paper to reduce noise in tags . Meanwhile , association mining technique is employed to expand tag set to tackle the sparsity problem . Experimental results show our tag-oriented summarization has a significant improvement over those not using tags . ',\n",
       " 10633621: 'News article extraction with template-independent wrapper We consider the problem of template-independent news extraction . The state-of-the-art news extraction method is based on template-level wrapper induction , which has two serious limitations . 1 ) It can not correctly extract pages belonging to an unseen template until the wrapper for that template has been generated . 2 ) It is costly to maintain up-to-date wrappers for hundreds of websites , because any change of a template may lead to the invalidation of the corresponding wrapper . In this paper we formalize news extraction as a machine learning problem and learn a template-independent wrapper using a very small number of labeled news pages from a single site . Novel features dedicated to news titles and bodies are developed respectively . Correlations between the news title and the news body are exploited . Our template-independent wrapper can extract news pages from different sites regardless of templates . In experiments , a wrapper is learned from 40 pages from a single news site . It achieved 98.1 % accuracy over 3,973 news pages from 12 news sites . ',\n",
       " 10643874: 'Malicious interface design : exploiting the user In an ideal world , interface design is the art and science of helping users accomplish tasks in a timely , efficient , and pleasurable manner . This paper studies the inverse situation , the vast emergence of deliberately constructed malicious interfaces that violate design best practices in order to accomplish goals counter to those of the user . This has become a commonplace occurrence both on and off the desktop , particularly on the web . A primary objective of this paper is to formally define this problem , including construction of a taxonomy of malicious interface techniques and a preliminary analysis of their impact on users . Findings are presented that gauge the self-reported tolerance and expectation levels of users with regard to malicious interfaces as well as the effectiveness and ease of use of existing countermeasures . A second objective of this paper is to increase awareness , dialogue , and research in a domain that we consider largely unexplored but critical to future usability of the WWW . Our results were accomplished through significant compilation of malicious interface techniques based on review of thousands of web sites and by conducting three surveys . Ultimately , this paper concludes that malicious interfaces are a ubiquitous problem that demands intervention by the security and human computer interaction communities in order to reduce the negative impact on the global user population . ',\n",
       " 10659466: 'Large scale integration of senses for the semantic web Nowadays , the increasing amount of semantic data available on the Web leads to a new stage in the potential of Semantic Web applications . However , it also introduces new issues due to the heterogeneity of the available semantic resources . One of the most remarkable is redundancy , that is , the excess of different semantic descriptions , coming from different sources , to describe the same intended meaning . In this paper , we propose a technique to perform a large scale integration of senses ( expressed as ontology terms ) , in order to cluster the most similar ones , when indexing large amounts of online semantic information . It can dramatically reduce the redundancy problem on the current Semantic Web . In order to make this objective feasible , we have studied the adaptability and scalability of our previous work on sense integration , to be translated to the much larger scenario of the Semantic Web . Our evaluation shows a good behavior of these techniques when used in large scale experiments , then making feasible the proposed approach . ',\n",
       " 10754076: 'Context-aware citation recommendation When you write papers , how many times do you want to make some citations at a place but you are not sure which papers to cite ? Do you wish to have a recommendation system which can recommend a small number of good candidates for every place that you want to make some citations ? In this paper , we present our initiative of building a context-aware citation recommendation system . High quality citation recommendation is challenging : not only should the citations recommended be relevant to the paper under composition , but also should match the local contexts of the places citations are made . Moreover , it is far from trivial to model how the topic of the whole paper and the contexts of the citation places should affect the selection and ranking of citations . To tackle the problem , we develop a context-aware approach . The core idea is to design a novel non-parametric probabilistic model which can measure the context-based relevance between a citation context and a document . Our approach can recommend citations for a context effectively . Moreover , it can recommend a set of citations for a paper with high quality . We implement a prototype system in CiteSeerX . An extensive empirical evaluation in the CiteSeerX digital library against many baselines demonstrates the effectiveness and the scalability of our approach . ',\n",
       " 1077174: \"DEW : DNS-enhanced web for faster content delivery With a key component of latency on the Web being connection set up between clients and Web servers , several ways to avoid connections have been explored . While the work in recent years on Content Distribution Networks ( CDNs ) have moved some content ` closer ' to users at the cost of increasing DNS traffic , they have not fully exploited the available unused potential of existing protocols . We explore ways by which a variety of Web responses can be piggybacked on DNS messages . While we evaluated our idea in the Web context , the approach is generic and not restricted to Web responses . We propose an architecture for HTTP piggybacking in DNS messages and carry out a detailed performance analysis based on a trace-driven simulation study . Our architecture requires minimal extensions to existing protocols , utilizing only the allowed optional fields for these extensions . It is fully compatible and can coexist with the current Web . \",\n",
       " 10772351: \"Shout out : integrating news and reader comments A useful approach for enabling computers to automatically create new content is utilizing the text , media , and information already present on the World Wide Web . The newly created content is known as `` machine-generated content '' . For example , a machine-generated content system may create a multimedia news show with two animated anchors presenting a news story ; one anchor reads the news story with text taken from an existing news article , and the other anchor regularly interrupts with his or her own opinion about the story . In this paper , we present such a system , and describe in detail its strategy for autonomously extracting and selecting the opinions given by the second anchor . \",\n",
       " 10777275: \"Implementing the media fragments URI specification In this paper , we describe two examples of implementations of the Media Fragments URI specification which is currently being developed by the W3C Media Fragments Working Group . The group 's mission is to create standard addressing schemes for media fragments on the Web using Uniform Resource Identifiers ( URIs ) . We describe two scenarios to illustrate the implementations . More specifically , we show how User Agents ( UA ) will either be able to resolve media fragment URIs without help from the server , or will make use of a media fragments-aware server . Finally , we present some ongoing discussions and issues regarding the implementation of the Media Fragments specification . \",\n",
       " 10795381: \"Facetedpedia : dynamic generation of query-dependent faceted interfaces for wikipedia This paper proposes Facetedpedia , a faceted retrieval system for information discovery and exploration in Wikipedia . Given the set of Wikipedia articles resulting from a keyword query , Facetedpedia generates a faceted interface for navigating the result articles . Compared with other faceted retrieval systems , Facetedpedia is fully automatic and dynamic in both facet generation and hierarchy construction , and the facets are based on the rich semantic information from Wikipedia . The essence of our approach is to build upon the collaborative vocabulary in Wikipedia , more specifically the intensive internal structures ( hyperlinks ) and folksonomy ( category system ) . Given the sheer size and complexity of this corpus , the space of possible choices of faceted interfaces is prohibitively large . We propose metrics for ranking individual facet hierarchies by user 's navigational cost , and metrics for ranking interfaces ( each with k facets ) by both their average pairwise similarities and average navigational costs . We thus develop faceted interface discovery algorithms that optimize the ranking metrics . Our experimental evaluation and user study verify the effectiveness of the system . \",\n",
       " 10814222: 'Privacy in dynamic social networks Anonymization of social networks before they are published or shared has become an important research question . Recent work on anonymizing social networks has looked at privacy preserving techniques for publishing a single instance of the network . However , social networks evolve and a single instance is inadequate for analyzing the evolution of the social network or for performing any longitudinal data analysis . We study the problem of repeatedly publishing social network data as the network evolves , while preserving privacy of users . Publishing multiple instances of the same network independently has privacy risks , since stitching the information together may allow an adversary to identify users in the networks . We propose methods to anonymize a dynamic network such that the privacy of users is preserved when new nodes and edges are added to the published network . These methods make use of link prediction algorithms to model the evolution of the social network . Using this predicted graph to perform group-based anonymization , the loss in privacy caused by new edges can be reduced . We evaluate the privacy loss on publishing multiple social network instances using our methods . ',\n",
       " 10844303: 'Clustering query refinements by user intent We address the problem of clustering the refinements of a user search query . The clusters computed by our proposed algorithm can be used to improve the selection and placement of the query suggestions proposed by a search engine , and can also serve to summarize the different aspects of information relevant to the original user query . Our algorithm clusters refinements based on their likely underlying user intents by combining document click and session co-occurrence information . At its core , our algorithm operates by performing multiple random walks on a Markov graph that approximates user search behavior . A user study performed on top search engine queries shows that our clusters are rated better than corresponding clusters computed using approaches that use only document click or only sessions co-occurrence information . ',\n",
       " 1084549: \"P2Cast : peer-to-peer patching scheme for VoD service Providing video on demand ( VoD ) service over the Internet in a scalable way is a challenging problem . In this paper , we propose P2Cast - an architecture that uses a peer-to-peer approach to cooperatively stream video using patching techniques , while only relying on unicast connections among peers . We address the following two key technical issues in P2Cast : ( 1 ) constructing an application overlay appropriate for streaming ; and ( 2 ) providing continuous stream playback ( without glitches ) in the face of disruption from an early departing client . Our simulation experiments show that P2Cast can serve many more clients than traditional client-server unicast service , and that it generally out-performs multicast-based patching if clients can cache more than of a stream 's initial portion . We handle disruptions by delaying the start of playback and applying the shifted forwarding technique . A threshold on the length of time during which arriving clients are served in a single session in P2Cast serves as a knob to adjust the balance between the scalability and the clients ' viewing quality in P2Cast . \",\n",
       " 10901561: \"Sig . ma : live views on the web of data We demonstrate Sig . ma , both a service and an end user application to access the Web of Data as an integrated information space . Sig . ma uses an holistic approach in which large scale semantic web indexing , logic reasoning , data aggregation heuristics , ad hoc ontology consolidation , external services and responsive user interaction all play together to create rich entity descriptions . These consolidated entity descriptions then form the base for embeddable data mashups , machine oriented services as well as data browsing services . Finally , we discuss Sig . ma 's peculiar characteristics and report on lessions learned and ideas it inspires . \",\n",
       " 10924972: 'Factorizing personalized Markov chains for next-basket recommendation Recommender systems are an important component of many websites . Two of the most popular approaches are based on matrix factorization ( MF ) and Markov chains ( MC ) . MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences . On the other hand , MC methods model sequential behavior by learning a transition graph over items that is used to predict the next action based on the recent actions of a user . In this paper , we present a method bringing both approaches together . Our method is based on personalized transition graphs over underlying Markov chains . That means for each user an own transition matrix is learned - thus in total the method uses a transition cube . As the observations for estimating the transitions are usually very limited , our method factorizes the transition cube with a pairwise interaction model which is a special case of the Tucker Decomposition . We show that our factorized personalized MC ( FPMC ) model subsumes both a common Markov chain and the normal matrix factorization model . For learning the model parameters , we introduce an adaption of the Bayesian Personalized Ranking ( BPR ) framework for sequential basket data . Empirically , we show that our FPMC model outperforms both the common matrix factorization and the unpersonalized MC model both learned with and without factorization . ',\n",
       " 10946910: 'Exploring web scale language models for search query processing It has been widely observed that search queries are composed in a very different style from that of the body or the title of a document . Many techniques explicitly accounting for this language style discrepancy have shown promising results for information retrieval , yet a large scale analysis on the extent of the language differences has been lacking . In this paper , we present an extensive study on this issue by examining the language model properties of search queries and the three text streams associated with each web document : the body , the title , and the anchor text . Our information theoretical analysis shows that queries seem to be composed in a way most similar to how authors summarize documents in anchor texts or titles , offering a quantitative explanation to the observations in past work . We apply these web scale n-gram language models to three search query processing ( SQP ) tasks : query spelling correction , query bracketing and long query segmentation . By controlling the size and the order of different language models , we find that the perplexity metric to be a good accuracy indicator for these query processing tasks . We show that using smoothed language models yields significant accuracy gains for query bracketing for instance , compared to using web counts as in the literature . We also demonstrate that applying web-scale language models can have marked accuracy advantage over smaller ones . ',\n",
       " 10950133: \"Regular expressions considered harmful in client-side XSS filters Cross-site scripting flaws have now surpassed buffer overflows as the world 's most common publicly-reported security vulnerability . In recent years , browser vendors and researchers have tried to develop client-side filters to mitigate these attacks . We analyze the best existing filters and find them to be either unacceptably slow or easily circumvented . Worse , some of these filters could introduce vulnerabilities into sites that were previously bug-free . We propose a new filter design that achieves both high performance and high precision by blocking scripts after HTML parsing but before execution . Compared to previous approaches , our approach is faster , protects against more vulnerabilities , and is harder for attackers to abuse . We have contributed an implementation of our filter design to the WebKit open source rendering engine , and the filter is now enabled by default in the Google Chrome browser . \",\n",
       " 11006514: 'Scalable techniques for document identifier assignment in inverted indexes Web search engines depend on the full-text inverted index data structure . Because the query processing performance is so dependent on the size of the inverted index , a plethora of research has focused on fast end effective techniques for compressing this structure . Recently , several authors have proposed techniques for improving index compression by optimizing the assignment of document identifiers to the documents in the collection , leading to significant reduction in overall index size . In this paper , we propose improved techniques for document identifier assignment . Previous work includes simple and fast heuristics such as sorting by URL , as well as more involved approaches based on the Traveling Salesman Problem or on graph partitioning . These techniques achieve good compression but do not scale to larger document collections . We propose a new framework based on performing a Traveling Salesman computation on a reduced sparse graph obtained through Locality Sensitive Hashing . This technique achieves improved compression while scaling to tens of millions of documents . Based on this framework , we describe a number of new algorithms , and perform a detailed evaluation on three large data sets showing improvements in index size . ',\n",
       " 11015761: \"Detecting the origin of text segments efficiently In the origin detection problem an algorithm is given a set S of documents , ordered by creation time , and a query document D. It needs to output for every consecutive sequence of k alphanumeric terms in D the earliest document in $ S$ in which the sequence appeared ( if such a document exists ) . Algorithms for the origin detection problem can , for example , be used to detect the `` origin '' of text segments in D and thus to detect novel content in D. They can also find the document from which the author of D has copied the most ( or show that D is mostly original . ) We concentrate on solutions that use only a fixed amount of memory . We propose novel algorithms for this problem and evaluate them together with a large number of previously published algorithms . Our results show that ( 1 ) detecting the origin of text segments efficiently can be done with very high accuracy even when the space used is less than 1 % of the size of the documents in $ S$ , ( 2 ) the precision degrades smoothly with the amount of available space , ( 3 ) various estimation techniques can be used to increase the performance of the algorithms . \",\n",
       " 11026417: 'Wikipedia vandalism detection Wikipedia is an online encyclopedia that anyone can access and edit . It has become one of the most important sources of knowledge online and many third party projects rely on it for a wide-range of purposes . The open model of Wikipedia allows pranksters , lobbyists and spammers to attack the integrity of the encyclopedia and this endangers it as a public resource . This is known in the community as vandalism . A plethora of methods have been developed within the Wikipedia and the scientific community to tackle this problem . We have participated in this effort and developed one of the leading approaches . Our research aims to create a fully-working antivandalism system and get it working in the real world . ',\n",
       " 11034828: 'The recurrence dynamics of social tagging How often do tags recur ? How hard is predicting tag recurrence ? What tags are likely to recur ? We try to answer these questions by analyzing the RSDC08 dataset , in both individual and collective settings . Our findings provide useful insights for the development of tag suggestion techniques etc. . ',\n",
       " 11052867: 'Data summaries for on-demand queries over linked data Typical approaches for querying structured Web Data collect ( crawl ) and pre-process ( index ) large amounts of data in a central data repository before allowing for query answering . However , this time-consuming pre-processing phase however leverages the benefits of Linked Data -- where structured data is accessible live and up-to-date at distributed Web resources that may change constantly -- only to a limited degree , as query results can never be current . An ideal query answering system for Linked Data should return current answers in a reasonable amount of time , even on corpora as large as the Web . Query processors evaluating queries directly on the live sources require knowledge of the contents of data sources . In this paper , we develop and evaluate an approximate index structure summarising graph-structured content of sources adhering to Linked Data principles , provide an algorithm for answering conjunctive queries over Linked Data on theWeb exploiting the source summary , and evaluate the system using synthetically generated queries . The experimental results show that our lightweight index structure enables complete and up-to-date query results over Linked Data , while keeping the overhead for querying low and providing a satisfying source ranking at no additional cost . ',\n",
       " 1107868: 'Groupme ! This paper presents the GroupMe ! system , a resource sharing system with advanced tagging functionality . GroupMe ! provides a novel user interface , which enables users to organize and arrange arbitrary Web resources into groups . The content of such groups can be overlooked and inspected immediately as resources are visualized in a multimedia-based fashion . In this paper , we furthermore introduce new folksonomy-based ranking strategies that exploit the group structure shipped with GroupMe ! folksonomies . Experiments show that those strategies significantly improve the performance of such ranking algorithms . ',\n",
       " 11081679: 'Exploiting content redundancy for web information extraction We propose a novel extraction approach that exploits content redundancy on the web to extract structured data from template-based web sites . We start by populating a seed database with records extracted from a few initial sites . We then identify values within the pages of each new site that match attribute values contained in the seed set of records . To filter out noisy attribute value matches , we exploit the fact that attribute values occur at fixed positions within template-based sites . We develop an efficient Apriori-style algorithm to systematically enumerate attribute position configurations with sufficient matching values across pages . Finally , we conduct an extensive experimental study with real-life web data to demonstrate the effectiveness of our extraction approach . ',\n",
       " 11108002: 'Incentivizing high-quality user-generated content We model the economics of incentivizing high-quality user generated content ( UGC ) , motivated by settings such as online review forums , question-answer sites , and comments on news articles and blogs . We provide a game-theoretic model within which to study the problem of incentivizing high quality UGC , in which contributors are strategic and motivated by exposure . Our model has the feature that both the quality of contributions as well as the extent of participation is determined endogenously in a free-entry Nash equilibrium . The model predicts , as observed in practice , that if exposure is independent of quality , there will be a flood of low quality contributions in equilibrium . An ideal mechanism in this context would elicit both high quality and high participation in equilibrium , with near-optimal quality as the available attention diverges , and should be easily implementable in practice . We consider a very simple elimination mechanism , which subjects each contribution to rating by some number A of viewers , and eliminates any contributions that are not uniformly rated positively . We construct and analyze free-entry Nash equilibria for this mechanism , and show that A can be chosen to achieve quality that tends to optimal , along with diverging participation , as the number of viewers diverges . ',\n",
       " 11134936: 'Constructing folksonomies by integrating structured metadata Aggregating many personal hierarchies into a common taxonomy , also known as a folksonomy , presents several challenges due to its sparseness , ambiguity , noise , and inconsistency . We describe an approach to folksonomy learning based on relational clustering that addresses these challenges by exploiting structured metadata contained in personal hierarchies . Our approach clusters similar hierarchies using their structure and tag statistics , then incrementally weaves them into a deeper , bushier tree . We study folksonomy learning using social metadata extracted from the photo-sharing site Flickr . We evaluate the learned folksonomy quantitatively by automatically comparing it to a reference taxonomy created by the Open Directory Project . Our empirical results suggest that the proposed approach improves upon the state-of-the-art folksonomy learning method . ',\n",
       " 11134938: 'Constructing folksonomies from user-specified relations on flickr Automatic folksonomy construction from tags has attracted much attention recently . However , inferring hierarchical relations between concepts from tags has a drawback in that it is difficult to distinguish between more popular and more general concepts . Instead of tags we propose to use user-specified relations for learning folksonomy . We explore two statistical frameworks for aggregating many shallow individual hierarchies , expressed through the collection\\\\/set relations on the social photosharing site Flickr , into a common deeper folksonomy that reflects how a community organizes knowledge . Our approach addresses a number of challenges that arise while aggregating information from diverse users , namely noisy vocabulary , and variations in the granularity level of the concepts expressed . Our second contribution is a method for automatically evaluating learned folksonomy by comparing it to a reference taxonomy , e.g. , the Web directory created by the Open Directory Project . Our empirical results suggest that user-specified relations are a good source of evidence for learning folksonomies . ',\n",
       " 11204787: \"Data quality in web archiving Web archives preserve the history of Web sites and have high long-term value for media and business analysts . Such archives are maintained by periodically re-crawling entire Web sites of interest . From an archivist 's point of view , the ideal case to ensure highest possible data quality of the archive would be to `` freeze '' the complete contents of an entire Web site during the time span of crawling and capturing the site . Of course , this is practically infeasible . To comply with the politeness specification of a Web site , the crawler needs to pause between subsequent http requests in order to avoid unduly high load on the site 's http server . As a consequence , capturing a large Web site may span hours or even days , which increases the risk that contents collected so far are incoherent with the parts that are still to be crawled . This paper introduces a model for identifying coherent sections of an archive and , thus , measuring the data quality in Web archiving . Additionally , we present a crawling strategy that aims to ensure archive coherence by minimizing the diffusion of Web site captures . Preliminary experiments demonstrate the usefulness of the model and the effectiveness of the strategy . \",\n",
       " 11212270: 'Web page rank prediction with markov models In this paper we propose a method for predicting the ranking position of a Web page . Assuming a set of successive past top-k rankings , we study the evolution of Web pages in terms of ranking trend sequences used for Markov Models training , which are in turn used to predict future rankings . The predictions are highly accurate for all experimental setups and similarity measures . ',\n",
       " 11227602: 'Modeling relationship strength in online social networks Previous work analyzing social networks has mainly focused on binary friendship relations . However , in online social networks the low cost of link formation can lead to networks with heterogeneous relationship strengths ( e.g. , acquaintances and best friends mixed together ) . In this case , the binary friendship indicator provides only a coarse representation of relationship information . In this work , we develop an unsupervised model to estimate relationship strength from interaction activity ( e.g. , communication , tagging ) and user similarity . More specifically , we formulate a link-based latent variable model , along with a coordinate ascent optimization procedure for the inference . We evaluate our approach on real-world data from Facebook and LinkedIn , showing that the estimated link weights result in higher autocorrelation and lead to improved classification accuracy . ',\n",
       " 11275612: \"Entity relation discovery from web tables and links The World-Wide Web consists not only of a huge number of unstructured texts , but also a vast amount of valuable structured data . Web tables ( 2 ) are a typical type of structured information that are pervasive on the web , and Web-scale methods that automatically extract web tables have been studied extensively ( 1 ) . Many powerful systems ( e.g. OCTOPUS ( 4 ) , Mesa ( 3 ) ) use extracted web tables as a fundamental component . In the database vernacular , a table is defined as a set of tuples which have the same attributes . Similarly , a web table is defined as a set of rows ( corresponding to database tuples ) which have the same column headers ( corresponding to database attributes ) . Therefore , to extract a web table is to extract a relation on the web . In databases , tables often contain foreign keys which refer to other tables . Therefore , it follows that hyperlinks inside a web table sometimes function as foreign keys to other relations whose tuples are contained in the hyperlink 's target pages . In this paper , we explore this idea by asking : can we discover new attributes for web tables by exploring hyperlinks inside web tables ? This poster proposes a solution that takes a web table as input . Frequent patterns are generated as new candidate relations by following hyperlinks in the web table . The confidence of candidates are evaluated , and trustworthy candidates are selected to become new attributes for the table . Finally , we show the usefulness of our method by performing experiments on a variety of web domains . \",\n",
       " 11275613: 'Extracting data records from the web using tag path clustering Fully automatic methods that extract lists of objects from the Web have been studied extensively . Record extraction , the first step of this object extraction process , identifies a set of Web page segments , each of which represents an individual object ( e.g. , a product ) . State-of-the-art methods suffice for simple search , but they often fail to handle more complicated or noisy Web page structures due to a key limitation -- their greedy manner of identifying a list of records through pairwise comparison ( i.e. , similarity match ) of consecutive segments . This paper introduces a new method for record extraction that captures a list of objects in a more robust way based on a holistic analysis of a Web page . The method focuses on how a distinct tag path appears repeatedly in the DOM tree of the Web document . Instead of comparing a pair of individual segments , it compares a pair of tag path occurrence patterns ( called visual signals ) to estimate how likely these two tag paths represent the same list of objects . The paper introduces a similarity measure that captures how closely the visual signals appear and interleave . Clustering of tag paths is then performed based on this similarity measure , and sets of tag paths that form the structure of data records are extracted . Experiments show that this method achieves higher accuracy than previous methods . ',\n",
       " 11277694: 'LINKREC : a unified framework for link recommendation with user attributes and graph structure With the phenomenal success of networking sites ( e.g. , Facebook , Twitter and LinkedIn ) , social networks have drawn substantial attention . On online social networking sites , link recommendation is a critical task that not only helps improve user experience but also plays an essential role in network growth . In this paper we propose several link recommendation criteria , based on both user attributes and graph structure . To discover the candidates that satisfy these criteria , link relevance is estimated using a random walk algorithm on an augmented social graph with both attribute and structure information . The global and local influence of the attributes is leveraged in the framework as well . Besides link recommendation , our framework can also rank attributes in a social network . Experiments on DBLP and IMDB data sets demonstrate that our method outperforms state-of-the-art methods based on network structure and node attribute information for link recommendation . ',\n",
       " 11338040: 'Cross-domain sentiment classification via spectral feature alignment Sentiment classification aims to automatically predict sentiment polarity ( e.g. , positive or negative ) of users publishing sentiment data ( e.g. , reviews , blogs ) . Although traditional classification algorithms can be used to train sentiment classifiers from manually labeled text data , the labeling work can be time-consuming and expensive . Meanwhile , users often use some different words when they express sentiment in different domains . If we directly apply a classifier trained in one domain to other domains , the performance will be very low due to the differences between these domains . In this work , we develop a general solution to sentiment classification when we do not have any labels in a target domain but have some labeled data in a different domain , regarded as source domain . In this cross-domain sentiment classification setting , to bridge the gap between the domains , we propose a spectral feature alignment ( SFA ) algorithm to align domain-specific words from different domains into unified clusters , with the help of domain-independent words as a bridge . In this way , the clusters can be used to reduce the gap between domain-specific words of the two domains , which can be used to train sentiment classifiers in the target domain accurately . Compared to previous approaches , SFA can discover a robust representation for cross-domain data by fully exploiting the relationship between the domain-specific and domain-independent words via simultaneously co-clustering them in a common latent space . We perform extensive experiments on two real world datasets , and demonstrate that SFA significantly outperforms previous approaches to cross-domain sentiment classification . ',\n",
       " 11338703: \"Enabling entity-based aggregators for web 2.0 data Selecting and presenting content culled from multiple heterogeneous and physically distributed sources is a challenging task . The exponential growth of the web data in modern times has brought new requirements to such integration systems . Data is not any more produced by content providers alone , but also from regular users through the highly popular Web 2.0 social and semantic web applications . The plethora of the available web content increased its demand by regular users who could not any more wait the development of advanced integration tools . They wanted to be able to build in a short time their own specialized integration applications . Aggregators came to the risk of these users . They allowed them not only to combine distributed content , but also to process it in ways that generate new services available for further consumption . To cope with the heterogeneous data , the Linked Data initiative aims at the creation and exploitation of correspondences across data values . In this work , although we share the Linked Data community vision , we advocate that for the modern web , linking at the data value level is not enough . Aggregators should base their integration tasks on the concept of an entity , i.e. , identifying whether different pieces of information correspond to the same real world entity , such as an event or a person . We describe our theory , system , and experimental results that illustrate the approach 's effectiveness . \",\n",
       " 11394824: 'Behavioral profiles for advanced email features We examine the behavioral patterns of email usage in a large-scale enterprise over a three-month period . In particular , we focus on two main questions : ( Q1 ) what do replies depend on ? and ( Q2 ) what is the gain of augmenting contacts through the friends of friends from the email social graph ? For Q1 , we identify and evaluate the significance of several factors that affect the reply probability and the email response time . We find that all factors of our considered set are significant , provide their relative ordering , and identify the recipient list size , and the intensity of email communication between the correspondents as the dominant factors . We highlight various novel threshold behaviors and provide support for existing hypotheses such as that of the least-effort reply . For Q2 , we find that the number of new contacts extracted from the friends-of-friends relationships amounts to a large number , but which is still a limited portion of the total enterprise size . We believe that our results provide significant insights towards informed design of advanced email features , including those of social-networking type . ',\n",
       " 11403481: 'Antourage : mining distance-constrained trips from flickr We study how to automatically extract tourist trips from large volumes of geo-tagged photographs . Working with more than 8 million of these photographs that are publicly available via photo - sharing communities such as Flickr and Panoramio , our goal is to satisfy the needs of a tourist who specifies a starting location ( typically a hotel ) together with a bounded travel distance and demands a tour that visits the popular sites along the way . Our system , named ANTOURAGE , solves this intractable problem using a novel adaptation of the max-min ant system ( MMAS ) meta-heuristic . Experiments using GPS metadata crawled from Flickr show that ANTOURAGE can generate high-quality tours . ',\n",
       " 11418806: 'Inferring query intent from reformulations and clicks Many researchers have noted that web search queries are often ambiguous or unclear . We present an approach for identifying the popular meanings of queries using web search logs and user click behavior . We show our approach to produce more complete and user-centric intents than expert judges by evaluating on TREC queries . This approach was also used by the TREC 2009 Web Track judges to obtain more representative topic descriptions from real queries . ',\n",
       " 1142421: 'Web-assisted annotation , semantic indexing and search of television and radio news The Rich News system , that can automatically annotate radio and television news with the aid of resources retrieved from the World Wide Web , is described . Automatic speech recognition gives a temporally precise but conceptually inaccurate annotation model . Information extraction from related web news sites gives the opposite : conceptual accuracy but no temporal data . Our approach combines the two for temporally accurate conceptual semantic annotation of broadcast news . First low quality transcripts of the broadcasts are produced using speech recognition , and these are then automatically divided into sections corresponding to individual news stories . A key phrases extraction component finds key phrases for each story and uses these to search for web pages reporting the same event . The text and meta-data of the web pages is then used to create index documents for the stories in the original broadcasts , which are semantically annotated using the KIM knowledge management platform . A web interface then allows conceptual search and browsing of news stories , and playing of the parts of the media files corresponding to each news story . The use of material from the World Wide Web allows much higher quality textual descriptions and semantic annotations to be produced than would have been possible using the ASR transcript directly . The semantic annotations can form a part of the Semantic Web , and an evaluation shows that the system operates with high precision , and with a moderate level of recall . ',\n",
       " 11485543: 'Ad-hoc object retrieval in the web of data Semantic Search refers to a loose set of concepts , challenges and techniques having to do with harnessing the information of the growing Web of Data ( WoD ) for Web search . Here we propose a formal model of one specific semantic search task : ad-hoc object retrieval . We show that this task provides a solid framework to study some of the semantic search problems currently tackled by commercial Web search engines . We connect this task to the traditional ad-hoc document retrieval and discuss appropriate evaluation metrics . Finally , we carry out a realistic evaluation of this task in the context of a Web search application . ',\n",
       " 11498123: 'On business activity modeling using grammars Web based applications offer a mainstream channel for businesses to manage their activities . We model such business activity in a grammar-based framework . The Backus Naur form notation is used to represent the syntax of a regular grammar corresponding to Web log patterns of interest . Then , a deterministic finite state machine is used to parse Web logs against the grammar . Detected tasks are associated with metadata such as time taken to perform the activity , and aggregated along relevant corporate dimensions . ',\n",
       " 1150148: 'CS AKTive space : representing computer science in the semantic web We present a Semantic Web application that we callCS AKTive Space . The application exploits a wide range of semantically heterogeneousand distributed content relating to Computer Science research in theUK . This content is gathered on a continuous basis using a variety of methods including harvesting and scraping as well as adopting a range models for content acquisition . The content currently comprises aroundten million RDF triples and we have developed storage , retrieval andmaintenance methods to support its management . The content is mediated through an ontology constructed for the application domainand incorporates components from other published ontologies . CS AKTive Spacesupports the exploration of patterns and implications inherent in the content and exploits a variety of visualisations and multi dimensional representations . Knowledge services supported in the applicationinclude investigating communities of practice : who is working , researching or publishing with whom . This work illustrates a number ofsubstantial challenges for the Semantic Web . These include problems of referential integrity , tractable inference and interaction support . Wereview our approaches to these issues and discuss relevant related work . ',\n",
       " 11556066: \"Fast and parallel webpage layout The web browser is a CPU-intensive program . Especially on mobile devices , webpages load too slowly , expending significant time in processing a document 's appearance . Due to power constraints , most hardware-driven speedups will come in the form of parallel architectures . This is also true of mobile devices such as phones and e-books . In this paper , we introduce new algorithms for CSS selector matching , layout solving , and font rendering , which represent key components for a fast layout engine . Evaluation on popular sites shows speedups as high as 80x . We also formulate the layout problem with attribute grammars , enabling us to not only parallelize our algorithm but prove that it computes in O ( log ) time and without reflow . \",\n",
       " 11570239: 'DSNotify : handling broken links in the web of data The Web of Data has emerged as a way of exposing structured linked data on the Web . It builds on the central building blocks of the Web ( URIs , HTTP ) and benefits from its simplicity and wide-spread adoption . It does , however , also inherit the unresolved issues such as the broken link problem . Broken links constitute a major challenge for actors consuming Linked Data as they require them to deal with reduced accessibility of data . We believe that the broken link problem is a major threat to the whole Web of Data idea and that both Linked Data consumers and providers will require solutions that deal with this problem . Since no general solutions for fixing such links in the Web of Data have emerged , we make three contributions into this direction : first , we provide a concise definition of the broken link problem and a comprehensive analysis of existing approaches . Second , we present DSNotify , a generic framework able to assist human and machine actors in fixing broken links . It uses heuristic feature comparison and employs a time-interval-based blocking technique for the underlying instance matching problem . Third , we derived benchmark datasets from knowledge bases such as DBpedia and evaluated the effectiveness of our approach with respect to the broken link problem . Our results show the feasibility of a time-interval-based blocking approach for systems that aim at detecting and fixing broken links in the Web of Data . ',\n",
       " 11616857: \"Volunteer computing : a model of the factors determining contribution to community-based scientific research Volunteer computing is a powerful way to harness distributed resources to perform large-scale tasks , similarly to other types of community-based initiatives . Volunteer computing is based on two pillars : the first is computational - allocating and managing large computing tasks ; the second is participative - making large numbers of individuals volunteer their computer resources to a project . While the computational aspects of volunteer computing received much research attention , the participative aspect remains largely unexplored . In this study we aim to address this gap : by drawing on social psychology and online communities research , we develop and test a three-dimensional model of the factors determining volunteer computing users ' contribution . We investigate one of the largest volunteer computing projects - SETI@home - by linking survey data about contributors ' motivations to their activity logs . Our findings highlight the differences between volunteer computing and other forms of community-based projects , and reveal the intricate relationship between individual motivations , social affiliation , tenure in the project , and resource contribution . Implications for research and practice are discussed . \",\n",
       " 11619605: 'Alhambra : a system for creating , enforcing , and testing browser security policies Alhambra is a browser-based system designed to enforce and test web browser security policies . At the core of Alhambra is a policy-enhanced browser supporting fine-grain security policies that restrict web page contents and execution . Alhambra requires no server-side modifications or additions to the web application . Policies can restrict the construction of the document as well as the execution of JavaScript using access control rules and a taint-tracking engine . Using the Alhambra browser , we present two security policies that we have built using our architecture , both designed to prevent cross-site scripting . The first policy uses a taint-tracking engine to prevent cross-site scripting attacks that exploit bugs in the client-side of the web applications . The second one uses browsing history to create policies that restrict the contents of documents and prevent the inclusion of malicious content . Using Alhambra we analyze the impact of policies on the compatibility of web pages . To test compatibility , Alhambra supports revisiting user-generated browsing sessions and comparing multiple security policies in parallel to quickly and automatically evaluate security policies . To compare security policies for identical pages we have also developed useful comparison metrics that quantify differences between identical pages executed with different security policies . Not only do we show that our policies are effective with minimal compatibility cost , we also demonstrate that Alhambra can enforce strong security policies and provide quantitative evaluation of the differences introduced by security policies . ',\n",
       " 11620085: \"Distributing private data in challenged network environments Developing countries face significant challenges in network access , making even simple network tasks unpleasant . Many standard techniques - caching and predictive prefetching - help somewhat , but provide little or no assistance for personal data that is needed only by a single user . Sulula addresses this problem by leveraging the near-ubiquity of cellular phones able to send and receive simple SMS messages . Rather than visit a kiosk and fetch data on demand - a tiresome process at best - users request a future visit . If capacity exists , the kiosk can schedule secure retrieval of that user 's data , saving time and more efficiently utilizing the kiosk 's limited connectivity . When the user arrives at a provisioned kiosk , she need only obtain the session key on-demand , and thereafter has instant access . In addition , Sulula allows users to schedule data uploads . Experimental results show significant gains for the end user , saving tens of minutes of time for a typical email\\\\/news reading session . We also describe a small , ongoing deployment in-country for proof-of-concept , lessons learned from that experience , and provide a discussion on pricing and marketplace issues that remain to be addressed to make the system viable for developing-world access . \",\n",
       " 11624616: \"Mining advertiser-specific user behavior using adfactors Consider an online ad campaign run by an advertiser . The ad serving companies that handle such campaigns record users ' behavior that leads to impressions of campaign ads , as well as users ' responses to such impressions . This is summarized and reported to the advertisers to help them evaluate the performance of their campaigns and make better budget allocation decisions . The most popular reporting statistics are the click-through rate and the conversion rate . While these are indicative of the effectiveness of an ad campaign , the advertisers often seek to understand more sophisticated long-term effects of their ads on the brand awareness and the user behavior that leads to the conversion , thus creating a need for the reporting measures that can capture both the duration and the frequency of the pathways to user conversions . In this paper , we propose an alternative data mining framework for analyzing user-level advertising data . In the aggregation step , we compress individual user histories into a graph structure , called the adgraph , representing local correlations between ad events . For the reporting step , we introduce several scoring rules , called the adfactors ( AF ) , that can capture global role of ads and ad paths in the adgraph , in particular , the structural correlation between an ad impression and the user conversion . We present scalable local algorithms for computing the adfactors ; all algorithms were implemented using the MapReduce programming model and the Pregel framework . Using an anonymous user-level dataset of sponsored search campaigns for eight different advertisers , we evaluate our framework with different adgraphs and adfactors in terms of their statistical fit to the data , and show its value for mining the long-term behavioral patterns in the advertising data . \",\n",
       " 11628441: \"Exploiting social context for review quality prediction Online reviews in which users publish detailed commentary about their experiences and opinions with products , services , or events are extremely valuable to users who rely on them to make informed decisions . However , reviews vary greatly in quality and are constantly increasing in number , therefore , automatic assessment of review helpfulness is of growing importance . Previous work has addressed the problem by treating a review as a stand-alone document , extracting features from the review text , and learning a function based on these features for predicting the review quality . In this work , we exploit contextual information about authors ' identities and social networks for improving review quality prediction . We propose a generic framework for incorporating social context information by adding regularization constraints to the text-based predictor . Our approach can effectively use the social context information available for large quantities of unlabeled reviews . It also has the advantage that the resulting predictor is usable even when social context is unavailable . We validate our framework within a real commerce portal and experimentally demonstrate that using social context information can help improve the accuracy of review quality prediction especially when the available training data is sparse . \",\n",
       " 11631340: 'Measurement and analysis of an online content voting network : a case study of Digg In online content voting networks , aggregate user activities ( e.g. , submitting and rating content ) make high-quality content thrive through the unprecedented scale , high dynamics and divergent quality of user generated content ( UGC ) . To better understand the nature and impact of online content voting networks , we have analyzed Digg , a popular online social news aggregator and rating website . Based on a large amount of data collected , we provide an in-depth study of Digg . We study structural properties of Digg social network , revealing some strikingly distinct properties such as low link symmetry and the power-law distribution of node outdegree with truncated tails . We explore impact of the social network on user digging activities , and investigate the issues of content promotion , content filtering , vote spam and content censorship , which are inherent to content rating networks . We also provide insight into design of content promotion algorithms and recommendation-assisted content discovery . Overall , we believe that the results presented in this paper are crucial in understanding online content rating networks . ',\n",
       " 11631791: \"Beyond position bias : examining result attractiveness as a source of presentation bias in clickthrough data Leveraging clickthrough data has become a popular approach for evaluating and optimizing information retrieval systems . Although data is plentiful , one must take care when interpreting clicks , since user behavior can be affected by various sources of presentation bias . While the issue of position bias in clickthrough data has been the topic of much study , other presentation bias effects have received comparatively little attention . For instance , since users must decide whether to click on a result based on its summary ( e.g. , the title , URL and abstract ) , one might expect clicks to favor `` more attractive '' results . In this paper , we examine result summary attractiveness as a potential source of presentation bias . This study distinguishes itself from prior work by aiming to detect systematic biases in click behavior due to attractive summaries inflating perceived relevance . Our experiments conducted on the Google web search engine show substantial evidence of presentation bias in clicks towards results with more attractive titles . \",\n",
       " 11633625: \"Automated object persistence for JavaScript Traditionally web applications have required an internet connection in order to work with data . Browsers have lacked any mechanisms to allow web applications to operate offline with a set of data to provide constant access to applications . Recently , through browser plug-ins such as Google Gears , browsers have gained the ability to persist data for offline use . However , until now it 's been difficult for a web developer using these plug-ins to manage persisting data both locally for offline use and in the internet cloud due to : synchronization requirements , managing throughput and latency to the cloud , and making it work within the confines of a standards-compliant web browser . Historically in non-browser environments , programming language environments have offered automated object persistence to shield the developer from these complexities . In our research we have created a framework which introduces automated persistence of data objects for JavaScript utilizing the internet . Unlike traditional object persistence solutions , ours relies only on existing or forthcoming internet standards and does not rely upon specific runtime mechanisms such as OS or interpreter\\\\/compiler support . A new design was required in order to be suitable to the internet 's unique characteristics of varying connection quality and a browser 's specific restrictions . We validate our approach using benchmarks which show that our framework can handle thousands of data objects automatically , reducing the amount of work needed by developers to support offline Web applications . \",\n",
       " 11638887: \"Diversifying web search results Result diversity is a topic of great importance as more facets of queries are discovered and users expect to find their desired facets in the first page of the results . However , the underlying questions of how ` diversity ' interplays with ` quality ' and when preference should be given to one or both are not well-understood . In this work , we model the problem as expectation maximization and study the challenges of estimating the model parameters and reaching an equilibrium . One model parameter , for example , is correlations between pages which we estimate using textual contents of pages and click data ( when available ) . We conduct experiments on diversifying randomly selected queries from a query log and the queries chosen from the disambiguation topics of Wikipedia . Our algorithm improves upon Google in terms of the diversity of random queries , retrieving 14 % to 38 % more aspects of queries in top 5 , while maintaining a precision very close to Google . On a more selective set of queries that are expected to benefit from diversification , our algorithm improves upon Google in terms of precision and diversity of the results , and significantly outperforms another baseline system for result diversification . \",\n",
       " 11639816: \"Statistical models of music-listening sessions in social media User experience in social media involves rich interactions with the media content and other participants in the community . In order to support such communities , it is important to understand the factors that drive the users ' engagement . In this paper we show how to define statistical models of different complexity to describe patterns of song listening in an online music community . First , we adapt the LDA model to capture music taste from listening activities across users and identify both the groups of songs associated with the specific taste and the groups of listeners who share the same taste . Second , we define a graphical model that takes into account listening sessions and captures the listening moods of users in the community . Our session model leads to groups of songs and groups of listeners with similar behavior across listening sessions and enables faster inference when compared to the LDA model . Our experiments with the data from an online media site demonstrate that the session model is better in terms of the perplexity compared to two other models : the LDA-based taste model that does not incorporate cross-session information and a baseline model that does not use latent groupings of songs . \",\n",
       " 11643476: \"Money , glory and cheap talk : analyzing strategic behavior of contestants in simultaneous crowdsourcing contests on TopCoder.com Crowdsourcing is a new Web phenomenon , in which a firm takes a function once performed in-house and outsources it to a crowd , usually in the form of an open contest . Designing efficient crowdsourcing mechanisms is not possible without deep understanding of incentives and strategic choices of all participants . This paper presents an empirical analysis of determinants of individual performance in multiple simultaneous crowdsourcing contests using a unique dataset for the world 's largest competitive software development portal : TopCoder.com . Special attention is given to studying the effects of the reputation system currently used by TopCoder.com on behavior of contestants . We find that individual specific traits together with the project payment and the number of project requirements are significant predictors of the final project quality . Furthermore , we find significant evidence of strategic behavior of contestants . High rated contestants face tougher competition from their opponents in the competition phase of the contest . In order to soften the competition , they move first in the registration phase of the contest , signing up early for particular projects . Although registration in TopCoder contests is non-binding , it deters entry of opponents in the same contest ; our lower bound estimate shows that this strategy generates significant surplus gain to high rated contestants . We conjecture that the reputation + cheap talk mechanism employed by TopCoder has a positive effect on allocative efficiency of simultaneous all-pay contests and should be considered for adoption in other crowdsourcing platforms . \",\n",
       " 11657105: 'Live web search experiments for the rest of us There are significant barriers to academic research into user Web search preferences . Academic researchers are unable to manipulate the results shown by a major search engine to users and would have no access to the interaction data collected by the engine . Our initial approach to overcoming this was to ask participants to submit queries to an experimental search engine rather than their usual search tool . Over several different experiments we found that initial user buy-in was high but that people quickly drifted back to their old habits and stopped contributing data . Here , we report our investigation of possible reasons why this occurs . An alternative approach is exemplified by the Lemur browser toolbar , which allows local collection of user interaction data from search engine sessions , but does not allow result pages to be modified . We will demonstrate a new Firefox toolbar that we have developed to support experiments in which search results may be arbitrarily manipulated . Using our toolbar , academics can set up the experiments they want to conduct , while collecting ( subject to human experimentation guidelines ) queries , clicks and dwell times as well as optional explicit judgments . ',\n",
       " 11659004: 'On the high density of leadership nuclei in endorsement social networks In this paper we study the community structure of endorsement networks , i.e. , social networks in which a directed edge u → v is asserting an action of support from user u to user v. Examples include scenarios in which a user u is favoring a photo , liking a post , or following the microblog of user v. Starting from the hypothesis that the footprint of a community in an endorsement network is a bipartite directed clique from a set of followers to a set of leaders , we apply frequent itemset mining techniques to discover such bicliques . Our analysis of real networks discovers that an interesting phenomenon is taking place : the leaders of a community are endorsing each other forming a very dense nucleus . ',\n",
       " 1167068: \"Semantic virtual environments Today 's Virtual Environment ( VE ) systems share a number of issues with the HTML-based World Wide Web . Their content is usually designed for presentation to humans , and thus is not suitable for machine access . This is complicated by the large number of different data models and network protocols in use . Accordingly , it is difficult to develop VE software , such as agents , services , and tools . In this paper we adopt the Semantic Web idea to the field of virtual environments . Using the Resource Description Framework ( RDF ) we establish a machine-understandable abstraction of existing VE systems -- the Semantic Virtual Environments ( SVE ) . On this basis it is possible to develop system-independent software , which could even operate across VE system boundaries . \",\n",
       " 1168781: \"Designing an architecture for delivering mobile information services to the rural developing world Implementing successful rural computing applications requires addressing a number of significant challenges . Recent advances in mobile phone computing capabilities make this device a likely candidate to address the client hardware constraints . Long battery life , wireless connectivity , solid-state memory , low price and immediate utility all make it better suited to rural conditions than a PC . However , current mobile software platforms are not as appropriate . Web-based mobile applications are hard to use , do not take advantage of the mobile phone 's media capabilities and require an online connection . Custom mobile applications are difficult to develop and distribute . To address these limitations we present CAM - a new framework for developing and deploying mobile computing applications in the rural developing world . CAM applications are accessed by capturing barcodes using the mobile phone camera , or entering numeric strings with the keypad . Supporting minimal navigation , direct linkage to paper practices and offline multi-media interaction , CAM is uniquely adapted to rural device , user and infrastructure constraints . To illustrate the breadth of the framework , we list a number of CAM-based applications that we have implemented or are planning . These include processing microfinance loans , facilitating rural supply chains , documenting grassroots innovation and accessing electronic medical histories . \",\n",
       " 11701167: \"Not so creepy crawler : easy crawler generation with standard xml queries Web crawlers are increasingly used for focused tasks such as the extraction of data from Wikipedia or the analysis of social networks like last . fm . In these cases , pages are far more uniformly structured than in the general Web and thus crawlers can use the structure of Web pages for more precise data extraction and more expressive analysis . In this demonstration , we present a focused , structure-based crawler generator , the `` Not so Creepy Crawler '' ( nc2 ) . What sets nc2 apart , is that all analysis and decision tasks of the crawling process are delegated to an ( arbitrary ) XML query engine such as XQuery or Xcerpt . Customizing crawlers just means writing ( declarative ) XML queries that can access the currently crawled document as well as the metadata of the crawl process . We identify four types of queries that together sufice to realize a wide variety of focused crawlers . We demonstrate nc2 with two applications : The first extracts data about cities from Wikipedia with a customizable set of attributes for selecting and reporting these cities . It illustrates the power of nc2 where data extraction from Wiki-style , fairly homogeneous knowledge sites is required . In contrast , the second use case demonstrates how easy nc2 makes even complex analysis tasks on social networking sites , here exemplified by last . fm . \",\n",
       " 11703421: 'SafeVchat : detecting obscene content and misbehaving users in online video chat services Online video chat services such as Chatroulette , Omegle , and vChatter that randomly match pairs of users in video chat sessions are fast becoming very popular , with over a million users per month in the case of Chatroulette . A key problem encountered in such systems is the presence of flashers and obscene content . This problem is especially acute given the presence of underage minors in such systems . This paper presents SafeVchat , a novel solution to the problem of flasher detection that employs an array of image detection algorithms . A key contribution of the paper concerns how the results of the individual detectors are fused together into an overall decision classifying the user as misbehaving or not , based on Dempster-Shafer Theory . The paper introduces a novel , motion-based skin detection method that achieves significantly higher recall and better precision . The proposed methods have been evaluated over real-world data and image traces obtained from Chatroulette.com . ',\n",
       " 11708130: 'Pragmatic evaluation of folksonomies Recently , a number of algorithms have been proposed to obtain hierarchical structures - so-called folksonomies - from social tagging data . Work on these algorithms is in part driven by a belief that folksonomies are useful for tasks such as : ( a ) Navigating social tagging systems and ( b ) Acquiring semantic relationships between tags . While the promises and pitfalls of the latter have been studied to some extent , we know very little about the extent to which folksonomies are pragmatically useful for navigating social tagging systems . This paper sets out to address this gap by presenting and applying a pragmatic framework for evaluating folksonomies . We model exploratory navigation of a tagging system as decentralized search on a network of tags . Evaluation is based on the fact that the performance of a decentralized search algorithm depends on the quality of the background knowledge used . The key idea of our approach is to use hierarchical structures learned by folksonomy algorithm as background knowledge for decentralized search . Utilizing decentralized search on tag networks in combination with different folksonomies as hierarchical background knowledge allows us to evaluate navigational tasks in social tagging systems . Our experiments with four state-of-the-art folksonomy algorithms on five different social tagging datasets reveal that existing folksonomy algorithms exhibit significant , previously undiscovered , differences with regard to their utility for navigation . Our results are relevant for engineers aiming to improve navigability of social tagging systems and for scientists aiming to evaluate different folksonomy algorithms from a pragmatic perspective . ',\n",
       " 11714006: \"visKQWL , a visual renderer for a semantic web query language KiWi is a semantic Wiki that combines the Wiki philosophy of collaborative content creation with the methods of the Semantic Web in order to enable effective knowledge management . Querying a Wiki must be simple enough for beginning users , yet powerful enough to accommodate experienced users . To this end , the keyword-based KiWi query language ( KWQL ) supports queries ranging from simple lists of keywords to expressive rules for selecting and reshaping Wiki ( meta - ) data . In this demo , we showcase visKWQL , a visual interface for the KWQL language aimed at supporting users in the query construction process . visKWQL and its editor are described , and their functionality is illustrated using example queries . visKWQL 's editor provides guidance throughout the query construction process through hints , warnings and highlighting of syntactic errors . The editor enables round-tripping between the twin languages KWQL and visKWQL , meaning that users can switch freely between the textual and visual form when constructing or editing a query . It is implemented using HTML , JavaScript , and CSS , and can thus be used in ( almost ) any web browser without any additional software . \",\n",
       " 11749957: 'LCA-based selection for XML document collections In this paper , we address the problem of database selection for XML document collections , that is , given a set of collections and a user query , how to rank the collections based on their goodness to the query . Goodness is determined by the relevance of the documents in the collection to the query . We consider keyword queries and support Lowest Common Ancestor ( LCA ) semantics for defining query results , where the relevance of each document to a query is determined by properties of the LCA of those nodes in the XML document that contain the query keywords . To avoid evaluating queries against each document in a collection , we propose maintaining in a preprocessing phase , information about the LCAs of all pairs of keywords in a document and use it to approximate the properties of the LCA-based results of a query . To improve storage and processing efficiency , we use appropriate summaries of the LCA information based on Bloom filters . We address both a boolean and a weighted version of the database selection problem . Our experimental results show that our approach incurs low errors in the estimation of the goodness of a collection and provides rankings that are very close to the actual ones . ',\n",
       " 11756683: \"A scalable machine-learning approach for semi-structured named entity recognition Named entity recognition studies the problem of locating and classifying parts of free text into a set of predefined categories . Although extensive research has focused on the detection of person , location and organization entities , there are many other entities of interest , including phone numbers , dates , times and currencies ( to name a few examples ) . We refer to these types of entities as `` semi-structured named entities '' , since they usually follow certain syntactic formats according to some conventions , although their structure is typically not well-defined . Regular expression solutions require significant amount of manual effort and supervised machine learning approaches rely on large sets of labeled training data . Therefore , these approaches do not scale when we need to support many semi-structured entity types in many languages and regions . In this paper , we study this problem and propose a novel three-level bootstrapping framework for the detection of semi-structured entities . We describe the proposed techniques for phone , date and time entities , and perform extensive evaluations on English , German , Polish , Swedish and Turkish documents . Despite the minimal input from the user , our approach can achieve 95 % precision and 84 % recall for phone entities , and 94 % precision and 81 % recall for date and time entities , on average . We also discuss implementation details and report run time performance results , which show significant improvements over regular expression based solutions . \",\n",
       " 11759742: \"Rated aspect summarization of short comments Web 2.0 technologies have enabled more and more people to freely comment on different kinds of entities ( e.g. sellers , products , services ) . The large scale of information poses the need and challenge of automatic summarization . In many cases , each of the user-generated short comments comes with an overall rating . In this paper , we study the problem of generating a `` rated aspect summary '' of short comments , which is a decomposed view of the overall ratings for the major aspects so that a user could gain different perspectives towards the target entity . We formally define the problem and decompose the solution into three steps . We demonstrate the effectiveness of our methods by using eBay sellers ' feedback comments . We also quantitatively evaluate each step of our methods and study how well human agree on such a summarization task . The proposed methods are quite general and can be used to generate rated aspect summary automatically given any collection of short comments each associated with an overall rating . \",\n",
       " 11782911: \"Differences in the mechanics of information diffusion across topics : idioms , political hashtags , and complex contagion on twitter There is a widespread intuitive sense that different kinds of information spread differently on-line , but it has been difficult to evaluate this question quantitatively since it requires a setting where many different kinds of information spread in a shared environment . Here we study this issue on Twitter , analyzing the ways in which tokens known as hashtags spread on a network defined by the interactions among Twitter users . We find significant variation in the ways that widely-used hashtags on different topics spread . Our results show that this variation is not attributable simply to differences in `` stickiness , '' the probability of adoption based on one or more exposures , but also to a quantity that could be viewed as a kind of `` persistence '' - the relative extent to which repeated exposures to a hashtag continue to have significant marginal effects . We find that hashtags on politically controversial topics are particularly persistent , with repeated exposures continuing to have unusually large marginal effects on adoption ; this provides , to our knowledge , the first large-scale validation of the `` complex contagion '' principle from sociology , which posits that repeated exposures to an idea are particularly crucial when the idea is in some way controversial or contentious . Among other findings , we discover that hashtags representing the natural analogs of Twitter idioms and neologisms are particularly non-persistent , with the effect of multiple exposures decaying rapidly relative to the first exposure . We also study the subgraph structure of the initial adopters for different widely-adopted hashtags , again finding structural differences across topics . We develop simulation-based and generative models to analyze how the adoption dynamics interact with the network structure of the early adopters on which a hashtag spreads . \",\n",
       " 11785: 'Using context - and content-based trust policies on the semantic web The current discussion about a future Semantic Web trust architecture is focused on reputational trust mechanisms based on explicit trust ratings . What is often overlooked is the fact that , besides of ratings , huge parts of the application-specific data published on the Semantic Web are also trust relevant and therefore can be used for flexible , fine-grained trust evaluations . In this poster we propose the usage of context - and content-based trust mechanisms and outline a trust architecture which allows the formulation of subjective and task-specific trust policies as a combination of reputation - , context - and content-based trust mechanisms . ',\n",
       " 11788753: 'Network bucket testing Bucket testing , also known as A\\\\/B testing , is a practice that is widely used by on-line sites with large audiences : in a simple version of the methodology , one evaluates a new feature on the site by exposing it to a very small fraction of the total user population and measuring its effect on this exposed group . For traditional uses of this technique , uniform independent sampling of the population is often enough to produce an exposed group that can serve as a statistical proxy for the full population . In on-line social network applications , however , one often wishes to perform a more complex test : evaluating a new social feature that will only produce an effect if a user and some number of his or her friends are exposed to it . In this case , independent uniform draws from the population will be unlikely to produce groups that contains users together with their friends , and so the construction of the sample must take the network structure into account . This leads quickly to challenging combinatorial problems , since there is an inherent tension between producing enough correlation to select users and their friends , but also enough uniformity and independence that the selected group is a reasonable sample of the full population . Here we develop an algorithmic framework for bucket testing in a network that addresses these challenges . First we describe a novel walk-based sampling method for producing samples of nodes that are internally well-connected but also approximately uniform over the population . Then we show how a collection of multiple independent subgraphs constructed this way can yield reasonable samples for testing . We demonstrate the effectiveness of our algorithms through computational experiments on large portions of the Facebook network . ',\n",
       " 11792499: \"Collaborative location and activity recommendations with GPS history data With the increasing popularity of location-based services , such as tour guide and location-based social network , we now have accumulated many location data on the Web . In this paper , we show that , by using the location data based on GPS and users ' comments at various locations , we can discover interesting locations and possible activities that can be performed there for recommendations . Our research is highlighted in the following location-related queries in our daily life : 1 ) if we want to do something such as sightseeing or food-hunting in a large city such as Beijing , where should we go ? 2 ) If we have already visited some places such as the Bird 's Nest building in Beijing 's Olympic park , what else can we do there ? By using our system , for the first question , we can recommend her to visit a list of interesting locations such as Tiananmen Square , Bird 's Nest , etc. . For the second question , if the user visits Bird 's Nest , we can recommend her to not only do sightseeing but also to experience its outdoor exercise facilities or try some nice food nearby . To achieve this goal , we first model the users ' location and activity histories that we take as input . We then mine knowledge , such as the location features and activity-activity correlations from the geographical databases and the Web , to gather additional inputs . Finally , we apply a collective matrix factorization method to mine interesting locations and activities , and use them to recommend to the users where they can visit if they want to perform some specific activities and what they can do if they visit some specific places . We empirically evaluated our system using a large GPS dataset collected by 162 users over a period of 2.5 years in the real-world . We extensively evaluated our system and showed that our system can outperform several state-of-the-art baselines . \",\n",
       " 11796084: 'SourceRank : relevance and trust assessment for deep web sources based on inter-source agreement One immediate challenge in searching the deep web databases is source selection - i.e. selecting the most relevant web databases for answering a given query . The existing database selection methods ( both text and relational ) assess the source quality based on the query-similarity-based relevance assessment . When applied to the deep web these methods have two deficiencies . First is that the methods are agnostic to the correctness ( trustworthiness ) of the sources . Secondly , the query based relevance does not consider the importance of the results . These two considerations are essential for the open collections like the deep web . Since a number of sources provide answers to any query , we conjuncture that the agreements between these answers are likely to be helpful in assessing the importance and the trustworthiness of the sources . We compute the agreement between the sources as the agreement of the answers returned . While computing the agreement , we also measure and compensate for possible collusion between the sources . This adjusted agreement is modeled as a graph with sources at the vertices . On this agreement graph , a quality score of a source that we call SourceRank , is calculated as the stationary visit probability of a random walk . We evaluate SourceRank in multiple domains , including sources in Google Base , with sizes up to 675 sources . We demonstrate that the SourceRank tracks source corruption . Further , our relevance evaluations show that SourceRank improves precision by 22-60 % over the Google Base and the other baseline methods . SourceRank has been implemented in a system called Factal . ',\n",
       " 11798933: 'Dynamic and graphical web page breakpoints Breakpoints are perhaps the quintessential feature of a de-bugger : they allow a developer to stop time and study the program state . Breakpoints are typically specified by selecting a line of source code . For large , complex , web pages with multiple developers , the relevant source line for a given user interface problem may not be known to the developer . In this paper we describe the implementation of breakpoints in dynamically created source , and on error messages , network events , DOMmutation , DOMobject property changes , and CSS style rule updates . Adding these domain-specific breakpoints to a general-purpose debugger for Javascript allows the developer to initiate the debugging process via Web page abstractions rather than lower level source code views . The breakpoints are implemented in the open source Fire-bug project , version 1.5 , for the Firefox Web browser . ',\n",
       " 11806298: 'HyperANF : approximating the neighborhood function of very large graphs on a budget The neighborhood function NG ( t ) of a graph G gives , for each t ∈ N , the number of pairs of nodes x , y such that y is reachable from x in less that t hops . The neighborhood function provides a wealth of information about the graph ( 10 ) ( e.g. , it easily allows one to compute its diameter ) , but it is very expensive to compute it exactly . Recently , the ANF algorithm ( 10 ) ( approximate neighborhood function ) has been proposed with the purpose of approximating NG ( t ) on large graphs . We describe a breakthrough improvement over ANF in terms of speed and scalability . Our algorithm , called HyperANF , uses the new HyperLogLog counters ( 5 ) and combines them efficiently through broadword programming ( 8 ) ; our implementation uses talk decomposition to exploit multi-core parallelism . With HyperANF , for the first time we can compute in a few hours the neighborhood function of graphs with billions of nodes with a small error and good confidence using a standard workstation . Then , we turn to the study of the distribution of the distances between reachable nodes ( that can be efficiently approximated by means of HyperANF ) , and discover the surprising fact that its index of dispersion provides a clear-cut characterisation of proper social networks vs. web graphs . We thus propose the spid ( Shortest-Paths Index of Dispersion ) of a graph as a new , informative statistics that is able to discriminate between the above two types of graphs . We believe this is the first proposal of a significant new non-local structural index for complex networks whose computation is highly scalable . ',\n",
       " 11807460: \"Prophiler : a fast filter for the large-scale detection of malicious web pages Malicious web pages that host drive-by-download exploits have become a popular means for compromising hosts on the Internet and , subsequently , for creating large-scale botnets . In a drive-by-download exploit , an attacker embeds a malicious script ( typically written in JavaScript ) into a web page . When a victim visits this page , the script is executed and attempts to compromise the browser or one of its plugins . To detect drive-by-download exploits , researchers have developed a number of systems that analyze web pages for the presence of malicious code . Most of these systems use dynamic analysis . That is , they run the scripts associated with a web page either directly in a real browser ( running in a virtualized environment ) or in an emulated browser , and they monitor the scripts ' executions for malicious activity . While the tools are quite precise , the analysis process is costly , often requiring in the order of tens of seconds for a single page . Therefore , performing this analysis on a large set of web pages containing hundreds of millions of samples can be prohibitive . One approach to reduce the resources required for performing large-scale analysis of malicious web pages is to develop a fast and reliable filter that can quickly discard pages that are benign , forwarding to the costly analysis tools only the pages that are likely to contain malicious code . In this paper , we describe the design and implementation of such a filter . Our filter , called Prophiler , uses static analysis techniques to quickly examine a web page for malicious content . This analysis takes into account features derived from the HTML contents of a page , from the associated JavaScript code , and from the corresponding URL . We automatically derive detection models that use these features using machine-learning techniques applied to labeled datasets . To demonstrate the effectiveness and efficiency of Prophiler , we crawled and collected millions of pages , which we analyzed for malicious behavior . Our results show that our filter is able to reduce the load on a more costly dynamic analysis tools by more than 85 % , with a negligible amount of missed malicious pages . \",\n",
       " 11811654: \"How much is your personal recommendation worth ? Suppose you buy a new laptop and , simply because you like it so much , you recommend it to friends , encouraging them to purchase it as well . What would be an adequate price for the vendor of the laptop to pay for your recommendation ? Personal recommendations like this are of considerable commercial interest , but unlike in sponsored search auctions there can be no truthful prices . Despite this `` lack of truthfulness '' the vendor of the product might still decide to pay you for recommendation e.g. because she wants to ( i ) provide you with an additional incentive to actually recommend her or to ( ii ) increase your satisfaction and\\\\/or brand loyalty . This leads us to investigate a pricing scheme based on the Shapley value ( 5 ) that satisfies certain `` axioms of fairness '' . We find that it is vulnerable to manipulations and show how to overcome these difficulties using the anonymity-proof Shapley value of ( 4 ) . \",\n",
       " 11820735: 'Exploiting web search engines to search structured databases Web search engines often federate many user queries to relevant structured databases . For example , a product related query might be federated to a product database containing their descriptions and specifications . The relevant structured data items are then returned to the user along with web search results . However , each structured database is searched in isolation . Hence , the search often produces empty or incomplete results as the database may not contain the required information to answer the query . In this paper , we propose a novel integrated search architecture . We establish and exploit the relationships between web search results and the items in structured databases to identify the relevant structured data items for a much wider range of queries . Our architecture leverages existing search engine components to implement this functionality at very low overhead . We demonstrate the quality and efficiency of our techniques through an extensive experimental study . ',\n",
       " 1182832: 'How semantics make better wikis Wikis are popular collaborative hypertext authoring environments , but they neither support structured access nor information reuse . Adding semantic annotations helps to address these limitations . We present an architecture for Semantic Wikis and discuss design decisions including structured access , views , and annotation language . We present our prototype SemperWiki that implements this architecture . ',\n",
       " 11835370: \"Efficient resource allocation and power saving in multi-tiered systems In this paper , we present Fastrack , a parameter-free algorithm for dynamic resource provisioning that uses simple statistics to promptly distill information about changes in workload burstiness . This information , coupled with the application 's end-to-end response times and system bottleneck characteristics , guide resource allocation that shows to be very effective under a broad variety of burstiness profiles and bottleneck scenarios . \",\n",
       " 11836329: \"Automatic extraction of clickable structured web contents for name entity queries Today the major web search engines answer queries by showing ten result snippets , which need to be inspected by users for identifying relevant results . In this paper we investigate how to extract structured information from the web , in order to directly answer queries by showing the contents being searched for . We treat users ' search trails ( i.e. , post-search browsing behaviors ) as implicit labels on the relevance between web contents and user queries . Based on such labels we use information extraction approach to build wrappers and extract structured information . An important observation is that many web sites contain pages for name entities of certain categories ( e.g. , AOL Music contains a page for each musician ) , and these pages have the same format . This makes it possible to build wrappers from a small amount of implicit labels , and use them to extract structured information from many web pages for different name entities . We propose STRUCLICK , a fully automated system for extracting structured information for queries containing name entities of certain categories . It can identify important web sites from web search logs , build wrappers from users ' search trails , filter out bad wrappers built from random user clicks , and combine structured information from different web sites for each query . Comparing with existing approaches on information extraction , STRUCLICK can assign semantics to extracted data without any human labeling or supervision . We perform comprehensive experiments , which show STRUCLICK achieves high accuracy and good scalability . \",\n",
       " 11839803: 'Finding hierarchy in directed online social networks Social hierarchy and stratification among humans is a well studied concept in sociology . The popularity of online social networks presents an opportunity to study social hierarchy for different types of networks and at different scales . We adopt the premise that people form connections in a social network based on their perceived social hierarchy ; as a result , the edge directions in directed social networks can be leveraged to infer hierarchy . In this paper , we define a measure of hierarchy in a directed online social network , and present an efficient algorithm to compute this measure . We validate our measure using ground truth including Wikipedia notability score . We use this measure to study hierarchy in several directed online social networks including Twitter , Delicious , YouTube , Flickr , LiveJournal , and curated lists of several categories of people based on different occupations , and different organizations . Our experiments on different online social networks show how hierarchy emerges as we increase the size of the network . This is in contrast to random graphs , where the hierarchy decreases as the network size increases . Further , we show that the degree of stratification in a network increases very slowly as we increase the size of the graph . ',\n",
       " 11840142: 'Ranking community answers via analogical reasoning Due to the lexical gap between questions and answers , automatically detecting right answers becomes very challenging for community question-answering sites . In this paper , we propose an analogical reasoning-based method . It treats questions and answers as relational data and ranks an answer by measuring the analogy of its link to a query with the links embedded in previous relevant knowledge ; the answer that links in the most analogous way to the new question is assumed to be the best answer . We based our experiments on 29.8 million Yahoo ! Answer question-answer threads and showed the effectiveness of the approach . ',\n",
       " 11840967: \"Equip tourists with knowledge mined from travelogues With the prosperity of tourism and Web 2.0 technologies , more and more people have willingness to share their travel experiences on the Web ( e.g. , weblogs , forums , or Web 2.0 communities ) . These so-called travelogues contain rich information , particularly including location-representative knowledge such as attractions ( e.g. , Golden Gate Bridge ) , styles ( e.g. , beach , history ) , and activities ( e.g. , diving , surfing ) . The location-representative information in travelogues can greatly facilitate other tourists ' trip planning , if it can be correctly extracted and summarized . However , since most travelogues are unstructured and contain much noise , it is difficult for common users to utilize such knowledge effectively . In this paper , to mine location-representative knowledge from a large collection of travelogues , we propose a probabilistic topic model , named as Location-Topic model . This model has the advantages of ( 1 ) differentiability between two kinds of topics , i.e. , local topics which characterize locations and global topics which represent other common themes shared by various locations , and ( 2 ) representation of locations in the local topic space to encode both location-representative knowledge and similarities between locations . Some novel applications are developed based on the proposed model , including ( 1 ) destination recommendation for on flexible queries , ( 2 ) characteristic summarization for a given destination with representative tags and snippets , and ( 3 ) identification of informative parts of a travelogue and enriching such highlights with related images . Based on a large collection of travelogues , the proposed framework is evaluated using both objective and subjective evaluation methods and shows promising results . \",\n",
       " 11841922: 'Optimal rare query suggestion with implicit user feedback Query suggestion has been an effective approach to help users narrow down to the information they need . However , most of existing studies focused on only popular\\\\/head queries . Since rare queries possess much less information ( e.g. , clicks ) than popular queries in the query logs , it is much more difficult to efficiently suggest relevant queries to a rare query . In this paper , we propose an optimal rare query suggestion framework by leveraging implicit feedbacks from users in the query logs . Our model resembles the principle of pseudo-relevance feedback which assumes that top-returned results by search engines are relevant . However , we argue that the clicked URLs and skipped URLs contain different levels of information and thus should be treated differently . Hence , our framework optimally combines both the click and skip information from users and uses a random walk model to optimize the query correlation . Our model specifically optimizes two parameters : ( 1 ) the restarting ( jumping ) rate of random walk , and ( 2 ) the combination ratio of click and skip information . Unlike the Rocchio algorithm , our learning process does not involve the content of the URLs but simply leverages the click and skip counts in the query-URL bipartite graphs . Consequently , our model is capable of scaling up to the need of commercial search engines . Experimental results on one-month query logs from a large commercial search engine with over 40 million rare queries demonstrate the superiority of our framework , with statistical significance , over the traditional random walk models and pseudo-relevance feedback models . ',\n",
       " 11848994: 'Growing parallel paths for entity-page discovery In this paper , we use the structural and relational information on the Web to find entity-pages . Specifically , given a Web site and an entity-page ( e.g. , department and faculty member homepage ) we seek to find all of the entity-pages of the same type ( e.g. , all faculty members in the department ) . To do this , we propose a web structure mining method which grows parallel paths through the web graph and DOM trees . We show that by utilizing these parallel paths we can efficiently discover all entity-pages of the same type . Finally , we demonstrate the accuracy of our method with a case study on various domains . ',\n",
       " 11850677: 'Learning to re-rank : query-dependent image re-ranking using click data Our objective is to improve the performance of keyword based image search engines by re-ranking their original results . To this end , we address three limitations of existing search engines in this paper . First , there is no straight-forward , fully automated way of going from textual queries to visual features . Image search engines therefore primarily rely on static and textual features for ranking . Visual features are mainly used for secondary tasks such as finding similar images . Second , image rankers are trained on query-image pairs labeled with relevance judgments determined by human experts . Such labels are well known to be noisy due to various factors including ambiguous queries , unknown user intent and subjectivity in human judgments . This leads to learning a sub-optimal ranker . Finally , a static ranker is typically built to handle disparate user queries . The ranker is therefore unable to adapt its parameters to suit the query at hand which again leads to sub-optimal results . We demonstrate that all of these problems can be mitigated by employing a re-ranking algorithm that leverages aggregate user click data . We hypothesize that images clicked in response to a query are mostly relevant to the query . We therefore re-rank the original search results so as to promote images that are likely to be clicked to the top of the ranked list . Our re-ranking algorithm employs Gaussian Process regression to predict the normalized click count for each image , and combines it with the original ranking score . Our approach is shown to significantly boost the performance of the Bing image search engine on a wide range of tail queries . ',\n",
       " 11858468: \"we . b : the web of short urls Short URLs have become ubiquitous . Especially popular within social networking services , short URLs have seen a significant increase in their usage over the past years , mostly due to Twitter 's restriction of message length to 140 characters . In this paper , we provide a first characterization on the usage of short URLs . Specifically , our goal is to examine the content short URLs point to , how they are published , their popularity and activity over time , as well as their potential impact on the performance of the web . Our study is based on traces of short URLs as seen from two different perspectives : i ) collected through a large-scale crawl of URL shortening services , and ii ) collected by crawling Twitter messages . The former provides a general characterization on the usage of short URLs , while the latter provides a more focused view on how certain communities use shortening services . Our analysis highlights that domain and website popularity , as seen from short URLs , significantly differs from the distributions provided by well publicised services such as Alexa . The set of most popular websites pointed to by short URLs appears stable over time , despite the fact that short URLs have a limited high popularity lifetime . Surprisingly short URLs are not ephemeral , as a significant fraction , roughly 50 % , appears active for more than three months . Overall , our study emphasizes the fact that short URLs reflect an `` alternative '' web and , hence , provide an additional view on web usage and content consumption complementing traditional measurement sources . Furthermore , our study reveals the need for alternative shortening architectures that will eliminate the non-negligible performance penalty imposed by today 's shortening services . \",\n",
       " 11859214: 'HyLiEn : a hybrid approach to general list extraction on the web We consider the problem of automatically extracting general lists from the web . Existing approaches are mostly dependent upon either the underlying HTML markup or the visual structure of the Web page . We present HyLiEn an unsupervised , Hybrid approach for automatic List discovery and Extraction on the Web . It employs general assumptions about the visual rendering of lists , and the structural representation of items contained in them . We show that our method significantly outperforms existing methods . ',\n",
       " 11867634: 'Inverted index compression via online document routing Modern search engines are expected to make documents searchable shortly after they appear on the ever changing Web . To satisfy this requirement , the Web is frequently crawled . Due to the sheer size of their indexes , search engines distribute the crawled documents among thousands of servers in a scheme called local index-partitioning , such that each server indexes only several million pages . To ensure documents from the same host ( e.g. , www.nytimes.com ) are distributed uniformly over the servers , for load balancing purposes , random routing of documents to servers is common . To expedite the time documents become searchable after being crawled , documents may be simply appended to the existing index partitions . However , indexing by merely appending documents , results in larger index sizes since document reordering for index compactness is no longer performed . This , in turn , degrades search query processing performance which depends heavily on index sizes . A possible way to balance quick document indexing with efficient query processing , is to deploy online document routing strategies that are designed to reduce index sizes . This work considers the effects of several online document routing strategies on the aggregated partitioned index size . We show that there exists a tradeoff between the compression of a partitioned index and the distribution of documents from the same host across the index partitions ( i.e. , host distribution ) . We suggest and evaluate several online routing strategies with regard to their compression , host distribution , and complexity . In particular , we present a term based routing algorithm which is shown analytically to provide better compression results than the industry standard random routing scheme . In addition , our algorithm demonstrates comparable compression performance and host distribution while having much better running time complexity than other document routing heuristics . Our findings are validated by experimental evaluation performed on a large benchmark collection of Web pages . ',\n",
       " 11876537: \"Analyzing and accelerating web access in a school in peri-urban India While computers and Internet access have growing penetration amongst schools in the developing world , intermittent connectivity and limited bandwidth often prevent them from being fully utilized by students and teachers . In this paper , we make two contributions to help address this problem . First , we characterize six weeks of HTTP traffic from a primary school outside of Bangalore , India , illuminating opportunities and constraints for improving performance in such settings . Second , we deploy an aggressive caching and prefetching engine and show that it accelerates a user 's overall browsing experience ( apart from video content ) by 2.8 x. Our accelerator leverages innovative techniques that have been proposed , but not evaluated in detail , including the effectiveness of serving stale pages , cached page highlighting , and client-side prefetching . Unlike proxy-based techniques , our system is bundled as an open-source Firefox plugin and runs directly on client machines . This allows easy installation and configuration by end users , which is especially important in developing regions where a lack of permissions or technical expertise often prevents modification of internal network settings . \",\n",
       " 11878695: \"Estimating sizes of social networks via biased sampling Online social networks have become very popular in recent years and their number of users is already measured in many hundreds of millions . For various commercial and sociological purposes , an independent estimate of their sizes is important . In this work , algorithms for estimating the number of users in such networks are considered . The proposed schemes are also applicable for estimating the sizes of networks ' sub-populations . The suggested algorithms interact with the social networks via their public APIs only , and rely on no other external information . Due to obvious traffic and privacy concerns , the number of such interactions is severely limited . We therefore focus on minimizing the number of API interactions needed for producing good size estimates . We adopt the abstraction of social networks as undirected graphs and use random node sampling . By counting the number of collisions or non-unique nodes in the sample , we produce a size estimate . Then , we show analytically that the estimate error vanishes with high probability for smaller number of samples than those required by prior-art algorithms . Moreover , although our algorithms are provably correct for any graph , they excel when applied to social network-like graphs . The proposed algorithms were evaluated on synthetic as well real social networks such as Facebook , IMDB , and DBLP . Our experiments corroborated the theoretical results , and demonstrated the effectiveness of the algorithms . \",\n",
       " 11881756: \"Visualization of Geo-annotated pictures in mobile phones In this work , a novel mobile browser for geo-referenced pictures is introduced and described . We use the term browser to denote a system aimed at browsing pictures selected from a large set like Internet photo sharing services . The criteria to filter a subset of pictures to browse are three : the user 's actual position , the user 's actual heading , and the user 's preferences . In this work we only focus on the first two criteria leaving the integration of user 's preferences for future developments . \",\n",
       " 11931844: 'Highly scalable web applications with zero-copy data transfer The performance of server-side applications is becoming increasingly important as more applications exploit the Web application model . Extensive work has been done to improve the performance of individual software components such as Web servers and programming language runtimes . This paper describes a novel approach to boost Web application performance by improving inter-process communication between a programming language runtime and Web server runtime . The approach reduces redundant processing for memory copying and the context switch overhead between user space and kernel space by exploiting the zero-copy data transfer methodology , such as the sendfile system call . In order to transparently utilize this optimization feature with existing Web applications , we propose enhancements of the PHP runtime , FastCGI protocol , and Web server . Our proposed approach achieves a 126 % performance improvement with micro-benchmarks and a 44 % performance improvement for a standard Web benchmark , SPECweb2005 . ',\n",
       " 1194360: 'Evaluating a new approach to strong web cache consistency with snapshots of collected content The problem of Web cache consistency continues to be an important one . Current Web caches use heuristic-based policies for determining the freshness of cached objects , often forcing content providers to unnecessarily mark their content as uncacheable simply to retain control over it . Server-driven invalidation has been proposed as a mechanism for providing strong cache consistency for Web objects , but it requires servers to maintain per-client state even for infrequently changing objects . We propose an alternative approach to strong cache consistency , called MONARCH , which does not require servers to maintain per-client state . In this work we focus on a new approach for evaluation of MONARCH in comparison with current practice and other cache consistency policies . This approach uses snapshots of content collected from real Web sites as input to a simulator . Results of the evaluation show MONARCH generates little more request traffic than an optimal cache coherency policy . ',\n",
       " 1194809: \"Smartback : supporting users in back navigation This paper presents the design and user evaluation of SmartBack , a feature that complements the standard Back button by enabling users to jump directly to key pages in their navigation session , making common navigation activities more efficient . Defining key pages was informed by the findings of a user study that involved detailed monitoring of Web usage and analysis of Web browsing in terms of navigation trails . The pages accessible through SmartBack are determined automatically based on the structure of the user 's navigation trails or page association with specific user 's activities , such as search or browsing bookmarked sites . We discuss implementation decisions and present results of a usability study in which we deployed the SmartBack prototype and monitored usage for a month in both corporate and home settings . The results show that the feature brings qualitative improvement to the browsing experience of individuals who use it . \",\n",
       " 11981850: \"Mining collective local knowledge from Google MyMaps The emerging popularity of location-aware devices and location-based services has generated a growing archive of digital traces of people 's activities and opinions in physical space . In this study , we leverage geo-referenced user-generated content from Google MyMaps to discover collective local knowledge and understand the differing perceptions of urban space . Working with the large collection of publicly available , annotation-rich MyMaps data , we propose a highly parallelizable approach in order to merge identical places , discover landmarks , and recommend places . Additionally , we conduct interviews with New York City residents\\\\/visitors to validate the quantitative findings . \",\n",
       " 12003689: 'Find me if you can : improving geographical prediction with social and spatial proximity Geography and social relationships are inextricably intertwined ; the people we interact with on a daily basis almost always live near us . As people spend more time online , data regarding these two dimensions -- geography and social relationships -- are becoming increasingly precise , allowing us to build reliable models to describe their interaction . These models have important implications in the design of location-based services , security intrusion detection , and social media supporting local communities . Using user-supplied address data and the network of associations between members of the Facebook social network , we can directly observe and measure the relationship between geography and friendship . Using these measurements , we introduce an algorithm that predicts the location of an individual from a sparse set of located users with performance that exceeds IP-based geolocation . This algorithm is efficient and scalable , and could be run on a network containing hundreds of millions of users . ',\n",
       " 12008191: \"AdHeat : an influence-based diffusion model for propagating hints to match ads In this paper , we present AdHeat , a social ad model considering user influence in addition to relevance for matching ads . Traditionally , ad placement employs the relevance model . Such a model matches ads with Web page content , user interests , or both . We have observed , however , on social networks that the relevance model suffers from two shortcomings . First , influential users ( users who contribute opinions ) seldom click ads that are highly relevant to their expertise . Second , because influential users ' contents and activities are attractive to other users , hint words summarizing their expertise and activities may be widely preferred . Therefore , we propose AdHeat , which diffuses hint words of influential users to others and then matches ads for each user with aggregated hints . We performed experiments on a large online Q&A community with half a million users . The experimental results show that AdHeat outperforms the relevance model on CTR ( click through rate ) by significant margins . \",\n",
       " 12092784: 'Efficiently querying rdf data in triple stores Efficiently querying RDF data is being an important factor in applying Semantic Web technologies to real-world applications . In this context , many efforts have been made to store and query RDF data in relational database using particular schemas . In this paper , we propose a new scheme to store , index , and query RDF data in triple stores . Graph feature of RDF data is taken into considerations which might help reduce the join costs on the vertical database structure . We would partition RDF triples into overlapped groups , store them in a triple table with one more column of group identity , and build up a signature tree to index them . Based on this infrastructure , a complex RDF query is decomposed into multiple pieces of sub-queries which could be easily filtered into some RDF groups using signature tree index , and finally is evaluated with a composed and optimized SQL with specific constraints . We compare the performance of our method with prior art on typical queries over a large scaled LUBM and UOBM benchmark data ( more than 10 million triples ) . For some extreme cases , they can promote 3 to 4 orders of magnitude . ',\n",
       " 12102: 'Meteor-s web service annotation framework The World Wide Web is emerging not only as an infrastructure for data , but also for a broader variety of resources that are increasingly being made available as Web services . Relevant current standards like UDDI , WSDL , and SOAP are in their fledgling years and form the basis of making Web services a workable and broadly adopted technology . However , realizing the fuller scope of the promise of Web services and associated service oriented architecture will requite further technological advances in the areas of service interoperation , service discovery , service composition , and process orchestration . Semantics , especially as supported by the use of ontologies , and related Semantic Web technologies , are likely to provide better qualitative and scalable solutions to these requirements . Just as semantic annotation of data in the Semantic Web is the first critical step to better search , integration and analytics over heterogeneous data , semantic annotation of Web services is an equally critical first step to achieving the above promise . Our approach is to work with existing Web services technologies and combine them with ideas from the Semantic Web to create a better framework for Web service discovery and composition . In this paper we present MWSAF ( METEOR-S Web Service Annotation Framework ) , a framework for semi-automatically marking up Web service descriptions with ontologies . We have developed algorithms to match and annotate WSDL files with relevant ontologies . We use domain ontologies to categorize Web services into domains . An empirical study of our approach is presented to help evaluate its performance . ',\n",
       " 12127726: 'Yago : a core of semantic knowledge We present YAGO , a light-weight and extensible ontology with high coverage and quality . YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts . This includes the Is-A hierarchy as well as non-taxonomic relations between entities ( such as HASONEPRIZE ) . The facts have been automatically extracted from Wikipedia and unified with WordNet , using a carefully designed combination of rule-based and heuristic methods described in this paper . The resulting knowledge base is a major step beyond WordNet : in quality by adding knowledge about individuals like persons , organizations , products , etc. with their semantic relationships - and in quantity by increasing the number of facts by more than an order of magnitude . Our empirical evaluation of fact correctness shows an accuracy of about 95 % . YAGO is based on a logically clean model , which is decidable , extensible , and compatible with RDFS . Finally , we show how YAGO can be further extended by state-of-the-art information extraction techniques . ',\n",
       " 12191750: 'YAGO2 : exploring and querying world knowledge in time , space , context , and many languages We present YAGO2 , an extension of the YAGO knowledge base with focus on temporal and spatial knowledge . It is automatically built from Wikipedia , GeoNames , and WordNet , and contains nearly 10 million entities and events , as well as 80 million facts representing general world knowledge . An enhanced data representation introduces time and location as first-class citizens . The wealth of spatio-temporal information in YAGO can be explored either graphically or through a special time - and space-aware query language . ',\n",
       " 1225527: 'Compressing and searching XML data via two zips XML is fast becoming the standard format to store , exchange and publish over the web , and is getting embedded in applications . Two challenges in handling XML are its size ( the XML representation of a document is significantly larger than its native state ) and the complexity of its search ( XML search involves path and content searches on labeled tree structures ) . We address the basic problems of compression , navigation and searching of XML documents . In particular , we adopt recently proposed theoretical algorithms ( 11 ) for succinct tree representations to design and implement a compressed index for XML , called XBZIPiNDEX , in which the XML document is maintained in a highly compressed format , and both navigation and searching can be done uncompressing only a tiny fraction of the data . This solution relies on compressing and indexing two arrays derived from the XML data . With detailed experiments we compare this with other compressed XML indexing and searching engines to show that XBZIPiNDEX has compression ratio up to 35 % better than the ones achievable by those other tools , and its time performance on some path and content search operations is order of magnitudes faster : few milliseconds over hundreds of MBs of XML files versus tens of seconds , on standard XML data sources . ',\n",
       " 1227358: 'Fluid annotations through open hypermedia : using and extending emerging web standards The Fluid Documents project has developed various research prototypes that show that powerful annotation techniques based on animated typographical changes can help readers utilize annotations more effectively . Our recently-developed Fluid Open Hypermedia prototype supports the authoring and browsing of fluid annotations on third-party Web pages . This prototype is an extension of the Arakne Environment , an open hypermedia application that can augment Web pages with externally stored hypermedia structures . This paper describes how various Web standards , including DOM , CSS , XLink , XPointer , and RDF , can be used and extended to support fluid annotations . ',\n",
       " 1229683: 'Structured objects in owl : representation and reasoning Applications of semantic technologies often require the representation of and reasoning with structured objects - that is , objects composed of parts connected in complex ways . Although OWL is a general and powerful language , its class descriptions and axioms can not be used to describe arbitrarily connected structures . An OWL representation of structured objects can thus be underconstrained , which reduces the inferences that can be drawn and causes performance problems in reasoning . To address these problems , we extend OWL with description graphs , which allow for the description of structured objects in a simple and precise way . To represent conditional aspects of the domain , we also allow for SWRL-like rules over description graphs . Based on an observation about the nature of structured objects , we ensure decidability of our formalism . We also present a hypertableau-based decision procedure , which we implemented in the HermiT reasoner . To evaluate its performance , we have extracted description graphs from the GALEN and FMA ontologies , classified them successfully , and even detected a modeling error in GALEN . ',\n",
       " 1229684: \"Bridging the gap between OWL and relational databases Schema statements in OWL are interpreted quite differently from analogous statements in relational databases . If these statements are meant to be interpreted as integrity constraints ( ICs ) , OWL 's interpretation may seem confusing and\\\\/or inappropriate . Therefore , we propose an extension of OWL with ICs that captures the intuition behind ICs in relational databases . We discuss the algorithms for checking IC satisfaction for different types of knowledge bases , and show that , if the constraints are satisfied , we can disregard them while answering a broad range of positive queries . \",\n",
       " 1247804: 'RDF triples in XML RDF\\\\/XML does not layer RDF on top of XML ina useful way . We use a simple direct representation of the RDF abstract syntax in XML . We add the ability to name graphs , noting that in practice this is already widely used . We use XSLT as a general syntactic extensibility mechanism to provide human friendly macros for our syntax . This provides a simple serialization solving a persistent problem in the Semantic Web . ',\n",
       " 1247888: \"Choosing reputable servents in a P2P network Peer-to-peer information sharing environments are increasingly gaining acceptance on the Internet as they provide an infrastructure in which the desired information can be located and downloaded while preserving the anonymity of both requestors and providers . As recent experience with P2P environments such as Gnutella shows , anonymity opens the door to possible misuses and abuses by resource providers exploiting the network as a way to spread tampered with resources , including malicious programs , such as Trojan Horses and viruses . In this paper we propose an approach to P2P security where servents can keep track , and share with others , information about the reputation of their peers . Reputation sharing is based on a distributed polling algorithm by which resource requestors can assess the reliability of perspective providers before initiating the download . The approach nicely complements the existing P2P protocols and has a limited impact on current implementations . Furthermore , it keeps the current level of anonymity of requestors and providers , as well as that of the parties sharing their view on others ' reputations . \",\n",
       " 125048: 'Named graphs , provenance and trust The Semantic Web consists of many RDF graphs nameable by URIs . This paper extends the syntax and semantics of RDF to cover such Named Graphs . This enables RDF statements that describe graphs , which is beneficial in many Semantic Web application areas . As a case study , we explore the application area of Semantic Web publishing : Named Graphs allow publishers to communicate assertional intent , and to sign their graphs ; information consumers can evaluate specific graphs using task-specific trust policies , and act on information from those Named Graphs that they accept . Graphs are trusted depending on : their content ; information about the graph ; and the task the user is performing . The extension of RDF to Named Graphs provides a formally defined framework to be a foundation for the Semantic Web trust layer . ',\n",
       " 1267363: 'Cooperative leases : scalable consistency maintenance in content distribution networks In this paper , we argue that cache consistency mechanisms designed for stand-alone proxies do not scale to the large number of proxies in a content distribution network and are not flexible enough to allow consistency guarantees to be tailored to object needs . To meet the twin challenges of scalability and flexibility , we introduce the notion of cooperative consistency along with a mechanism , called cooperative leases , to achieve it . By supporting & Dgr ; - consistency semantics and by using a single lease for multiple proxies , cooperative leases allows the notion of leases to be applied in a flexible , scalable manner to CDNs . Further , the approach employs application-level multicast to propagate server notifications to proxies in a scalable manner . We implement our approach in the Apache web server and the Squid proxy cache and demonstrate its efficacy using a detailed experimental evaluation . Our results show a factor of 2.5 reduction in server message overhead and a 20 % reduction in server state space overhead when compared to original leases albeit at an increased inter-proxy communication overhead . ',\n",
       " 1275176: 'Extracting query modifications from nonlinear SVMs When searching the WWW , users often desire results restricted to a particular document category . Ideally , a user would be able to filter results with a text classifier to minimize false positive results ; however , current search engines allow only simple query modifications . To automate the process of generating effective query modifications , we introduce a sensitivity analysis-based method for extracting rules from nonlinear support vector machines . The proposed method allows the user to specify a desired precision while attempting to maximize the recall . Our method performs several levels of dimensionality reduction and is vastly faster than searching the combination feature space ; moreover , it is very effective on real-world data . ',\n",
       " 12995751: 'Influence and passivity in social media The ever-increasing amount of information flowing through Social Media forces the members of these networks to compete for attention and influence by relying on other people to spread their message . A large study of information propagation within Twitter reveals that the majority of users act as passive information consumers and do not forward the content to the network . Therefore , in order for individuals to become influential they must not only obtain attention and thus be popular , but also overcome user passivity . We propose an algorithm that determines the influence and passivity of users based on their information forwarding activity . An evaluation performed with a 2.5 million user dataset shows that our influence measure is a good predictor of URL clicks , outperforming several other measures that do not explicitly take user passivity into account . We demonstrate that high popularity does not necessarily imply high influence and vice-versa . ',\n",
       " 13005831: 'Recommendations for the long tail by term-query graph We define a new approach to the query recommendation problem . In particular , our main goal is to design a model enabling the generation of query suggestions also for rare and previously unseen queries . In other words we are targeting queries in the long tail . The model is based on a graph having two sets of nodes : Term nodes , and Query nodes . The graph induces a Markov chain on which a generic random walker starts from a subset of Term nodes , moves along Query nodes , and restarts ( with a given probability ) only from the same initial subset of Term nodes . Computing the stationary distribution of such a Markov chain is equivalent to extracting the so-called Center-piece Subgraph from the graph associated with the Markov chain itself . Given a query , we extract its terms and we set the restart subset to this term set . Therefore , we do not require a query to have been previously observed for the recommending model to be able to generate suggestions . ',\n",
       " 13009292: \"An xpath-based discourse analysis module for spoken dialogue systems This paper describes an XPath-based discourse analysis module for Spoken Dialogue Systems that allows the dialogue author to easily manipulate and query both the user input 's semantic representation and the dialogue context using a simple and compact formalism . We show that , in managing the human-machine interaction , the discourse context and the dialogue history are effectively represented as Document Object Model ( DOM ) structures . DOM defines interfaces that dialogue scripts can use to dynamically access and update the content , the structure and the style of the documents . In general , this approach applies also to richer multimedia and multimodal interactions where the interpretation of the user input depends on a combination of input modalitie . \",\n",
       " 13023730: 'Anonymizing user profiles for personalized web search We study the problem of anonymizing user profiles so that user privacy is sufficiently protected while the anonymized profiles are still effective in enabling personalized web search . We propose a Bayes-optimal privacy notion to bound the prior and posterior probability of associating a user with an individual term in the anonymized user profile set . We also propose a novel bundling technique that clusters user profiles into groups by taking into account the semantic relationships between the terms while satisfying the privacy constraint . We evaluate our approach through a set of preliminary experiments using real data demonstrating its feasibility and effectiveness . ',\n",
       " 13058954: \"Context-sensitive query auto-completion Query auto completion is known to provide poor predictions of the user 's query when her input prefix is very short ( e.g. , one or two characters ) . In this paper we show that context , such as the user 's recent queries , can be used to improve the prediction quality considerably even for such short prefixes . We propose a context-sensitive query auto completion algorithm , NearestCompletion , which outputs the completions of the user 's input that are most similar to the context queries . To measure similarity , we represent queries and contexts as high-dimensional term-weighted vectors and resort to cosine similarity . The mapping from queries to vectors is done through a new query expansion technique that we introduce , which expands a query by traversing the query recommendation tree rooted at the query . In order to evaluate our approach , we performed extensive experimentation over the public AOL query log . We demonstrate that when the recent user 's queries are relevant to the current query she is typing , then after typing a single character , NearestCompletion 's MRR is 48 % higher relative to the MRR of the standard MostPopularCompletion algorithm on average . When the context is irrelevant , however , NearestCompletion 's MRR is essentially zero . To mitigate this problem , we propose HybridCompletion , which is a hybrid of NearestCompletion with MostPopularCompletion . HybridCompletion is shown to dominate both NearestCompletion and MostPopularCompletion , achieving a total improvement of 31.5 % in MRR relative to MostPopularCompletion on average . \",\n",
       " 13058990: \"Unsupervised query segmentation using only query logs We introduce an unsupervised query segmentation scheme that uses query logs as the only resource and can effectively capture the structural units in queries . We believe that Web search queries have a unique syntactic structure which is distinct from that of English or a bag-of-words model . The segments discovered by our scheme help understand this underlying grammatical structure . We apply a statistical model based on Hoeffding 's Inequality to mine significant word n-grams from queries and subsequently use them for segmenting the queries . Evaluation against manually segmented queries shows that this technique can detect rare units that are missed by our Pointwise Mutual Information ( PMI ) baseline . \",\n",
       " 13059029: 'The web of topics : discovering the topology of topic evolution in a corpus In this paper we study how to discover the evolution of topics over time in a time-stamped document collection . Our approach is uniquely designed to capture the rich topology of topic evolution inherent in the corpus . Instead of characterizing the evolving topics at fixed time points , we conceptually define a topic as a quantized unit of evolutionary change in content and discover topics with the time of their appearance in the corpus . Discovered topics are then connected to form a topic evolution graph using a measure derived from the underlying document network . Our approach allows inhomogeneous distribution of topics over time and does not impose any topological restriction in topic evolution graphs . We evaluate our algorithm on the ACM corpus . The topic evolution graphs obtained from the ACM corpus provide an effective and concrete summary of the corpus with remarkably rich topology that are congruent to our background knowledge . In a finer resolution , the graphs reveal concrete information about the corpus that were previously unknown to us , suggesting the utility of our approach as a navigational tool for the corpus . ',\n",
       " 13059042: 'Ranking related entities for web search queries Entity ranking is a recent paradigm that refers to retrieving and ranking related objects and entities from different structured sources in various scenarios . Entities typically have associated categories and relationships with other entities . In this work , we present an extensive analysis of Web-scale entity ranking , based on machine learned ranking models using an ensemble of pairwise preference models . Our proposed system for entity ranking uses structured knowledge bases , entity relationship graphs and user data to derive useful features to facilitate semantic search with entities directly within the learning to rank framework . The experimental results are validated on a large-scale graph containing millions of entities and hundreds of millions of entity relationships . We show that our proposed ranking solution clearly improves a simple user behavior based ranking model . ',\n",
       " 13059044: \"Efficient diversification of search results using query logs We study the problem of diversifying search results by exploiting the knowledge mined from query logs . Our proposal exploits the presence of different `` specializations '' of queries in query logs to detect the submission of ambiguous\\\\/faceted queries , and manage them by diversifying the search results returned in order to cover the different possible interpretations of the query . We present an original formulation of the results diversification problem in terms of an objective function to be maximized that admits the finding of an optimal solution in linear time . \",\n",
       " 13059085: 'Layered label propagation : a multiresolution coordinate-free ordering for compressing social networks We continue the line of research on graph compression started with WebGraph , but we move our focus to the compression of social networks in a proper sense ( e.g. , LiveJournal ) : the approaches that have been used for a long time to compress web graphs rely on a specific ordering of the nodes ( lexicographical URL ordering ) whose extension to general social networks is not trivial . In this paper , we propose a solution that mixes clusterings and orders , and devise a new algorithm , called Layered Label Propagation , that builds on previous work on scalable clustering and can be used to reorder very large graphs ( billions of nodes ) . Our implementation uses task decomposition to perform aggressively on multi-core architecture , making it possible to reorder graphs of more than 600 millions nodes in a few hours . Experiments performed on a wide array of web graphs and social networks show that combining the order produced by the proposed algorithm with the WebGraph compression framework provides a major increase in compression with respect to all currently known techniques , both on web graphs and on social networks . These improvements make it possible to analyze in main memory significantly larger graphs . ',\n",
       " 13059173: 'Leveraging auxiliary text terms for automatic image annotation This paper proposes a novel algorithm to annotate web images by automatically aligning the images with their most relevant auxiliary text terms . First , the DOM-based web page segmentation is performed to extract images and their most relevant auxiliary text blocks . Second , automatic image clustering is used to partition the web images into a set of groups according to their visual similarity contexts , which significantly reduces the uncertainty on the relatedness between the images and their auxiliary terms . The semantics of the visually-similar images in the same cluster are then described by the same ranked list of terms which frequently co-occur in their text blocks . Finally , a relevance re-ranking process is performed over a term correlation network to further refine the ranked term list . Our experiments on a large-scale database of web pages have provided very positive results . ',\n",
       " 13059270: 'Accelerating instant question search with database techniques Distributed question answering services , like Yahoo Answer and Aardvark , are known to be useful for end users and have also opened up numerous topics ranging in many research fields . In this paper , we propose a user-support tool for composing questions in such services . Our system incrementally recommends similar questions while users are typing their question in a sentence , which gives the users opportunities to know that there are similar questions that have already been solved . A question database is semantically analyzed and searched in the semantic space by boosting the performance of similarity searches with database techniques such as server\\\\/client caching and LSH ( Locality Sensitive Hashing ) . The more text the user enters , the more similar the recommendations will become to the ultimately desired question . This unconscious editing-as-a-sequence-of-searches approach helps users to form their question incrementally through interactive supplementary information . Not only askers nor repliers , but also service providers have advantages such as that the knowledge of the service will be autonomously refined by avoiding for novice users to repeat questions which have been already solved . ',\n",
       " 13059318: 'EP-SPARQL : a unified language for event processing and stream reasoning Streams of events appear increasingly today in various Web applications such as blogs , feeds , sensor data streams , geospatial information , on-line financial data , etc. . Event Processing ( EP ) is concerned with timely detection of compound events within streams of simple events . State-of-the-art EP provides on-the-fly analysis of event streams , but can not combine streams with background knowledge and can not perform reasoning tasks . On the other hand , semantic tools can effectively handle background knowledge and perform reasoning thereon , but can not deal with rapidly changing data provided by event streams . To bridge the gap , we propose Event Processing SPARQL ( EP-SPARQL ) as a new language for complex events and Stream Reasoning . We provide syntax and formal semantics of the language and devise an effective execution model for the proposed formalism . The execution model is grounded on logic programming , and features effective event processing and inferencing capabilities over temporal and static knowledge . We provide an open-source prototype implementation and present a set of tests to show the usefulness and effectiveness of our approach . ',\n",
       " 13059382: 'Dynamics of bidding in a P2P lending service : effects of herding and predicting loan success Online peer-to-peer ( P2P ) lending services are a new type of social platform that enables individuals borrow and lend money directly from one to another . In this paper , we study the dynamics of bidding behavior in a P2P loan auction website , Prosper.com . We investigate the change of various attributes of loan requesting listings over time , such as the interest rate and the number of bids . We observe that there is herding behavior during bidding , and for most of the listings , the numbers of bids they receive reach spikes at very similar time points . We explain these phenomena by showing that there are economic and social factors that lenders take into account when deciding to bid on a listing . We also observe that the profits the lenders make are tied with their bidding preferences . Finally , we build a model based on the temporal progression of the bidding , that reliably predicts the success of a loan request listing , as well as whether a loan will be paid back or not . ',\n",
       " 13059388: 'Standing on the shoulders of ants : stigmergy in the web Stigmergy is a biological term used when discussing insect or swarm behavior , and describes a model supporting environmental communication separately from artefacts or agents . This phenomenon is demonstrated in the behavior of ants and their food gathering process when following pheromone trails , or similarly termites and their termite mound building process . What is interesting with this mechanism is that highly organized societies are achieved without an apparent management structure . Stigmergic behavior is implicit in the Web where the volume of users provides a self-organizing and self-contextualization of content in sites which facilitate collaboration . However , the majority of content is generated by a minority of the Web participants . A significant contribution from this research would be to create a model of Web stigmergy , identifying virtual pheromones and their importance in the collaborative process . This paper explores how exploiting stigmergy has the potential of providing a valuable mechanism for identifying and analyzing online user behavior recording actionable knowledge otherwise lost in the existing web interaction dynamics . Ultimately this might assist our building better collaborative Web sites . ',\n",
       " 13059401: \"CoSi : context-sensitive keyword query interpretation on RDF databases The demo will present CoSi , a system that enables context-sensitive interpretation of keyword queries on RDF databases . The techniques for representing , managing and exploiting query history are central to achieving this objective . The demonstration will show the effectiveness of our approach for capturing a user 's querying context from their query history . Further , it will show how context is utilized to influence the interpretation of a new query . The demonstration is based on DBPedia , the RDF representation of Wikipedia . \",\n",
       " 13059585: 'Measurement and analysis of cyberlocker services Cyberlocker Services ( CLS ) such as RapidShare and Megaupload have recently become popular . The decline of Peer-to-Peer ( P2P ) file sharing has prompted various services including CLS to replace it . We propose a comprehensive multi-level characterization of the CLS ecosystem . We answer three research questions : ( a ) what is a suitable measurement infrastructure for gathering CLS workloads ; ( b ) what are the characteristics of the CLS ecosystem ; and ( c ) what are the implications of CLS on Web 2.0 ( and the Internet ) . To the best of our knowledge , this work is the first to characterize the CLS ecosystem . The work will highlight the content , usage , performance , infrastructure , quality of service , and evolution characteristics of CLS . ',\n",
       " 13059663: \"EntityTagger : automatically tagging entities with descriptive phrases We consider the problem of entity tagging : given one or more named entities from a specific domain , the goal is to automatically associate descriptive phrases , referred to as etags ( entity tags ) , to each entity . Consider a product catalog containing product names and possibly short descriptions . For a product in the catalog , say Ricoh G600 Digital Camera , we want to associate etags such as `` water resistant '' , `` rugged '' and `` outdoor '' to it , even though its name or description does not mention those phrases . Entity tagging can enable more effective search over entities . We propose to leverage signals in web documents to perform such tagging . We develop techniques to perform such tagging in a domain independent manner while ensuring high precision and high recall . \",\n",
       " 13059781: \"A self organizing document map algorithm for large scale hyperlinked data inspired by neuronal migration Web document clustering is one of the research topics that is being pursued continuously due to the large variety of applications . Since Web documents usually have variety and diversity in terms of domains , content and quality , one of the technical difficulties is to find a reasonable number and size of clusters . In this research , we pay attention to SOMs ( Self Organizing Maps ) because of their capability of visualized clustering that helps users to investigate characteristics of data in detail . The SOM is widely known as a `` scalable '' algorithm because of its capability to handle large numbers of records . However , it is effective only when the vectors are small and dense . Although several research efforts on making the SOM scalable have been conducted , technical issues on scalability and performance for sparse high-dimensional data such as hyperlinked documents still remain . In this paper , we introduce MIGSOM , an SOM algorithm inspired by a recent discovery on neuronal migration . The two major advantages of MIGSOM are its scalability for sparse high-dimensional data and its clustering visualization functionality . In this paper , we describe the algorithm and implementation , and show the practicality of the algorithm by applying MIGSOM to a huge scale real data set : Wikipedia 's hyperlink data . \",\n",
       " 13059898: \"Enhancing web search with entity intent Web entities , such as documents and hyperlinks , are created for different purposes , or intents . Existing intent-based retrieval methods largely focus on information seekers ' intent expressed by queries , ignoring the other side of the problem : web content creators ' intent . We argue that understanding why the content was created is also important . In this work , we propose to classify such intents into two broad categories : `` navigational '' and `` informational '' . Then we incorporate such intents into traditional retrieval models , and show their effect on ranking performance . \",\n",
       " 13059932: \"SCAD : collective discovery of attribute values Search engines today offer a rich user experience , no longer restricted to `` ten blue links '' . For example , the query `` Canon EOS Digital Camera '' returns a photo of the digital camera , and a list of suitable merchants and prices . Similar results are offered in other domains like food , entertainment , travel , etc. . All these experiences are fueled by the availability of structured data about the entities of interest . To obtain this structured data , it is necessary to solve the following problem : given a category of entities with its schema , and a set of Web pages that mention and describe entities belonging to the category , build a structured representation for the entity under the given schema . Specifically , collect structured numerical or discrete attributes of the entities . Most previous approaches regarded this as an information extraction problem on individual documents , and made no special use of numerical attributes . In contrast , we present an end-to-end framework which leverages signals not only from the Web page context , but also from a collective analysis of all the pages corresponding to an entity , and from constraints related to the actual values within the domain . Our current implementation uses a general and flexible Integer Linear Program ( ILP ) to integrate all these signals into holistic decisions over all attributes . There is one ILP per entity and it is small enough to be solved in under 38 milliseconds in our experiments . We apply the new framework to a setting of significant practical importance : catalog expansion for Commerce search engines , using data from Bing Shopping . Finally , we present experiments that validate the effectiveness of the framework and its superiority to local extraction . \",\n",
       " 13060051: 'Open and decentralized access across location-based services Users now interact with multiple Location-Based Services ( LBS ) through a myriad set of location-aware devices and interfaces . However , current LBS tend to be centralized silos with ad-hoc APIs , which limits potential for information sharing and reuse . Further , LBS subscriptions and user experiences are not easily portable across devices . We propose a general architecture for providing open and decentralized access to LBS , based on Tiled Feeds - a RESTful protocol for access and interactions with LBS using feeds , and Feed Subscription Management ( FSM ) - a generalized feed-based service management protocol . We describe two client designs , and demonstrate how they enable standardized access to LBS services , promote information sharing and mashup creation , and offer service management across various types of location-enabled devices . ',\n",
       " 13060083: 'Game theoretic models for social network analysis The existing methods and techniques for social network analysis are inadequate to capture both the behavior ( such as rationality and intelligence ) of individuals and the strategic interactions that occur among these individuals . Game theory is a natural tool to overcome this inadequacy since it provides rigorous mathematical models of strategic interaction among autonomous , intelligent , and rational agents . Motivated by the above observation , this tutorial provides the conceptual underpinnings of the use of game theoretic models in social network analysis . In the first part of the tutorial , we provide rigorous foundations of relevant concepts in game theory and social network analysis . In the second part of the tutorial , we present a comprehensive study of four contemporary and pertinent problems in social networks : social network formation , determining in influential individuals for viral marketing , query incentive networks , and community detection . ',\n",
       " 13060159: 'Comparative study of clustering techniques for short text documents We compare various document clustering techniques including K-means , SVD-based method and a graph-based approach and their performance on short text data collected from Twitter . We define a measure for evaluating the cluster error with these techniques . Observations show that graph-based approach using affinity propagation performs best in clustering short text data with minimal cluster error . ',\n",
       " 13060226: 'Pay as you browse : microcomputations as micropayments in web-based services Currently , several online businesses deem that advertising revenues alone are not sufficient to generate profits and are therefore set to charge for online content . In this paper , we explore a complement to the current advertisement model ; more specifically , we propose a micropayment model for non-specialized commodity web-services based on microcomputations . In our model , a user that wishes to access online content offered by a website does not need to register or pay to access the website ; instead , he will accept to run microcomputations on behalf of the website in exchange for access to the content . These microcomputations can , for example , support ongoing computing projects that have clear social benefits ( e.g. , projects relating to HIV , dengue , cancer , etc. ) or can contribute towards commercial computing projects . We argue that this micropayment model is economically and technically viable and that it can be integrated in existing distributed computing frameworks ( e.g. , the BOINC platform ) . We implement a preliminary prototype of a system based on our model through which we evaluate its performance and usability . Finally , we analyze the security and privacy of our proposal and we show that it ensures payment for the content while preserving the privacy of users . ',\n",
       " 13060384: \"Towards liquid service oriented architectures The advent of Cloud computing platforms , and the growing pervasiveness of Multicore processor architectures have revealed the inadequateness of traditional programming models based on sequential computations , opening up many challenges for research on parallel programming models for building distributed , service-oriented systems . More in detail , the dynamic nature of Cloud computing and its virtualized infrastructure pose new challenges in term of application design , deployment and dynamic reconfiguration . An application developed to be delivered as a service in the Cloud has to deal with poorly understood issues such as elasticity , infinite scalability and portability across heterogeneous virtualized environments . In this position paper we define the problem of providing a novel parallel programming model for building application services that can be transparently deployed on multicore and cloud execution environments . To this end , we introduce and motivate a research plan for the definition of a novel programming framework for Web service-based applications . Our vision called `` Liquid Architecture '' is based on a programming model inspired by core ideas tied to the REST architectural style coupled with a self-configuring runtime that allows transparent deployment of Web services on a broad range of heterogeneous platforms , from multicores to clouds . \",\n",
       " 13060390: 'Geographical topic discovery and comparison This paper studies the problem of discovering and comparing geographical topics from GPS-associated documents . GPS-associated documents become popular with the pervasiveness of location-acquisition technologies . For example , in Flickr , the geo-tagged photos are associated with tags and GPS locations . In Twitter , the locations of the tweets can be identified by the GPS locations from smart phones . Many interesting concepts , including cultures , scenes , and product sales , correspond to specialized geographical distributions . In this paper , we are interested in two questions : ( 1 ) how to discover different topics of interests that are coherent in geographical regions ? ( 2 ) how to compare several topics across different geographical locations ? To answer these questions , this paper proposes and compares three ways of modeling geographical topics : location-driven model , text-driven model , and a novel joint model called LGTA ( Latent Geographical Topic Analysis ) that combines location and text . To make a fair comparison , we collect several representative datasets from Flickr website including Landscape , Activity , Manhattan , National park , Festival , Car , and Food . The results show that the first two methods work in some datasets but fail in others . LGTA works well in all these datasets at not only finding regions of interests but also providing effective comparisons of the topics across different locations . The results confirm our hypothesis that the geographical distributions can help modeling topics , while topics provide important cues to group different geographical regions . ',\n",
       " 13060588: \"Factal : integrating deep web based on trust and relevance We demonstrate `` Factal '' -- a system for integrating deep web sources . Factal is based on the recently introduced source selection method `` SourceRank '' ; which is a measure of trust and relevance based on the agreement between the sources . SourceRank selects popular and trustworthy sources from autonomous and open collections like the deep web . This trust and popularity awareness distinguishes Factal from the existing systems like Google Product Search . Factal selects and searches active online databases on multiple domains . The demonstration scenarios include improved trustworthiness , relevance of results , and comparison shopping . We believe that by incorporating effective source selection based on the SourceRank , Factal demonstrates a significant step towards a deep-web-scale integration system . \",\n",
       " 13060608: \"Automatic construction of a context-aware sentiment lexicon : an optimization approach The explosion of Web opinion data has made essential the need for automatic tools to analyze and understand people 's sentiments toward different topics . In most sentiment analysis applications , the sentiment lexicon plays a central role . However , it is well known that there is no universally optimal sentiment lexicon since the polarity of words is sensitive to the topic domain . Even worse , in the same domain the same word may indicate different polarities with respect to different aspects . For example , in a laptop review , `` large '' is negative for the battery aspect while being positive for the screen aspect . In this paper , we focus on the problem of learning a sentiment lexicon that is not only domain specific but also dependent on the aspect in context given an unlabeled opinionated text collection . We propose a novel optimization framework that provides a unified and principled way to combine different sources of information for learning such a context-dependent sentiment lexicon . Experiments on two data sets ( hotel reviews and customer feedback surveys on printers ) show that our approach can not only identify new sentiment words specific to the given domain but also determine the different polarities of a word depending on the aspect in context . In further quantitative evaluation , our method is proved to be effective in constructing a high quality lexicon by comparing with a human annotated gold standard . In addition , using the learned context-dependent sentiment lexicon improved the accuracy in an aspect-level sentiment classification task . \",\n",
       " 13060664: \"A word at a time : computing word relatedness using temporal semantic analysis Computing the degree of semantic relatedness of words is a key functionality of many language applications such as search , clustering , and disambiguation . Previous approaches to computing semantic relatedness mostly used static language resources , while essentially ignoring their temporal aspects . We believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time . Consider , for instance , a newspaper archive spanning many years . Two words such as `` war '' and `` peace '' might rarely co-occur in the same articles , yet their patterns of use over time might be similar . In this paper , we propose a new semantic relatedness model , Temporal Semantic Analysis ( TSA ) , which captures this temporal information . The previous state of the art method , Explicit Semantic Analysis ( ESA ) , represented word semantics as a vector of concepts . TSA uses a more refined representation , where each concept is no longer scalar , but is instead represented as time series over a corpus of temporally-ordered documents . To the best of our knowledge , this is the first attempt to incorporate temporal evidence into models of semantic relatedness . Empirical evaluation shows that TSA provides consistent improvements over the state of the art ESA results on multiple benchmarks . \",\n",
       " 13060669: \"We know who you followed last summer : inferring social link creation times in twitter Understanding a network 's temporal evolution appears to require multiple observations of the graph over time . These often expensive repeated crawls are only able to answer questions about what happened from observation to observation , and not what happened before or between network snapshots . Contrary to this picture , we propose a method for Twitter 's social network that takes a single static snapshot of network edges and user account creation times to accurately infer when these edges were formed . This method can be exact in theory , and we demonstrate empirically for a large subset of Twitter relationships that it is accurate to within a few hours in practice . We study users who have a very large number of edges or who are recommended by Twitter . We examine the graph formed by these nearly 1,800 Twitter celebrities and their 862 million edges in detail , showing that a single static snapshot can give novel insights about Twitter 's evolution . We conclude from this analysis that real-world events and changes to Twitter 's interface for recommending users strongly influence network growth . \",\n",
       " 13060679: \"Characterizing search intent diversity into click models Modeling a user 's click-through behavior in click logs is a challenging task due to the well-known position bias problem . Recent advances in click models have adopted the examination hypothesis which distinguishes document relevance from position bias . In this paper , we revisit the examination hypothesis and observe that user clicks can not be completely explained by relevance and position bias . Specifically , users with different search intents may submit the same query to the search engine but expect different search results . Thus , there might be a bias between user search intent and the query formulated by the user , which can lead to the diversity in user clicks . This bias has not been considered in previous works such as UBM , DBN and CCM . In this paper , we propose a new intent hypothesis as a complement to the examination hypothesis . This hypothesis is used to characterize the bias between the user search intent and the query in each search session . This hypothesis is very general and can be applied to most of the existing click models to improve their capacities in learning unbiased relevance . Experimental results demonstrate that after adopting the intent hypothesis , click models can better interpret user clicks and achieve a significant NDCG improvement . \",\n",
       " 13060796: 'Sentence-level contextual opinion retrieval Existing opinion retrieval techniques do not provide context-dependent relevant results . Most of the approaches used by state-of-the-art techniques are based on frequency of query terms , such that all documents containing query terms are retrieved , regardless of contextual relevance to the intent of the human seeking the opinion . However , in a particular opinionated document , words could occur in different contexts , yet meet the frequency attached to a certain opinion threshold , thus explicitly creating a bias in overall opinion retrieved . In this paper we propose a sentence-level contextual model for opinion retrieval using grammatical tree derivations and approval voting mechanism . Model evaluation performed between our contextual model , BM25 , and language model shows that the model can be effective for contextual opinion retrieval such as faceted opinion retrieval . ',\n",
       " 13060825: 'Statically locating web application bugs caused by asynchronous calls Ajax becomes more and more important for web applications that care about client side user experience . It allows sending requests asynchronously , without blocking clients from continuing execution . Callback functions are only executed upon receiving the responses . While such mechanism makes browsing a smooth experience , it may cause severe problems in the presence of unexpected network latency , due to the non-determinism of asynchronism . In this paper , we demonstrate the possible problems caused by the asynchronism and propose a static program analysis to automatically detect such bugs in web applications . As client side Ajax code is often wrapped in server-side scripts , we also develop a technique that extracts client-side JavaScript code from server-side scripts . We evaluate our technique on a number of real-world web applications . Our results show that it can effectively identify real bugs . We also discuss possible ways to avoid such bugs . ',\n",
       " 13060918: \"Embedding MindMap as a service for user-driven composition of web applications The World Wide Web is evolving towards a very large distributed platform allowing ubiquitous access to a wide range of Web applications with minimal delay and no installation required . Such Web applications range from having users undertake simple tasks , such as filling a form , to more complex tasks including collaborative work , project management , and more generally , creating , consulting , annotating , and sharing Web content . However , users are lacking a simple but yet powerful mechanism to compose Web applications , similarly to what desktop environments allowed for decades using the file explorer paradigm and the desktop metaphor . Attempts have been made to adapt the desktop metaphor to the Web environment giving birth to Webtops ( Web desktops ) . It essentially consisted of embedding a desktop environment in a Web browser and provide access to various Web applications within the same User Interface . However , those attempts did not take into consideration to the radical differences between Web and desktop environments and applications . In this work , we introduce a new approach for Web application composition based on the mindmap metaphor . It allows browsing artifacts ( Web resources ) and enabling user-driven composition of their associated Web applications . Essentially , a mindmap is a graph of widgets representing artifacts created or used by Web applications and allow to list and launch all possible Web applications associated to each artifact . A tool has been developed to experiment the new metaphor and is provided as a service to be embedded in Web applications via a Web browser 's plug-in . We demonstrate in this paper three case studies regarding the DBLP Web site , Wikipedia and Google Picasa Web applications . \",\n",
       " 13061041: 'Parallel boosted regression trees for web search ranking Gradient Boosted Regression Trees ( GBRT ) are the current state-of-the-art learning paradigm for machine learned web-search ranking - a domain notorious for very large data sets . In this paper , we propose a novel method for parallelizing the training of GBRT . Our technique parallelizes the construction of the individual regression trees and operates using the master-worker paradigm as follows . The data are partitioned among the workers . At each iteration , the worker summarizes its data-partition using histograms . The master processor uses these to build one layer of a regression tree , and then sends this layer to the workers , allowing the workers to build histograms for the next layer . Our algorithm carefully orchestrates overlap between communication and computation to achieve good performance . Since this approach is based on data partitioning , and requires a small amount of communication , it generalizes to distributed and shared memory machines , as well as clouds . We present experimental results on both shared memory machines and clusters for two large scale web search ranking data sets . We demonstrate that the loss in accuracy induced due to the histogram approximation in the regression tree creation can be compensated for through slightly deeper trees . As a result , we see no significant loss in accuracy on the Yahoo data sets and a very small reduction in accuracy for the Microsoft LETOR data . In addition , on shared memory machines , we obtain almost perfect linear speed-up with up to about 48 cores on the large data sets . On distributed memory machines , we get a speedup of 25 with 32 processors . Due to data partitioning our approach can scale to even larger data sets , on which one can reasonably expect even higher speedups . ',\n",
       " 13061054: 'The freshman handbook : a hint for the server placement of social networks There has been a recent unprecedented increase in the use of Online Social Networks ( OSNs ) to expand our social life , exchange information and share common interests . Many popular OSNs today attract hundreds of millions of users who share tremendous amount of data on it such as Facebook , Twitter , and Buzz . Given the huge business opportunities OSNs may bring , more and more new social applications has emerged on the Internet . For these newcomers in the social network business , one of the first key decisions to make is to where to deploy the computational resources to best accommodate future client requests . In this work , we aim at providing useful suggests to the new born social network providers ( freshman ) on the intelligent server placement , by exploring available public information from existing social network communities . In this work , we first propose three scalable server placement strategies for OSNs . Our solution can scalably select server locations among all the possible locations , at the same time reducing the cost for inter-user data sharing . ',\n",
       " 13061186: 'Detecting group review spam It is well-known that many online reviews are not written by genuine users of products , but by spammers who write fake reviews to promote or demote some target products . Although some existing works have been done to detect fake reviews and individual spammers , to our knowledge , no work has been done on detecting spammer groups . This paper focuses on this task and proposes an effective technique to detect such groups . ',\n",
       " 13061206: 'Scalable integration and processing of linked data The goal of this tutorial is to introduce , motivate and detail techniques for integrating heterogeneous structured data from across the Web . Inspired by the growth in Linked Data publishing , our tutorial aims at educating Web researchers and practitioners about this new publishing paradigm . The tutorial will show how Linked Data enables uniform access , parsing and interpretation of data , and how this novel wealth of structured data can potentially be exploited for creating new applications or enhancing existing ones . As such , the tutorial will focus on Linked Data publishing and related Semantic Web technologies , introducing scalable techniques for crawling , indexing and automatically integrating structured heterogeneous Web data through reasoning . ',\n",
       " 13061384: \"Mobile search pattern evolution : the trend and the impact of voice queries In this paper we study the characteristics of search queries submitted from mobile devices using Yahoo ! Search for Mobile during a 2 months period in early of 2010 , and compare the results with a similar study conducted in late 2007 . The major findings include 1 ) mobile search queries have become much more diverse , and 2 ) user interest and information needs have been substantially changed at least in some areas of search topics , including adult and local intent queries . In addition we investigate the impact of voice query search interface offered by Yahoo ! 's mobile search service . We examine how unstructured spoken queries differ from conventional search queries . \",\n",
       " 13061550: \"Hierarchical organization of unstructured consumer reviews In this paper , we propose to organize the aspects of a specific product into a hierarchy by simultaneously taking advantages of domain structure knowledge as well as consumer reviews . Based on the derived hierarchy , we generate a hierarchical organization of the consumer reviews based on various aspects of the product , and aggregate consumer opinions on the aspects . With such hierarchical organization , people can easily grasp the overview of consumer reviews and opinions on various aspects , as well as seek consumer reviews and opinions on any specific aspect by navigating through the hierarchy . We conduct evaluation on two product review data sets : Liu et al. 's data set containing 314 reviews for five products ( 2 ) , and our review corpus which is collected from forum Web sites containing 60,786 reviews for five popular products . The experimental results demonstrate the effectiveness of our approach . \",\n",
       " 13061552: 'Finding influential mediators in social networks Given a social network , who are the key players controlling the bottlenecks of influence propagation if some persons would like to activate specific individuals ? In this paper , we tackle the problem of selecting a set of k mediator nodes as the influential gateways whose existence determines the activation probabilities of targeted nodes from some given seed nodes . We formally define the k-Mediators problem . To have an effective and efficient solution , we propose a three-step greedy method by considering the probabilistic influence and the structural connectivity on the pathways from sources to targets . To the best of our knowledge , this is the first work to consider the k-Mediators problem in networks . Experiments on the DBLP co-authorship graph show the effectiveness and efficiency of the proposed method . ',\n",
       " 13061595: 'Domain-independent entity extraction from web search query logs Query logs of a Web search engine have been increasingly used as a vital source for data mining . This paper presents a study on large-scale domain-independent entity extraction from search query logs . We present a completely unsupervised method to extract entities by applying pattern-based heuristics and statistical measures . We compare against existing techniques that use Web documents as well as search logs , and show that we improve over the state of the art . We also provide an in-depth qualitative analysis outlining differences and commonalities between these methods . ',\n",
       " 13061601: 'Track globally , deliver locally : improving content delivery networks by tracking geographic social cascades Providers such as YouTube offer easy access to multimedia content to millions , generating high bandwidth and storage demand on the Content Delivery Networks they rely upon . More and more , the diffusion of this content happens on online social networks such as Facebook and Twitter , where social cascades can be observed when users increasingly repost links they have received from others . In this paper we describe how geographic information extracted from social cascades can be exploited to improve caching of multimedia files in a Content Delivery Network . We take advantage of the fact that social cascades can propagate in a geographically limited area to discern whether an item is spreading locally or globally . This informs cache replacement policies , which utilize this information to ensure that content relevant to a cascade is kept close to the users who may be interested in it . We validate our approach by using a novel dataset which combines social interaction data with geographic information : we track social cascades of YouTube links over Twitter and build a proof-of-concept geographic model of a realistic distributed Content Delivery Network . Our performance evaluation shows that we are able to improve cache hits with respect to cache policies without geographic and social information . ',\n",
       " 13061675: 'Filtering microblogging messages for social tv Social TV was named one of the ten most important emerging technologies in 2010 by the MIT Technology Review . Manufacturers of set-top boxes and televisions have recently started to integrate access to social networks into their products . Some of these systems allow users to read microblogging messages related to the TV program they are currently watching . However , such systems suffer from low precision and recall when they use the title of the show as keywords when retrieving messages , without any additional filtering . We propose a bootstrapping approach to collecting microblogging messages related to a given TV program . We start with a small set of annotated data , in which , for a given show and a candidate message , we annotate the pair to be relevant or irrelevant . From this annotated data set , we train an initial classifier . The features are designed to capture the association between the TV program and the message . Using our initial classifier and a large dataset of unlabeled messages we derive broader features for a second classifier to further improve precision . ',\n",
       " 13061698: \"Learning to rank with multiple objective functions We investigate the problem of learning to rank with document retrieval from the perspective of learning for multiple objective functions . We present solutions to two open problems in learning to rank : first , we show how multiple measures can be combined into a single graded measure that can be learned . This solves the problem of learning from a ` scorecard ' of measures by making such scorecards comparable , and we show results where a standard web relevance measure ( NDCG ) is used for the top-tier measure , and a relevance measure derived from click data is used for the second-tier measure ; the second-tier measure is shown to significantly improve while leaving the top-tier measure largely unchanged . Second , we note that the learning-to-rank problem can itself be viewed as changing as the ranking model learns : for example , early in learning , adjusting the rank of all documents can be advantageous , but later during training , it becomes more desirable to concentrate on correcting the top few documents for each query . We show how an analysis of these problems leads to an improved , iteration-dependent cost function that interpolates between a cost function that is more appropriate for early learning , with one that is more appropriate for late-stage learning . The approach results in a significant improvement in accuracy with the same size models . We investigate these ideas using LambdaMART , a state-of-the-art ranking algorithm . \",\n",
       " 13061707: \"Information credibility on twitter We analyze the information credibility of news propagated through Twitter , a popular microblogging service . Previous research has shown that most of the messages posted on Twitter are truthful , but the service is also used to spread misinformation and false rumors , often unintentionally . On this paper we focus on automatic methods for assessing the credibility of a given set of tweets . Specifically , we analyze microblog postings related to `` trending '' topics , and classify them as credible or not credible , based on features extracted from them . We use features from users ' posting and re-posting ( `` re-tweeting '' ) behavior , from the text of the posts , and from citations to external sources . We evaluate our methods using a significant number of human assessments about the credibility of items on a recent sample of Twitter postings . Our results shows that there are measurable differences in the way messages propagate , that can be used to classify them automatically as credible or not credible , with precision and recall in the range of 70 % to 80 % . \",\n",
       " 13061722: \"Towards a theory model for product search With the growing pervasiveness of the Internet , online search for products and services is constantly increasing . Most product search engines are based on adaptations of theoretical models devised for information retrieval . However , the decision mechanism that underlies the process of buying a product is different than the process of locating relevant documents or objects . We propose a theory model for product search based on expected utility theory from economics . Specifically , we propose a ranking technique in which we rank highest the products that generate the highest surplus , after the purchase . In a sense , the top ranked products are the `` best value for money '' for a specific user . Our approach builds on research on `` demand estimation '' from economics and presents a solid theoretical foundation on which further research can build on . We build algorithms that take into account consumer demographics , heterogeneity of consumer preferences , and also account for the varying price of the products . We show how to achieve this without knowing the demographics or purchasing histories of individual consumers but by using aggregate demand data . We evaluate our work , by applying the techniques on hotel search . Our extensive user studies , using more than 15,000 user-provided ranking comparisons , demonstrate an overwhelming preference for the rankings generated by our techniques , compared to a large number of existing strong state-of-the-art baselines . \",\n",
       " 13061737: \"On the informativeness of cascade and intent-aware effectiveness measures The Maximum Entropy Method provides one technique for validating search engine effectiveness measures . Under this method , the value of an effectiveness measure is used as a constraint to estimate the most likely distribution of relevant documents under a maximum entropy assumption . This inferred distribution may then be compared to the actual distribution to quantify the `` informativeness '' of the measure . The inferred distribution may also be used to estimate values for other effectiveness measures . Previous work focused on traditional effectiveness measures , such as average precision . In this paper , we extend the Maximum Entropy Method to the newer cascade and intent-aware effectiveness measures by considering the dependency of the documents ranked in a results list . These measures are intended to reflect the novelty and diversity of search results in addition to the traditional relevance . Our results indicate that intent-aware measures based on the cascade model are informative in terms of both inferring actual distribution and predicting the values of other retrieval measures . \",\n",
       " 13061804: 'Online spelling correction for query completion In this paper , we study the problem of online spelling correction for query completions . Misspelling is a common phenomenon among search engines queries . In order to help users effectively express their information needs , mechanisms for automatically correcting misspelled queries are required . Online spelling correction aims to provide spell corrected completion suggestions as a query is incrementally entered . As latency is crucial to the utility of the suggestions , such an algorithm needs to be not only accurate , but also efficient . To tackle this problem , we propose and study a generative model for input queries , based on a noisy channel transformation of the intended queries . Utilizing spelling correction pairs , we train a Markov n-gram transformation model that captures user spelling behavior in an unsupervised fashion . To find the top spell-corrected completion suggestions in real-time , we adapt the A \\\\* search algorithm with various pruning heuristics to dynamically expand the search space efficiently . Evaluation of the proposed methods demonstrates a substantial increase in the effectiveness of online spelling correction over existing techniques . ',\n",
       " 13061836: 'Helix : online enterprise data analytics The size , heterogeneity and dynamicity of data within an enterprise makes indexing , integration and analysis of the data increasingly difficult tasks . On the other hand , there has been a massive increase in the amount of high-quality open data available on the Web that could provide invaluable insights to data analysts and business intelligence specialists within the enterprise . The goal of Helix project is to provide users within the enterprise with a platform that allows them to perform online analysis of almost any type and amount of internal data using the power of external knowledge bases available on the Web . Such a platform requires a novel , data-format agnostic indexing mechanism , and light-weight data linking techniques that could link semantically related records across internal and external data sources of various characteristics . We present the initial architecture of our system and discuss several research challenges involved in building such a system . ',\n",
       " 13061839: 'Truthy : mapping the spread of astroturf in microblog streams Online social media are complementing and in some cases replacing person-to-person social interaction and redefining the diffusion of information . In particular , microblogs have become crucial grounds on which public relations , marketing , and political battles are fought . We demonstrate a web service that tracks political memes in Twitter and helps detect astroturfing , smear campaigns , and other misinformation in the context of U.S. political elections . We also present some cases of abusive behaviors uncovered by our service . Our web service is based on an extensible framework that will enable the real-time analysis of meme diffusion in social media by mining , visualizing , mapping , classifying , and modeling massive streams of public microblogging events . ',\n",
       " 13061918: 'Measuring a commercial content delivery network Content delivery networks ( CDNs ) have become a crucial part of the modern Web infrastructure . This paper studies the performance of the leading content delivery provider - Akamai . It measures the performance of the current Akamai platform and considers a key architectural question faced by both CDN designers and their prospective customers : whether the co-location approach to CDN platforms adopted by Akamai , which tries to deploy servers in numerous Internet locations , brings inherent performance benefits over a more consolidated data center approach pursued by other influential CDNs such as Limelight . We believe the methodology we developed for this study will be useful for other researchers in the CDN arena . ',\n",
       " 13061924: 'Citizen sensor data mining , social media analytics and development centric web applications With the rapid rise in the popularity of social media ( 500M + Facebook users , 100M + twitter users ) , and near ubiquitous mobile access ( 4 + billion actively-used mobile phones ) , the sharing of observations and opinions has become common-place ( nearly 100M tweets a day , 1.8 trillion SMSs in US last year ) . This has given us an unprecedented access to the pulse of a populace and the ability to perform analytics on social data to support a variety of socially intelligent applications -- be it towards targeted online content delivery , crisis management , organizing revolutions or promoting social development in underdeveloped and developing countries . This tutorial will address challenges and techniques for building applications that support a broad variety of users and types of social media . This tutorial will focus on social intelligence applications for social development , and cover the following research efforts in sufficient depth : 1 ) understanding and analysis of informal text , esp . microblogs ( e.g. , issues of cultural entity extraction and role of semantic\\\\/background knowledge enhanced techniques ) , and 2 ) building social media analytics platforms . Technical insights will be coupled with identification of computational techniques and real-world examples . ',\n",
       " 13061972: \"Traffic characterization and internet usage in rural Africa While Internet connectivity has reached a significant part of the world 's population , those living in rural areas of the developing world are still largely disconnected . Recent efforts have provided Internet connectivity to a growing number of remote locations , yet Internet traffic demands cause many of these networks to fail to deliver basic quality of service needed for simple applications . For an in-depth investigation of the problem , we gather and analyze network traces from a rural wireless network in Macha , Zambia . We supplement our analysis with on-site interviews from Macha , Zambia and Dwesa , South Africa , another rural community that hosts a local wireless network . The results reveal that Internet traffic in rural Africa differs significantly from the developed world . We observe dominance of web-based traffic , as opposed to peer-to-peer traffic common in urban areas . Application-wise , online social networks are the most popular , while the majority of bandwidth is consumed by large operating system updates . Our analysis also uncovers numerous network anomalies , such as significant malware traffic . Finally , we find a strong feedback loop between network performance and user behavior . Based on our findings , we conclude with a discussion of new directions in network design that take into account both technical and social factors . \",\n",
       " 13061989: \"Learning to model relatedness for news recommendation With the explosive growth of online news readership , recommending interesting news articles to users has become extremely important . While existing Web services such as Yahoo ! and Digg attract users ' initial clicks by leveraging various kinds of signals , how to engage such users algorithmically after their initial visit is largely under-explored . In this paper , we study the problem of post-click news recommendation . Given that a user has perused a current news article , our idea is to automatically identify `` related '' news articles which the user would like to read afterwards . Specifically , we propose to characterize relatedness between news articles across four aspects : relevance , novelty , connection clarity , and transition smoothness . Motivated by this understanding , we define a set of features to capture each of these aspects and put forward a learning approach to model relatedness . In order to quantitatively evaluate our proposed measures and learn a unified relatedness function , we construct a large test collection based on a four-month commercial news corpus with editorial judgments . The experimental results show that the proposed heuristics can indeed capture relatedness , and that the learned unified relatedness function works quite effectively . \",\n",
       " 13062025: \"Model characterization curves for federated search using click-logs : predicting user engagement metrics for the span of feasible operating points Modern day federated search engines aggregate heterogeneous types of results from multiple vertical search engines and compose a single search engine result page ( SERP ) . The search engine aggregates the results and produces one ranked list , constraining the vertical results to specific slots on the SERP . The usual way to compare two ranking algorithms is to first fix their operating points ( internal thresholds ) , and then run an online experiment that lasts multiple weeks . Online user engagement metrics are then compared to decide which algorithm is better . However , this method does not characterize and compare the behavior over the entire span of operating points . Furthermore , this time-consuming approach is not practical if we have to conduct the experiment over numerous operating points . In this paper we propose a method of characterizing the performance of models that allows us to predict answers to `` what if '' questions about online user engagement using click-logs over the entire span of feasible operating points . We audition verticals at various slots on the SERP and generate click-logs . This log is then used to create operating curves between variables of interest ( for example between result quality and click-through ) . The operating point for the system then can be chosen to achieve a specific trade-off between the variables . We apply this methodology to predict i ) the online performance of two different models , ii ) the impact of changing internal quality thresholds on clickthrough , iii ) the behavior of introducing a new feature , iv ) which machine learning loss function will give better online engagement , v ) the impact of sampling distribution of head and tail queries in the training process . The results are reported on a well-known federated search engine . We validate the predictions with online experiments . \",\n",
       " 13062026: 'Towards semantic knowledge propagation from text corpus to web images In this paper , we study the problem of transfer learning from text to images in the context of network data in which link based bridges are available to transfer the knowledge between the different domains . The problem of classification of image data is often much more challenging than text data because of the following two reasons : ( a ) Labeled text data is very widely available for classification purposes . On the other hand , this is often not the case for image data , in which a lot of images are available from many sources , but many of them are often not labeled . ( b ) The image features are not directly related to semantic concepts inherent in class labels . On the other hand , since text data tends to have natural semantic interpretability ( because of their human origins ) , they are often more directly related to class labels . Therefore , the relationships between the images and text features also provide additional hints for the classification process in terms of the image feature transformations which provide the most effective results . The semantic challenges of image features are glaringly evident , when we attempt to recognize complex abstract concepts , and the visual features often fail to discriminate such concepts . However , the copious availability of bridging relationships between text and images in the context of web and social network data can be used in order to design for effective classifiers for image data . One of our goals in this paper is to develop a mathematical model for the functional relationships between text and image features , so as indirectly transfer semantic knowledge through feature transformations . This feature transformation is accomplished by mapping instances from different domains into a common space of unspecific topics . This is used as a bridge to semantically connect the two heterogeneous spaces . This is also helpful for the cases where little image data is available for the classification process . We evaluate our knowledge transfer techniques on an image classification task with labeled text corpora and show the effectiveness with respect to competing algorithms . ',\n",
       " 13062112: 'VoiSTV : voice-enabled social TV Until recently , the TV viewing experience has not been a very social activity compared to activities on the World Wide Web . In this work , we will present a Voice-enabled Social TV system ( VoiSTV ) which allows users to interact , follow and monitor the online social media messages related to a TV show while watching it . Users can create , send , and reply to messages using spoken language . VoiSTV also provides metadata information about TV shows such as trends , hot topics , popularity as well as aggregated sentiment of show-related messages , all of which are valuable for TV program search and recommendation . ',\n",
       " 13062121: \"A self-training approach for resolving object coreference on the semantic web An object on the Semantic Web is likely to be denoted with multiple URIs by different parties . Object coreference resolution is to identify `` equivalent '' URIs that denote the same object . Driven by the Linking Open Data ( LOD ) initiative , millions of URIs have been explicitly linked with owl : sameAs statements , but potentially coreferent ones are still considerable . Existing approaches address the problem mainly from two directions : one is based upon equivalence inference mandated by OWL semantics , which finds semantically coreferent URIs but probably omits many potential ones ; the other is via similarity computation between property-value pairs , which is not always accurate enough . In this paper , we propose a self-training approach for object coreference resolution on the Semantic Web , which leverages the two classes of approaches to bridge the gap between semantically coreferent URIs and potential candidates . For an object URI , we firstly establish a kernel that consists of semantically coreferent URIs based on owl : sameAs , ( inverse ) functional properties and ( max - ) cardinalities , and then extend such kernel iteratively in terms of discriminative property-value pairs in the descriptions of URIs . In particular , the discriminability is learned with a statistical measurement , which not only exploits key characteristics for representing an object , but also takes into account the matchability between properties from pragmatics . In addition , frequent property combinations are mined to improve the accuracy of the resolution . We implement a scalable system and demonstrate that our approach achieves good precision and recall for resolving object coreference , on both benchmark and large-scale datasets . \",\n",
       " 13062191: \"Improving recommendation for long-tail queries via templates The ability to aggregate huge volumes of queries over a large population of users allows search engines to build precise models for a variety of query-assistance features such as query recommendation , correction , etc. . Yet , no matter how much data is aggregated , the long-tail distribution implies that a large fraction of queries are rare . As a result , most query assistance services perform poorly or are not even triggered on long-tail queries . We propose a method to extend the reach of query assistance techniques ( and in particular query recommendation ) to long-tail queries by reasoning about rules between query templates rather than individual query transitions , as currently done in query-flow graph models . As a simple example , if we recognize that ` Montezuma ' is a city in the rare query `` Montezuma surf '' and if the rule ` city surf → beach has been observed , we are able to offer `` Montezuma beach '' as a recommendation , even if the two queries were never observed in a same session . We conducted experiments to validate our hypothesis , first via traditional small-scale editorial assessments but more interestingly via a novel automated large scale evaluation methodology . Our experiments show that general coverage can be relatively increased by 24 % using templates without penalizing quality . Furthermore , for 36 % of the 95M queries in our query flow graph , which have no out edges and thus could not be served recommendations , we can now offer at least one recommendation in 98 % of the cases . \",\n",
       " 13062261: 'Automatically generating labels based on unified click model Ground truth labels are one of the most important parts in many test collections for information retrieval . Each label , depicting the relevance between a query-document pair , is usually judged by a human , and this process is time-consuming and labor-intensive . Automatically Generating labels from click-through data has attracted increasing attention . In this paper , we propose a Unified Click Model to predict the multi-level labels , which aims at comprehensively considering the advantages of the Position Models and Cascade Models . Experiments show that the proposed click model outperforms the existing click models in predicting the multi-level labels , and could replace the labels judged by humans for test collections . ',\n",
       " 13062305: 'A non-syntactic approach for text sentiment classification with stopwords The present approach uses stopwords and the gaps that occur between successive stopwords - formed by contentwords - as features for sentiment classification . ',\n",
       " 13062337: 'DIDO : a disease-determinants ontology from web sources This paper introduces DIDO , a system providing convenient access to knowledge about factors involved in human diseases , automatically extracted from textual Web sources . The knowledge base is bootstrapped by integrating entities from hand-crafted sources like MeSH and OMIM . As these are short on relationships between dierent types of biomedical entities , DIDO employs flexible and robust pattern learning and constraint-based reasoning methods to automatically extract new relational facts from textual sources . These facts can then be iteratively added to the knowledge base . The result is a semantic graph of typed entities and relations between diseases , their symptoms , and their factors , with emphasis on environmental factors but covering also molecular determinants . We demonstrate the value of DIDO for knowledge discovery about causal factors and properties of complex diseases , including factor-disease chains . ',\n",
       " 13062396: 'A framework for evaluating network measures for functional importance Many metrics such as degree , closeness , and PageRank have been introduced to determine the relative importance of a node within a network . The desired function of a network , however , is domain-specific . For example , the robustness can be crucial for a communication network , while efficiency is more preferred for fast spreading of advertisements in viral marketing . The information provided by some widely used measures are often conflicting under such varying demands . In this paper , we present a novel framework for evaluating network metrics regarding typical functional requirements . We also propose an analysis of five well established measures to compare their performance of ranking nodes on functional importance in a real-life network . ',\n",
       " 13062406: \"Milgram-routing in social networks We demonstrate how a recent model of social networks ( `` Affiliation Networks '' , ( 21 ) ) offers powerful cues in local routing within social networks , a theme made famous by sociologist Milgram 's `` six degrees of separation '' experiments . This model posits the existence of an `` interest space '' that underlies a social network ; we prove that in networks produced by this model , not only do short paths exist among all pairs of nodes but natural local routing algorithms can discover them effectively . Specifically , we show that local routing can discover paths of length O ( log2 n ) to targets chosen uniformly at random , and paths of length O ( 1 ) to targets chosen with probability proportional to their degrees . Experiments on the co-authorship graph derived from DBLP data confirm our theoretical results , and shed light into the power of one step of lookahead in routing algorithms for social networks . \",\n",
       " 13062409: 'Low-infrastructure methods to improve internet access for mobile users in emerging regions As information technology supports more aspects of modern life , digital access has become an important tool for developing regions to lift themselves from poverty . Though broadband internet connectivity will not be universally available in the short-term , widely-employed mobile devices coupled with novel delay-tolerant networking do allow limited forms of connectivity . This paper explores the design space for internet access systems operating with constrained connectivity . Our starting point is C-LINK , a collaborative caching system that enhances the performance of interactive web access over DTN and cellular connectivity . We discuss our experiences and results from deploying C-LINK in Nicaragua , before moving on to a broader design study of other issues that further influence operation . We consider the impact of ( i ) storing web content collaboratively cached across all user nodes , ( ii ) hybrid transport layers exploiting the best attributes of limited cellular and DTN-style connectivity . We also explore the behavior of future systems under a range of usage and mobility scenarios . Even under adverse conditions , our techniques can improve average service latency for page requests by a factor of 2X . Our results point to the considerable power of leveraging user mobility and collaboration in providing very-low-infrastructure internet access to developing regions . ',\n",
       " 13062436: \"Buy-it-now or take-a-chance : a simple sequential screening mechanism We present a simple auction mechanism which extends the second-price auction with reserve and is truthful in expectation . This mechanism is particularly effective in private value environments where the distribution of valuations are irregular . Bidders can `` buy-it-now '' , or alternatively `` take-a-chance '' where the top d bidders are equally likely to win . The randomized take-a-chance allocation incentivizes high valuation bidders to buy-it-now . We show that for a large class of valuations , this mechanism achieves similar allocations and revenues as Myerson 's optimal mechanism , and outperforms the second-price auction with reserve . In addition , we present an evaluation of bid data from Microsoft 's AdECN platform . We find the valuations are irregular , and counterfactual experiments suggest our BIN-TAC mechanism would improve revenue by 11 % relative to an optimal second-price mechanism with reserve . \",\n",
       " 13062801: 'Second international workshop on RESTful design ( WS-REST 2011 ) Over the past few years , the discussion between the two major architectural styles for designing and implementing Web services , the RPC-oriented approach and the resource-oriented approach , has been mainly held outside of traditional research communities . Mailing lists , forums and developer communities have seen long and fascinating debates around the assumptions , strengths , and weaknesses of these two approaches . The Second International Workshop on RESTful Design ( WS-REST 2011 ) has the goal of getting more researchers involved in the debate by providing a forum where discussions around the resource-oriented style of Web services design take place . Representational State Transfer ( REST ) is an architectural style and as such can be applied in different ways , can be extended by additional constraints , or can be specialized with more specific interaction patterns . WS-REST is the premier forum for discussing research ideas , novel applications and results centered around REST at the World Wide Web conference , which provides a great setting to host this second edition of the workshop dedicated to research on the architectural style underlying the Web . ',\n",
       " 13062845: \"An adaptive ontology-based approach to identify correlation between publications In this paper , we propose an adaptive ontology-based approach for related paper identification , to meet most researchers ' practical needs . By searching ontology , we can return a diverse set of papers that are explicitly and implicitly related to an input paper . Moreover , our approach does not rely on known ontology . Instead , we build and update ontology for a collection with any domain of interest . Being independent from known ontology , our approach is much more adaptive for different domains . \",\n",
       " 13062852: 'Extracting events and event descriptions from Twitter This paper describes methods for automatically detecting events involving known entities from Twitter and understanding both the events as well as the audience reaction to them . We show that NLP techniques can be used to extract events , their main actors and the audience reactions with encouraging results . ',\n",
       " 13062911: 'Analysis and tracking of emotions in english and bengali texts : a computational approach The present discussion highlights the aspects of an ongoing doctoral thesis grounded on the analysis and tracking of emotions from English and Bengali texts . Development of lexical resources and corpora meets the preliminary urgencies . The research spectrum aims to identify the evaluative emotional expressions at word , phrase , sentence , and document level granularities along with their associated holders and topics . Tracking of emotions based on topic or event was carried out by employing sense based affect scoring techniques . The labeled emotion corpora are being prepared from unlabeled examples to cope with the scarcity of emotional resources , especially for the resource constraint language like Bengali . Different unsupervised , supervised and semi-supervised strategies , adopted for coloring each outline of the research spectrum produce satisfactory outcomes ',\n",
       " 13062962: 'Performance enhancement of scheduling algorithms in clusters and grids using improved dynamic load balancing techniques This paper describes the research work done for during PhD study . Cluster computing , grid computing and cloud computing are distributed computing environments ( DCEs ) widely accepted for the next generation Web based commercial and scientific applications . These applications work around the globally distributed data of petabyte scale that can only be processed by the aggregating the capability of globally distributed resources . The resource management and process scheduling in large scale distributed computing environment are a challenging task . In this research work we have devised new scheduling algorithms and resource management strategies specially designed for the cluster and grid cloud and peer-to-peer computing . The research work finally presented the distributed computing solutions to one scientific and one commercial application viz . e-Learning and data mining . ',\n",
       " 13063216: 'A study on the impact of product images on user clicks for online shopping In this paper we study the importance of image based features on the click-through rate ( CTR ) in the context of a large scale product search engine . Typically product search engines use text based features in their ranking function . We present a novel idea of using image based features , common in the photography literature , in addition to text based features . We used a stochastic gradient boosting based regression model to learn relationships between features and CTR . Our results indicate statistically significant correlations between the image features and CTR . We also see improvements to NDCG and mean standard regression . ',\n",
       " 13063239: \"Predicting popular messages in Twitter Social network services have become a viable source of information for users . In Twitter , information deemed important by the community propagates through retweets . Studying the characteristics of such popular messages is important for a number of tasks , such as breaking news detection , personalized message recommendation , viral marketing and others . This paper investigates the problem of predicting the popularity of messages as measured by the number of future retweets and sheds some light on what kinds of factors influence information propagation in Twitter . We formulate the task into a classification problem and study two of its variants by investigating a wide spectrum of features based on the content of the messages , temporal information , metadata of messages and users , as well as structural properties of the users ' social graph on a large scale dataset . We show that our method can successfully predict messages which will attract thousands of retweets with good performance . \",\n",
       " 13063418: \"ARROW : GenerAting SignatuRes to Detect DRive-By DOWnloads A drive-by download attack occurs when a user visits a webpage which attempts to automatically download malware without the user 's consent . Attackers sometimes use a malware distribution network ( MDN ) to manage a large number of malicious webpages , exploits , and malware executables . In this paper , we provide a new method to determine these MDNs from the secondary URLs and redirect chains recorded by a high-interaction client honeypot . In addition , we propose a novel drive-by download detection method . Instead of depending on the malicious content used by previous methods , our algorithm first identifies and then leverages the URLs of the MDN 's central servers , where a central server is a common server shared by a large percentage of the drive-by download attacks in the same MDN . A set of regular expression-based signatures are then generated based on the URLs of each central server . This method allows additional malicious webpages to be identified which launched but failed to execute a successful drive-by download attack . The new drive-by detection system named ARROW has been implemented , and we provide a large-scale evaluation on the output of a production drive-by detection system . The experimental results demonstrate the effectiveness of our method , where the detection coverage has been boosted by 96 % with an extremely low false positive rate . \",\n",
       " 13063481: 'Efficient k-nearest neighbor graph construction for generic similarity measures K-Nearest Neighbor Graph ( K-NNG ) construction is an important operation with many web related applications , including collaborative filtering , similarity search , and many others in data mining and machine learning . Existing methods for K-NNG construction either do not scale , or are specific to certain similarity measures . We present NN-Descent , a simple yet efficient algorithm for approximate K-NNG construction with arbitrary similarity measures . Our method is based on local search , has minimal space overhead and does not rely on any shared global index . Hence , it is especially suitable for large-scale applications where data structures need to be distributed over the network . We have shown with a variety of datasets and similarity measures that the proposed method typically converges to above 90 % recall with each point comparing only to several percent of the whole dataset on average . ',\n",
       " 13063484: \"Towards identifying arguments in Wikipedia pages Wikipedia is one of the most widely used repositories of human knowledge today , contributed mostly by a few hundred thousand regular editors . In this open environment , inevitably , differences of opinion arise among editors of the same article . Especially for polemical topics such as religion and politics , difference of opinions among editors may lead to intense `` edit wars '' in which editors compete to have their opinions and points of view accepted . While such disputes can compromise the reliability of the article ( or at least portions of it ) , they are recorded in the edit history of the articles . We posit that exposing such disputes to the reader , and pointing to the portions of the text where they manifest most prominently can be beneficial in helping concerned readers in understanding such topics . In this paper , we discuss our initial efforts towards the problem of automatic evaluation of extracting controversial points in Wikipedia pages . \",\n",
       " 13063564: \"Efficiently evaluating graph constraints in content-based publish\\\\/subscribe We introduce the problem of evaluating graph constraints in content-based publish\\\\/subscribe ( pub\\\\/sub ) systems . This problem formulation extends traditional content-based pub\\\\/sub systems in the following manner : publishers and subscribers are connected via a ( logical ) directed graph G with node and edge constraints , which limits the set of valid paths between them . Such graph constraints can be used to model a Web advertising exchange ( where there may be restrictions on how advertising networks can connect advertisers and publishers ) and content delivery problems in social networks ( where there may be restrictions on how information can be shared via the social graph ) . In this context , we develop efficient algorithms for evaluating graph constraints over arbitrary directed graphs G. We also present experimental results that demonstrate the effectiveness and scalability of the proposed algorithms using a realistic dataset from Yahoo ! 's Web advertising exchange . \",\n",
       " 13063596: 'Web-scale entity-relation search architecture Enabling entity search and ranking at Web-scale is fraught with many challenges : annotating the corpus with entities and types , query language design , index design , query processing logic , and answer consolidation . We describe a Web-scale entity search engine we are building to handle over a billion Web pages , over 200,000 types , over 1,500,000 entities , and hundreds of entity annotations per page . We describe the design of compressed , token span oriented indices for entity and type annotations . Our prototype demonstrates the practicality of Web-scale entity-relation search . ',\n",
       " 13063612: 'Identifying primary content from web pages and its application to web search ranking Web pages are usually highly structured documents . In some documents , content with different functionality is laid out in blocks , some merely supporting the main discourse . In other documents , there may be several blocks of unrelated main content . Indexing a web page as if it were a linear document can cause problems because of the diverse nature of its content . If the retrieval function treats all blocks of the web page equally without attention to structure , it may lead to irrelevant query matches . In this paper , we describe how content quality of different blocks of a web page can be utilized to improve a retrieval function . Our method is based on segmenting a web page into semantically coherent blocks and learning a predictor of segment content quality . We also describe how to use segment content quality estimates as weights in the BM25F formulation . Experimental results show our method improves relevance of retrieved results by as much as 4.5 % compared to BM25F that treats the body of a web page as a single section , and by a larger margin of over 9 % for difficult queries . ',\n",
       " 13063679: 'Einstein : physicist or vegetarian ? summarizing semantic type graphs for knowledge discovery The Web and , in particular , knowledge-sharing communities such as Wikipedia contain a huge amount of information encompassing disparate and diverse fields . Knowledge bases such as DBpedia or Yago represent the data in a concise and more structured way bearing the potential of bringing database tools to Web Search . The wealth of data , however , poses the challenge of how to retrieve important and valuable information , which is often intertwined with trivial and less important details . This calls for an efficient and automatic summarization method . In this demonstration proposal , we consider the novel problem of summarizing the information related to a given entity , like a person or an organization . To this end , we utilize the rich type graph that knowledge bases provide for each entity , and define the problem of selecting the best cost-restricted subset of types as summary with good coverage of salient properties . We propose a demonstration of our system which allows the user to specify the entity to summarize , an upper bound on the cost of the resulting summary , as well as to browse the knowledge base in a more simple and intuitive manner . ',\n",
       " 13063697: 'Rewriting queries on SPARQL views The problem of answering SPARQL queries over virtual SPARQL views is commonly encountered in a number of settings , including while enforcing security policies to access RDF data , or when integrating RDF data from disparate sources . We approach this problem by rewriting SPARQL queries over the views to equivalent queries over the underlying RDF data , thus avoiding the costs entailed by view materialization and maintenance . We show that SPARQL query rewriting combines the most challenging aspects of rewriting for the relational and XML cases : like the relational case , SPARQL query rewriting requires synthesizing multiple views ; like the XML case , the size of the rewritten query is exponential to the size of the query and the views . In this paper , we present the first native query rewriting algorithm for SPARQL . For an input SPARQL query over a set of virtual SPARQL views , the rewritten query resembles a union of conjunctive queries and can be of exponential size . We propose optimizations over the basic rewriting algorithm to ( i ) minimize each conjunctive query in the union ; ( ii ) eliminate conjunctive queries with empty results from evaluation ; and ( iii ) efficiently prune out big portions of the search space of empty rewritings . The experiments , performed on two RDF stores , show that our algorithms are scalable and independent of the underlying RDF stores . Furthermore , our optimizations have order of magnitude improvements over the basic rewriting algorithm in both the rewriting size and evaluation time . ',\n",
       " 13063699: 'Autopedia : automatic domain-independent Wikipedia article generation This paper proposes a general framework , named Autopedia , to generate high-quality wikipedia articles for given concepts in any domains , by automatically selecting the best wikipedia template consisting the sub-topics to organize the article for the input concept . Experimental results on 4,526 concepts validate the effectiveness of Autopedia , and the wikipedia template selection approach which takes into account both the template quality and the semantic relatedness between the input concept and its sibling concepts , performs the best . ',\n",
       " 13063758: \"Summarization of archived and shared personal photo collections The volume of personal photos hosted on photo archives and social sharing platforms has been increasing exponentially . It is difficult to get an overview of a large collection of personal photos without browsing though the entire database manually . In this research , we propose a framework to generate representative subset summaries from photo collections hosted on web archives or social networks . We define salient properties of an effective photo summary and model summarization as an optimization of these properties , given the size constraints . We also introduce metrics for evaluating photo summaries based on their information content and the ability to satisfy user 's information needs . Our experiments show that our summarization framework performs better than baseline algorithms . \",\n",
       " 13063830: 'SmartInt : using mined attribute dependencies to integrate fragmented web databases Many web databases can be seen as providing partial and overlapping information about entities in the world . To answer queries effectively , we need to integrate the information about the individual entities that are fragmented over multiple sources . At first blush this is just the inverse of traditional database normalization problem - rather than go from a universal relation to normalized tables , we want to reconstruct the universal relation given the tables ( sources ) . The standard way of reconstructing the entities will involve joining the tables . Unfortunately , because of the autonomous and decentralized way in which the sources are populated , they often do not have Primary Key - Foreign Key relations . While tables do share attributes , direct joins over these shared attributes can result in reconstruction of many spurious entities thus seriously compromising precision . We present a unified approach that supports intelligent retrieval over fragmented web databases by mining and using inter-table dependencies . Experiments with the prototype implementation , SmartInt , show that its retrieval strikes a good balance between precision and recall . ',\n",
       " 13063949: 'Video summarization via transferrable structured learning It is well-known that textual information such as video transcripts and video reviews can significantly enhance the performance of video summarization algorithms . Unfortunately , many videos on the Web such as those from the popular video sharing site YouTube do not have useful textual information . The goal of this paper is to propose a transfer learning framework for video summarization : in the training process both the video features and textual features are exploited to train a summarization algorithm while for summarizing a new video only its video features are utilized . The basic idea is to explore the transferability between videos and their corresponding textual information . Based on the assumption that video features and textual features are highly correlated with each other , we can transfer textual information into knowledge on summarization using video information only . In particular , we formulate the video summarization problem as that of learning a mapping from a set of shots of a video to a subset of the shots using the general framework of SVM-based structured learning . Textual information is transferred by encoding them into a set of constraints used in the structured learning process which tend to provide a more detailed and accurate characterization of the different subsets of shots . Experimental results show significant performance improvement of our approach and demonstrate the utility of textual information for enhancing video summarization . ',\n",
       " 13063954: \"Here , there , and everywhere : correlated online behaviors can lead to overestimates of the effects of advertising Measuring the causal effects of online advertising ( adfx ) on user behavior is important to the health of the WWW publishing industry . In this paper , using three controlled experiments , we show that observational data frequently lead to incorrect estimates of adfx . The reason , which we label `` activity bias , '' comes from the surprising amount of time-based correlation between the myriad activities that users undertake online . In Experiment 1 , users who are exposed to an ad on a given day are much more likely to engage in brand-relevant search queries as compared to their recent history for reasons that had nothing do with the advertisement . In Experiment 2 , we show that activity bias occurs for page views across diverse websites . In Experiment 3 , we track account sign-ups at a competitor 's ( of the advertiser ) website and find that many more people sign-up on the day they saw an advertisement than on other days , but that the true `` competitive effect '' was minimal . In all three experiments , exposure to a campaign signals doing `` more of everything '' in given period of time , making it difficult to find a suitable `` matched control '' using prior behavior . In such cases , the `` match '' is fundamentally different from the exposed group , and we show how and why observational methods lead to a massive overestimate of adfx in such circumstances . \",\n",
       " 13064078: 'Highly efficient algorithms for structural clustering of large websites In this paper , we present a highly scalable algorithm for structurally clustering webpages for extraction . We show that , using only the URLs of the webpages and simple content features , it is possible to cluster webpages effectively and efficiently . At the heart of our techniques is a principled framework , based on the principles of information theory , that allows us to effectively leverage the URLs , and combine them with content and structural properties . Using an extensive evaluation over several large full websites , we demonstrate the effectiveness of our techniques , at a scale unattainable by previous techniques . ',\n",
       " 13064160: 'Web scale NLP : a case study on url word breaking This paper uses the URL word breaking task as an example to elaborate what we identify as crucial in designing statistical natural language processing ( NLP ) algorithms for Web scale applications : ( 1 ) rudimentary multilingual capabilities to cope with the global nature of the Web , ( 2 ) multi-style modeling to handle diverse language styles seen in the Web contents , ( 3 ) fast adaptation to keep pace with the dynamic changes of the Web , ( 4 ) minimal heuristic assumptions for generalizability and robustness , and ( 5 ) possibilities of efficient implementations and minimal manual efforts for processing massive amount of data at a reasonable cost . We first show that the state-of-the-art word breaking techniques can be unified and generalized under the Bayesian minimum risk ( BMR ) framework that , using a Web scale N-gram , can meet the first three requirements . We discuss how the existing techniques can be viewed as introducing additional assumptions to the basic BMR framework , and describe a generic yet efficient implementation called word synchronous beam search . Testing the framework and its implementation on a series of large scale experiments reveals the following . First , the language style used to build the model plays a critical role in the word breaking task , and the most suitable for the URL word breaking task appears to be that of the document title where the best performance is obtained . Models created from other language styles , such as from document body , anchor text , and even queries , exhibit varying degrees of mismatch . Although all styles benefit from increasing modeling power which , in our experiments , corresponds to the use of a higher order N-gram , the gain is most recognizable for the title model . The heuristics proposed by the prior arts do contribute to the word breaking performance for mismatched or less powerful models , but are less effective and , in many cases , lead to poorer performance than the matched model with minimal assumptions . For the matched model based on document titles , an accuracy rate of 97.18 % can already be achieved using simple trigram without any heuristics . ',\n",
       " 13064222: 'A game theoretic formulation of the service provisioning problem in cloud systems Cloud computing is an emerging paradigm which allows the on-demand delivering of software , hardware , and data as services . As cloud-based services are more numerous and dynamic , the development of efficient service provisioning policies become increasingly challenging . Game theoretic approaches have shown to gain a thorough analytical understanding of the service provisioning problem . In this paper we take the perspective of Software as a Service ( SaaS ) providers which host their applications at an Infrastructure as a Service ( IaaS ) provider . Each SaaS needs to comply with quality of service requirements , specified in Service Level Agreement ( SLA ) contracts with the end-users , which determine the revenues and penalties on the basis of the achieved performance level . SaaS providers want to maximize their revenues from SLAs , while minimizing the cost of use of resources supplied by the IaaS provider . Moreover , SaaS providers compete and bid for the use of infrastructural resources . On the other hand , the IaaS wants to maximize the revenues obtained providing virtualized resources . In this paper we model the service provisioning problem as a Generalized Nash game , and we propose an efficient algorithm for the run time management and allocation of IaaS resources to competing SaaSs . ',\n",
       " 13064271: 'Survivability-oriented self-tuning of web systems Running in a highly uncertain and changing environment , Web systems can not always provide full set of services with optim ¬ al quality , especially when the workload is high or failures in subsys-tems occur frequently . It is thus desirable to continuously maintain a high satisfaction level of the system value proposition , hereafter survivability assurance , while relaxing\\\\/sacrificing certain quality\\\\/functional requirements that are not crucial to the survival of the Web systems . In this paper , we propose a requirements-driven self-tuning method for survivability assurance of Web systems . Using a value-based feedback controller plus a requirements-oriented reasoner , our method makes both quality and functional requirements tradeoffs decisions at runtime . ',\n",
       " 13064294: \"OntoTrix : a hybrid visualization for populated ontologies Most Semantic Web data visualization tools structure the representation according to the concept definitions and interrelations that constitute the ontology 's vocabulary . Instances are often treated as somewhat peripheral information , when considered at all . These instances , that populate ontologies , represent an essential part of any knowledge base , and are often orders of magnitude more numerous than the concept definitions that give them machine-processable meaning . We present a visualization technique designed to enable users to visualize large instance sets and the relations that connect them . This hybrid visualization uses both node-link and adjacency matrix representations of graphs to visualize different parts of the data depending on their semantic and local structural properties , exploiting ontological knowledge to drive the graph layout . The representation is embedded in an environment that features advanced interaction techniques for easy navigation , including support for smooth continuous zooming and coordinated views . \",\n",
       " 13064296: 'Information spreading in context Information spreading processes are central to human interactions . Despite recent studies in online domains , little is known about factors that could affect the dissemination of a single piece of information . In this paper , we address this challenge by combining two related but distinct datasets , collected from a large scale privacy-preserving distributed social sensor system . We find that the social and organizational context significantly impacts to whom and how fast people forward information . Yet the structures within spreading processes can be well captured by a simple stochastic branching model , indicating surprising independence of context . Our results build the foundation of future predictive models of information flow and provide significant insights towards design of communication platforms . ',\n",
       " 13064323: 'A user-tunable approach to marketplace search The notion of relevance is key to the performance of search engines as they interpret the user queries and respond with matching results . Online search engines have used other features beyond pure IR features to return relevant matching documents . However , over-emphasis on relevance could lead to redundancy in search results . In document search , diversity is simply the variety of documents that span the result set . In an online marketplace the diversity in the result set is represented by items for sale by different sellers at different prices with different sales options . For such a marketplace , in order to minimize query abandonment and the risk of dissatisfaction to the average user , several factors like diversity , trust and value need to be taken into account . Previous work in this field ( 4 ) has shown an impossibility result that there exists no such function that can optimize for all these factors . Since these factors and the measures associated with the factors could be subjective we take an approach of giving the control back to the user . In this paper we describe an interface which enables users to have more control over the optimization function used to present the results . We demonstrate this for search on eBay - one of the largest online marketplaces with a vibrant user community and dynamic inventory . We use an algorithm based on bounded greedy selection ( 5 ) to construct the result set based on parameters specified by the user . ',\n",
       " 13064431: \"Dynamic learning-based mechanism design for dependent valued exchange economies Learning private information from multiple strategic agents poses challenge in many Internet applications . Sponsored search auctions , crowdsourcing , Amazon 's mechanical turk , various online review forums are examples where we are interested in learning true values of the advertisers or true opinion of the reviewers . The common thread in these decision problems is that the optimal outcome depends on the private information of all the agents , while the decision of the outcome can be chosen only through reported information which may be manipulated by the strategic agents . The other important trait of these applications is their dynamic nature . The advertisers in an online auction or the users of mechanical turk arrive and depart , and when present , interact with the system repeatedly , giving the opportunity to learn their types . Dynamic mechanisms , which learn from the past interactions and make present decisions depending on the expected future evolution of the game , has been shown to improve performance over repeated versions of static mechanisms . In this paper , we will survey the past and current state-of-the-art dynamic mechanisms and analyze a new setting where the agents consist of buyers and sellers , known as exchange economies , and agents having value interdependency , which are relevant in applications illustrated through examples . We show that known results of dynamic mechanisms with independent value settings can not guarantee certain desirable properties in this new significantly different setting . In the future work , we propose to analyze similar settings with dynamic types and population . \",\n",
       " 13064442: 'Identifying enrichment candidates in textbooks Many textbooks written in emerging countries lack clear and adequate coverage of important concepts . We propose a technological solution for algorithmically identifying those sections of a book that are not well written and could benefit from better exposition . We provide a decision model based on the syntactic complexity of writing and the dispersion of key concepts . The model parameters are learned using a tune set which is algorithmically generated using a versioned authoritative web resource as a proxy . We evaluate the proposed methodology over a corpus of Indian textbooks which demonstrates its effectiveness in identifying enrichment candidates . ',\n",
       " 13064535: 'Two-stream indexing for spoken web search This paper presents two-stream processing of audio to index the audio content for Spoken Web search . The first stream indexes the meta-data associated with a particular audio document . The meta-data is usually very sparse , but accurate . This therefore results in a high-precision , low-recall index . The second stream uses a novel language-independent speech recognition to generate text to be indexed . Owing to the multiple languages and the noise in user generated content on the Spoken Web , the speech recognition accuracy of such systems is not high , thus they result in a low-precision , high-recall index . The paper attempts to use these two complementary streams to generate a combined index to increase the precision-recall performance in audio content search . The problem of audio content search is motivated by the real world implication of the Web in developing regions , where due to literacy and affordability issues , people use Spoken Web which consists of interconnected VoiceSites , which have content in audio . The experiments are based on more than 20,000 audio documents spanning over seven live VoiceSites and four different languages . The results suggest significant improvement over a meta-data-only or a speech-recognitiononly system , thus justifying the two-stream processing approach . Audio content search is a growing problem area and this paper wishes to be a first step to solving this at a large scale , across languages , in a Web context . ',\n",
       " 13064551: \"Harnessing the wisdom of crowds : video event detection based on synchronous comments With the recent explosive growth of the number of videos on the Web , it becomes more important to facilitate users ' demand for locating their preferred event clips in the lengthy and voluminous programs . Although there has been a great deal of study on generic event detection in recent years , the performance of existing approaches is still far from satisfactory . In this paper , we propose an integrated framework for general event detection . The key idea is that we utilize the synchronous comments to segment the video into clips with semantic text analysis , while taking into account the relationship between the users who write the comments . By borrowing the power of `` the wisdom of crowds '' , we experimentally demonstrate that our approach can effectively detect video events . \",\n",
       " 13064602: \"CONQUER : a system for efficient context-aware query suggestions Many of today 's search engines provide autocompletion while the user is typing a query string . This type of dynamic query suggestion can help users to formulate queries that better represent their search intent during Web search interactions . In this paper , we demonstrate our query suggestion system called CONQUER , which allows to efficiently suggest queries for a given partial query and a number of available query context observations . The context-awareness allows for suggesting queries tailored to a given context , e.g. , the user location or the time of day . CONQUER uses a suggestion model that is based on the combined probabilities of sequential query patterns and context observations . For this , the weight of a context in a query suggestion can be adjusted online , for example , based on the learned user behavior or user profiles . We demonstrate the functionality of CONQUER based on 6 million queries from an AOL query log using the time of day and the country domain of the clicked URLs in the search result as context observations . \",\n",
       " 13064675: 'Understanding the functions of business accounts on Twitter This paper performs an initial exploration of business Twitter accounts in order to start understanding how businesses interact with their users and viceversa . We provide an analysis of business tweet types and topics and show that specific business tweet classes such as deals and events can be reliably identified for customer use . ',\n",
       " 13064722: 'A classification based framework for concept summarization In this paper we propose a novel classification based framework for finding a small number of images summarizing a concept . Our method exploits metadata information available with the images to get the category information using Latent Dirichlet Allocation . We modify the import vector machine formulation based on kernel logistic regression to solve the underlying classification problem . We show that the import vectors provide a good summary satisfying important properties such as coverage , diversity and balance . Furthermore , the framework allows users to specify desired distributions over category , time etc , that a summary should satisfy . Experimental results show that the proposed method performs better than state-of-the-art summarization methods in terms of satisfying important visual and semantic properties . ',\n",
       " 13064730: 'Choreography conformance via synchronizability Choreography analysis has been a crucial problem in service oriented computing . Interactions among services involve message exchanges across organizational boundaries in a distributed computing environment , and in order to build such systems in a reliable manner , it is necessary to develop techniques for analyzing such interactions . Choreography conformance involves verifying that a set of services behave according to a given choreography specification that characterizes their interactions . Unfortunately this is an undecidable problem when services interact with asynchronous communication . In this paper we present techniques that identify if the interaction behavior for a set of services remain the same when asynchronous communication is replaced with synchronous communication . This is called the synchronizability problem and determining the synchronizability of a set of services has been an open problem for several years . We solve this problem in this paper . Our results can be used to identify synchronizable services for which choreography conformance can be checked efficiently . Our results on synchronizability are applicable to any software infrastructure that supports message-based interactions . ',\n",
       " 13087531: 'Relational duality : unsupervised extraction of semantic relations between entities on the web Extracting semantic relations among entities is an important first step in various tasks in Web mining and natural language processing such as information extraction , relation detection , and social network mining . A relation can be expressed extensionally by stating all the instances of that relation or intensionally by defining all the paraphrases of that relation . For example , consider the ACQUISITION relation between two companies . An extensional definition of ACQUISITION contains all pairs of companies in which one company is acquired by another ( e.g. ( YouTube , Google ) or ( Powerset , Microsoft ) ) . On the other hand we can intensionally define ACQUISITION as the relation described by lexical patterns such as X is acquired by Y , or Y purchased X , where X and Y denote two companies . We use this dual representation of semantic relations to propose a novel sequential co-clustering algorithm that can extract numerous relations efficiently from unlabeled data . We provide an efficient heuristic to find the parameters of the proposed coclustering algorithm . Using the clusters produced by the algorithm , we train an L1 regularized logistic regression model to identify the representative patterns that describe the relation expressed by each cluster . We evaluate the proposed method in three different tasks : measuring relational similarity between entity pairs , open information extraction ( Open IE ) , and classifying relations in a social network system . Experiments conducted using a benchmark dataset show that the proposed method improves existing relational similarity measures . Moreover , the proposed method significantly outperforms the current state-of-the-art Open IE systems in terms of both precision and recall . The proposed method correctly classifies 53 relation types in an online social network containing 470 ; 671 nodes and 35 ; 652 ; 475 edges , thereby demonstrating its efficacy in real-world relation detection tasks . ',\n",
       " 13089030: \"Who says what to whom on twitter We study several longstanding questions in media communications research , in the context of the microblogging service Twitter , regarding the production , flow , and consumption of information . To do so , we exploit a recently introduced feature of Twitter known as `` lists '' to distinguish between elite users - by which we mean celebrities , bloggers , and representatives of media outlets and other formal organizations - and ordinary users . Based on this classification , we find a striking concentration of attention on Twitter , in that roughly 50 % of URLs consumed are generated by just 20K elite users , where the media produces the most information , but celebrities are the most followed . We also find significant homophily within categories : celebrities listen to celebrities , while bloggers listen to bloggers etc ; however , bloggers in general rebroadcast more information than the other categories . Next we re-examine the classical `` two-step flow '' theory of communications , finding considerable support for it on Twitter . Third , we find that URLs broadcast by different categories of users or containing different types of content exhibit systematically different lifespans . And finally , we examine the attention paid by the different user categories to different news topics . \",\n",
       " 13106832: \"Piazza : data management infrastructure for semantic web applications The Semantic Web envisions a World Wide Web in which data is described with rich semantics and applications can pose complex queries . To this point , researchers have defined new languages for specifying meanings for concepts and developed techniques for reasoning about them , using RDF as the data model . To flourish , the Semantic Web needs to be able to accommodate the huge amounts of existing data and the applications operating on them . To achieve this , we are faced with two problems . First , most of the world 's data is available not in RDF but in XML ; XML and the applications consuming it rely not only on the domain structure of the data , but also on its document structure . Hence , to provide interoperability between such sources , we must map between both their domain structures and their document structures . Second , data management practitioners often prefer to exchange data through local point-to-point data translations , rather than mapping to common mediated schemas or ontologies . This paper describes the Piazza system , which addresses these challenges . Piazza offers a language for mediating between data sources on the Semantic Web , which maps both the domain structure and document structure . Piazza also enables interoperation of XML data with RDF data that is accompanied by rich OWL ontologies . Mappings in Piazza are provided at a local scale between small sets of nodes , and our query answering algorithm is able to chain sets mappings together to obtain relevant data from across the Piazza network . We also describe an implemented scenario in Piazza and the lessons we learned from it . \",\n",
       " 13109: \"Detecting web page structure for adaptive viewing on small form factor devices Mobile devices have already been widely used to access the Web . However , because most available web pages are designed for desktop PC in mind , it is inconvenient to browse these large web pages on a mobile device with a small screen . In this paper , we propose a new browsing convention to facilitate navigation and reading on a small-form-factor device . A web page is organized into a two level hierarchy with a thumbnail representation at the top level for providing a global view and index to a set of sub-pages at the bottom level for detail information . A page adaptation technique is also developed to analyze the structure of an existing web page and split it into small and logically related units that fit into the screen of a mobile device . For a web page not suitable for splitting , auto-positioning or scrolling-by-block is used to assist the browsing as an alterative . Our experimental results show that our proposed browsing convention and developed page adaptation scheme greatly improve the user 's browsing experiences on a device with a small display . \",\n",
       " 13152279: 'Analysis of community structure in Wikipedia We present the results of a community detection analysis of the Wikipedia graph . Distinct communities in Wikipedia contain semantically closely related articles . The central topic of a community can be identified using PageRank . Extracted communities can be organized hierarchically similar to manually created Wikipedia category structure . ',\n",
       " 13185533: \"Generalized fact-finding Once information retrieval has located a document , and information extraction has provided its contents , how do we know whether we should actually believe it ? Fact-finders are a state-of-the-art class of algorithms that operate in a manner analogous to Kleinberg 's Hubs and Authorities , iteratively computing the trustworthiness of an information source as a function of the believability of the claims it makes , and the believability of a claim as a function of the trustworthiness of those sources asserting it . However , as fact-finders consider only `` who claims what '' , they ignore a great deal of relevant background and contextual information . We present a framework for `` lifting '' ( generalizing ) the fact-finding process , allowing us to elegantly incorporate knowledge such as the confidence of the information extractor and the attributes of the information sources . Experiments demonstrate that leveraging this information significantly improves performance over existing , `` unlifted '' fact-finding algorithms . \",\n",
       " 13190545: 'Unified analysis of streaming news News clustering , categorization and analysis are key components of any news portal . They require algorithms capable of dealing with dynamic data to cluster , interpret and to temporally aggregate news articles . These three tasks are often solved separately . In this paper we present a unified framework to group incoming news articles into temporary but tightly-focused storylines , to identify prevalent topics and key entities within these stories , and to reveal the temporal structure of stories as they evolve . We achieve this by building a hybrid clustering and topic model . To deal with the available wealth of data we build an efficient parallel inference algorithm by sequential Monte Carlo estimation . Time and memory costs are nearly constant in the length of the history , and the approach scales to hundreds of thousands of documents . We demonstrate the efficiency and accuracy on the publicly available TDT dataset and data of a major internet news site . ',\n",
       " 13193305: \"Talking about data : sharing richly structured information through blogs and wikis The web has dramatically enhanced people 's ability to communicate ideas , knowledge , and opinions . But the authoring tools that most people understand , blogs and wikis , primarily guide users toward authoring text . In this work , we show that substantial gains in expressivity and communication would accrue if people could easily share richly structured information in meaningful visualizations . We then describe several extensions we have created for blogs and wikis that enable users to publish , share , and aggregate such structured information using the same workflows they apply to text . In particular , we aim to preserve those attributes that make blogs and wikis so effective : one-click access to the information , one-click publishing of content , natural authoring interfaces , and the ability to easily copy-and-paste information and visualizations from other sources . \",\n",
       " 13240315: \"Stop thinking , start tagging : tag semantics emerge from collaborative verbosity Recent research provides evidence for the presence of emergent semantics in collaborative tagging systems . While several methods have been proposed , little is known about the factors that influence the evolution of semantic structures in these systems . A natural hypothesis is that the quality of the emergent semantics depends on the pragmatics of tagging : Users with certain usage patterns might contribute more to the resulting semantics than others . In this work , we propose several measures which enable a pragmatic differentiation of taggers by their degree of contribution to emerging semantic structures . We distinguish between categorizers , who typically use a small set of tags as a replacement for hierarchical classification schemes , and describers , who are annotating resources with a wealth of freely associated , descriptive keywords . To study our hypothesis , we apply semantic similarity measures to 64 different partitions of a real-world and large-scale folksonomy containing different ratios of categorizers and describers . Our results not only show that `` verbose '' taggers are most useful for the emergence of tag semantics , but also that a subset containing only 40 % of the most ` verbose ' taggers can produce results that match and even outperform the semantic precision obtained from the whole dataset . Moreover , the results suggest that there exists a causal link between the pragmatics of tagging and resulting emergent semantics . This work is relevant for designers and analysts of tagging systems interested ( i ) in fostering the semantic development of their platforms , ( ii ) in identifying users introducing `` semantic noise '' , and ( iii ) in learning ontologies . \",\n",
       " 13251515: \"The paths more taken : matching DOM trees to search logs for accurate webpage clustering An unsupervised clustering of the webpages on a website is a primary requirement for most wrapper induction and automated data extraction methods . Since page content can vary drastically across pages of one cluster ( e.g. , all product pages on amazon.com ) , traditional clustering methods typically use some distance function between the DOM trees representing a pair of webpages . However , without knowing which portions of the DOM tree are `` important , '' such distance functions might discriminate between similar pages based on trivial features ( e.g. , differing number of reviews on two product pages ) , or club together distinct types of pages based on superficial features present in the DOM trees of both ( e.g. , matching footer\\\\/copyright ) , leading to poor clustering performance . We propose using search logs to automatically find paths in the DOM trees that mark out important portions of pages , e.g. , the product title in a product page . Such paths are identified via a global analysis of the entire website , whereby search data for popular pages can be used to infer good paths even for other pages that receive little or no search traffic . The webpages on the website are then clustered using these `` key '' paths . Our algorithm only requires information on search queries , and the webpages clicked in response to them ; there is no need for human input , and it does not need to be told which portion of a webpage the user found interesting . The resulting clusterings achieve an adjusted RAND score of over 0.9 on half of the websites ( a score of 1 indicating a perfect clustering ) , and 59 % better scores on average than competing algorithms . Besides leading to refined clusterings , these key paths can be useful in the wrapper induction process itself , as shown by the high degree of match between the key paths and the manually identified paths used in existing wrappers for these sites ( 90 % average precision ) . \",\n",
       " 13386371: 'Protocol-aware matching of web service interfaces for adapter development With the rapid growth in the number of online Web services , the problem of service adaptation has received significant attention . In matching and adaptation , the functional description of services including interface and data as well as behavioral descriptions are important . Existing work on matching and adaptation focuses only on one aspect . In this paper , we present a semi-automated matching approach that considers both service descriptions . We introduce two protocol-aware service interface matching algorithms , i.e. depth-based interface matching and iterative reference-based interface matching . These algorithms refine the results of interface matching by incorporating the ordering constraints imposed by business protocol definitions on service operations . We have implemented a prototype and performed experiments using the specification of synthetic and real-world Web services . Experiments show that the proposed approaches lead to a significant improvement in the quality of matching between services . ',\n",
       " 13395437: 'Randomization tests for distinguishing social influence and homophily effects Relational autocorrelation is ubiquitous in relational domains . This observed correlation between class labels of linked instances in a network ( e.g. , two friends are more likely to share political beliefs than two randomly selected people ) can be due to the effects of two different social processes . If social influence effects are present , instances are likely to change their attributes to conform to their neighbor values . If homophily effects are present , instances are likely to link to other individuals with similar attribute values . Both these effects will result in autocorrelated attribute values . When analyzing static relational networks it is impossible to determine how much of the observed correlation is due each of these factors . However , the recent surge of interest in social networks has increased the availability of dynamic network data . In this paper , we present a randomization technique for temporal network data where the attributes and links change over time . Given data from two time steps , we measure the gain in correlation and assess whether a significant portion of this gain is due to influence and\\\\/or homophily . We demonstrate the efficacy of our method on semi-synthetic data and then apply the method to a real-world social networks dataset , showing the impact of both influence and homophily effects . ',\n",
       " 13403801: 'Tag ranking Social media sharing web sites like Flickr allow users to annotate images with free tags , which significantly facilitate Web image search and organization . However , the tags associated with an image generally are in a random order without any importance or relevance information , which limits the effectiveness of these tags in search and other applications . In this paper , we propose a tag ranking scheme , aiming to automatically rank the tags associated with a given image according to their relevance to the image content . We first estimate initial relevance scores for the tags based on probability density estimation , and then perform a random walk over a tag similarity graph to refine the relevance scores . Experimental results on a 50 , 000 Flickr photo collection show that the proposed tag ranking method is both effective and efficient . We also apply tag ranking into three applications : ( 1 ) tag-based image search , ( 2 ) tag recommendation , and ( 3 ) group recommendation , which demonstrates that the proposed tag ranking approach really boosts the performances of social-tagging related applications . ',\n",
       " 13501869: \"Hybrid keyword search auctions Search auctions have become a dominant source of revenue generation on the Internet . Such auctions have typically used per-click bidding and pricing . We propose the use of hybrid auctions where an advertiser can make a per-impression as well as a per-click bid , and the auctioneer then chooses one of the two as the pricing mechanism . We assume that the advertiser and the auctioneer both have separate beliefs ( called priors ) on the click-probability of an advertisement . We first prove that the hybrid auction is truthful , assuming that the advertisers are risk-neutral . We then show that this auction is superior to the existing per-click auction in multiple ways : We show that risk-seeking advertisers will choose only a per-impression bid whereas risk-averse advertisers will choose only a per-click bid , and argue that both kind of advertisers arise naturally . Hence , the ability to bid in a hybrid fashion is important to account for the risk characteristics of the advertisers . For obscure keywords , the auctioneer is unlikely to have a very sharp prior on the click-probabilities . In such situations , we show that having the extra information from the advertisers in the form of a per-impression bid can result in significantly higher revenue . An advertiser who believes that its click-probability is much higher than the auctioneer 's estimate can use per-impression bids to correct the auctioneer 's prior without incurring any extra cost . The hybrid auction can allow the advertiser and auctioneer to implement complex dynamic programming strategies to deal with the uncertainty in the click-probability using the same basic auction . The per-click and per-impression bidding schemes can only be used to implement two extreme cases of these strategies . As Internet commerce matures , we need more sophisticated pricing models to exploit all the information held by each of the participants . We believe that hybrid auctions could be an important step in this direction . The hybrid auction easily extends to multiple slots , and is also applicable to scenarios where the hybrid bidding is per-impression and per-action ( i.e. CPM and CPA ) , or per-click and per-action ( i.e. CPC and CPA ) . \",\n",
       " 13502389: \"Search shortcuts : driving users towards their goals Giving suggestions to users of Web-based services is a common practice aimed at enhancing their navigation experience . Major Web Search Engines usually provide `` Suggestions '' under the form of queries that are , to some extent , related to the current query typed by the user , and the knowledge learned from the past usage of the system . In this work we introduce `` Search Shortcuts '' as `` Successful '' queries allowed , in the past , users to satisfy their information needs . Differently from conventional suggestion techniques , our search shortcuts allows to evaluate effectiveness by exploiting a simple train-and-test approach . We have applied several Collaborative Filtering algorithms to this problem , evaluating them on a real query log data . We generate the shortcuts from all user sessions belonging to the testing set , and measure the quality of the shortcuts suggested by considering the similarity between them and the navigational user behavior . \",\n",
       " 13502390: \"Building term suggestion relational graphs from collective intelligence This paper proposes an effective approach to provide relevant search terms for conceptual Web search . ` Semantic Term Suggestion ' function has been included so that users can find the most appropriate query term to what they really need . Conventional approaches for term suggestion involve extracting frequently occurring key terms from retrieved documents . They must deal with term extraction difficulties and interference from irrelevant documents . In this paper , we propose a semantic term suggestion function called Collective Intelligence based Term Suggestion ( CITS ) . CITS provides a novel social-network based framework for relevant terms suggestion with a semantic graph of the search term without limiting to the specific query term . A visualization of semantic graph is presented to the users to help browsing search results from related terms in the semantic graph . The search results are ranked each time according to their relevance to the related terms in the entire query session . Comparing to two popular commercial search engines , a user study of 18 users on 50 search terms showed better user satisfactions and indicated the potential usefulness of proposed method in real-world search applications . \",\n",
       " 13502392: \"WPBench : a benchmark for evaluating the client-side performance of web 2.0 applications In this paper , a benchmark called WPBench is reported to evaluate the responsiveness of Web browsers for modern Web 2.0 applications . In WPBench , variations of servers and networks are removed and the benchmark result is the closest to what Web users would perceive . To achieve these , WPBench records users ' interactions with typical Web 2.0 applications , and then replays Web navigations when benchmarking browsers . The replay mechanism can emulate the actual user interactions and the characteristics of the servers and the networks in a consistent way independent of browsers so that any browser compliant to the standards can be benchmarked fairly . In addition to describing the design and generation of WPBench , we also report the WPBench comparison results on the responsiveness performance for three popular Web browsers : Internet Explorer , Firefox and Chrome . \",\n",
       " 13502500: \"Web 2.0 : blind to an accessible new world With the advent of Web 2.0 technologies , websites have evolved from static pages to dynamic , interactive Web-based applications with the ability to replicate common desktop functionality . However , for blind and visually impaired individuals who rely upon screen readers , Web 2.0 applications force them to adapt to an inaccessible use model . Many technologies , including WAI-ARIA , AJAX , and improved screen reader support , are rapidly evolving to improve this situation . However , simply combining them does not solve the problems of screen reader users . The main contributions of this paper are two models of interaction for screen reader users , for both traditional websites and Web 2.0 applications . Further contributions are a discussion of accessibility difficulties screen reader users encounter when interacting with Web 2.0 applications , a user workflow design model for improving Web 2.0 accessibility , and a set of design requirements for developers to ease the user 's burden and increase accessibility . These models , accessibility difficulties , and design implications are based directly on responses and lessons learned from usability research focusing on Web 2.0 usage and screen reader users . Without the conscious effort of Web engineers and designers , most blind and visually impaired users will shy away from using new Web 2.0 technology in favor of desktop based applications . \",\n",
       " 13502857: 'Buzz-based recommender system In this paper , we describe a buzz-based recommender system based on a large source of queries in an eCommerce application . The system detects bursts in query trends . These bursts are linked to external entities like news and inventory information to find the queries currently in-demand which we refer to as buzz queries . The system follows the paradigm of limited quantity merchandising , in the sense that on a per-day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity , and improving activity and stickiness on the site . A semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood . ',\n",
       " 13502912: 'How much can behavioral targeting help online advertising ? Behavioral Targeting ( BT ) is a technique used by online advertisers to increase the effectiveness of their campaigns , and is playing an increasingly important role in the online advertising market . However , it is underexplored in academia when looking at how much BT can truly help online advertising in commercial search engines . To answer this question , in this paper we provide an empirical study on the click-through log of advertisements collected from a commercial search engine . From the comprehensively experiment results on the sponsored search log of the commercial search engine over a period of seven days , we can draw three important conclusions : ( 1 ) Users who clicked the same ad will truly have similar behaviors on the Web ; ( 2 ) Click-Through Rate ( CTR ) of an ad can be averagely improved as high as 670 % by properly segmenting users for behavioral targeted advertising in a sponsored search ; ( 3 ) Using the short term user behaviors to represent users is more effective than using the long term user behaviors for BT . The statistical t-test verifies that all conclusions drawn in the paper are statistically significant . To the best of our knowledge , this work is the first empirical study for BT on the click-through log of real world ads . ',\n",
       " 13503261: 'Threshold selection for web-page classification with highly skewed class distribution We propose a novel cost-efficient approach to threshold selection for binary web-page classification problems with imbalanced class distributions . In many binary-classification tasks the distribution of classes is highly skewed . In such problems , using uniform random sampling in constructing sample sets for threshold setting requires large sample sizes in order to include a statistically sufficient number of examples of the minority class . On the other hand , manually labeling examples is expensive and budgetary considerations require that the size of sample sets be limited . These conflicting requirements make threshold selection a challenging problem . Our method of sample-set construction is a novel approach based on stratified sampling , in which manually labeled examples are expanded to reflect the true class distribution of the web-page population . Our experimental results show that using false positive rate as the criterion for threshold setting results in lower-variance threshold estimates than using other widely used accuracy measures such as F1 and precision . ',\n",
       " 13504156: 'Thumbs-up : a game for playing to rank search results Human computation is an effective way to channel human effort spent playing games to solving computational problems that are easy for humans but difficult for computers to automate . We propose Thumbs-Up , a new game for human computation with the purpose of playing to rank search result . Our experience from users shows that Thumbs-Up is not only fun to play , but produces more relevant rankings than both a major search engine and optimal rank aggregation using the Kemeny rule . ',\n",
       " 13505677: 'iRIN : image retrieval in image-rich information networks In this demo , we present a system called iRIN designed for performing image retrieval in image-rich information networks . We first introduce MoK-SimRank to significantly improve the speed of SimRank , one of the most popular algorithms for computing node similarity in information networks . Next , we propose an algorithm called SimLearn to ( 1 ) extend MoK-SimRank to heterogeneous image-rich information network , and ( 2 ) account for both link-based and content-based similarities by seamlessly integrating reinforcement learning with feature learning . ',\n",
       " 13506151: 'Deriving music theme annotations from user tags Music theme annotations would be really beneficial for supporting retrieval , but are often neglected by users while annotating . Thus , in order to support users in tagging and to fill the gaps in the tag space , in this paper we develop algorithms for recommending theme annotations . Our methods exploit already existing user tags , the lyrics of music tracks , as well as combinations of both . We compare the results for our recommended theme annotations against genre and style recommendations - a much easier and already studied task . We evaluate the quality of our recommended tags against an expert ground truth data set . Our results are promising and provide interesting insights into possible extensions for music tagging systems to support music search . ',\n",
       " 13507253: \"RuralCafe : web search in the rural developing world The majority of people in rural developing regions do not have access to the World Wide Web . Traditional network connectivity technologies have proven to be prohibitively expensive in these areas . The emergence of new long-range wireless technologies provide hope for connecting these rural regions to the Internet . However , the network connectivity provided by these new solutions are by nature intermittent due to high network usage rates , frequent power-cuts and the use of delay tolerant links . Typical applications , especially interactive applications like web search , do not tolerate intermittent connectivity . In this paper , we present the design and implementation of RuralCafe , a system intended to support efficient web search over intermittent networks . RuralCafe enables users to perform web search asynchronously and find what they are looking for in one round of intermittency as opposed to multiple rounds of search\\\\/downloads . RuralCafe does this by providing an expanded search query interface which allows a user to specify additional query terms to maximize the utility of the results returned by a search query . Given knowledge of the limited available network resources , RuralCafe performs optimizations to prefetch pages to best satisfy a search query based on a user 's search preferences . In addition , RuralCafe does not require modifications to the web browser , and can provide single round search results tailored to various types of networks and economic constraints . We have implemented and evaluated the effectiveness of RuralCafe using queries from logs made to a large search engine , queries made by users in an intermittent setting , and live queries from a small testbed deployment . We have also deployed a prototype of RuralCafe in Kerala , India . \",\n",
       " 13507543: 'A class-feature-centroid classifier for text categorization Automated text categorization is an important technique for many web applications , such as document indexing , document filtering , and cataloging web resources . Many different approaches have been proposed for the automated text categorization problem . Among them , centroid-based approaches have the advantages of short training time and testing time due to its computational efficiency . As a result , centroid-based classifiers have been widely used in many web applications . However , the accuracy of centroid-based classifiers is inferior to SVM , mainly because centroids found during construction are far from perfect locations . We design a fast Class-Feature-Centroid ( CFC ) classifier for multi-class , single-label text categorization . In CFC , a centroid is built from two important class distributions : inter-class term index and inner-class term index . CFC proposes a novel combination of these indices and employs a denormalized cosine measure to calculate the similarity score between a text vector and a centroid . Experiments on the Reuters-21578 corpus and 20-newsgroup email collection show that CFC consistently outperforms the state-of-the-art SVM classifiers on both micro-F1 and macro-F1 scores . Particularly , CFC is more effective and robust than SVM when data is sparse . ',\n",
       " 13507714: \"Graffiti : node labeling in heterogeneous networks We introduce a multi-label classification model and algorithm for labeling heterogeneous networks , where nodes belong to different types and different types have different sets of classification labels . We present a graph-based approach which models the mutual influence between nodes in the network as a random walk . When viewing class labels as `` colors '' , the random surfer is `` spraying '' different node types with different color palettes ; hence the name Graffiti . We demonstrate the performance gains of our method by comparing it to three state-of-the-art techniques for graph-based classification . \",\n",
       " 13509697: \"Analyzing seller practices in a Brazilian marketplace E-commerce is growing at an exponential rate . In the last decade , there has been an explosion of online commercial activity enabled by World Wide Web ( WWW ) . These days , many consumers are less attracted to online auctions , preferring to buy merchandise quickly using fixed-price negotiations . Sales at Amazon.com , the leader in online sales of fixed-price goods , rose 37 % in the first quarter of 2008 . At eBay , where auctions make up 58 % of the site 's sales , revenue rose 14 % . In Brazil , probably by cultural influence , online auctions are not been popular . This work presents a characterization and analysis of fixed-price online negotiations . Using actual data from a Brazilian marketplace , we analyze seller practices , considering seller profiles and strategies . We show that different sellers adopt strategies according to their interests , abilities and experience . Moreover , we confirm that choosing a selling strategy is not simple , since it is important to consider the seller 's characteristics to evaluate the applicability of a strategy . The work also provides a comparative analysis of some selling practices in Brazil with popular worldwide marketplaces . \",\n",
       " 13510800: 'Releasing search queries and clicks privately The question of how to publish an anonymized search log was brought to the forefront by a well-intentioned , but privacy-unaware AOL search log release . Since then a series of ad-hoc techniques have been proposed in the literature , though none are known to be provably private . In this paper , we take a major step towards a solution : we show how queries , clicks and their associated perturbed counts can be published in a manner that rigorously preserves privacy . Our algorithm is decidedly simple to state , but non-trivial to analyze . On the opposite side of privacy is the question of whether the data we can safely publish is of any use . Our findings offer a glimmer of hope : we demonstrate that a non-negligible fraction of queries and clicks can indeed be safely published via a collection of experiments on a real search log . In addition , we select an application , keyword generation , and show that the keyword suggestions generated from the perturbed data resemble those generated from the original data . ',\n",
       " 13512950: 'Adaptive bidding for display advertising Motivated by the emergence of auction-based marketplaces for display ads such as the Right Media Exchange , we study the design of a bidding agent that implements a display advertising campaign by bidding in such a marketplace . The bidding agent must acquire a given number of impressions with a given target spend , when the highest external bid in the marketplace is drawn from an unknown distribution P. The quantity and spend constraints arise from the fact that display ads are usually sold on a CPM basis . We consider both the full information setting , where the winning price in each auction is announced publicly , and the partially observable setting where only the winner obtains information about the distribution ; these differ in the penalty incurred by the agent while attempting to learn the distribution . We provide algorithms for both settings , and prove performance guarantees using bounds on uniform closeness from statistics , and techniques from online learning . We experimentally evaluate these algorithms : both algorithms perform very well with respect to both target quantity and spend ; further , our algorithm for the partially observable case performs nearly as well as that for the fully observable setting despite the higher penalty incurred during learning . ',\n",
       " 13513557: 'Measuring the similarity between implicit semantic relations from the web Measuring the similarity between semantic relations that hold among entities is an important and necessary step in various Web related tasks such as relation extraction , information retrieval and analogy detection . For example , consider the case in which a person knows a pair of entities ( e.g. Google , YouTube ) , between which a particular relation holds ( e.g. acquisition ) . The person is interested in retrieving other such pairs with similar relations ( e.g. Microsoft , Powerset ) . Existing keyword-based search engines can not be applied directly in this case because , in keyword-based search , the goal is to retrieve documents that are relevant to the words used in a query -- not necessarily to the relations implied by a pair of words . We propose a relational similarity measure , using a Web search engine , to compute the similarity between semantic relations implied by two pairs of words . Our method has three components : representing the various semantic relations that exist between a pair of words using automatically extracted lexical patterns , clustering the extracted lexical patterns to identify the different patterns that express a particular semantic relation , and measuring the similarity between semantic relations using a metric learning approach . We evaluate the proposed method in two tasks : classifying semantic relations between named entities , and solving word-analogy questions . The proposed method outperforms all baselines in a relation classification task with a statistically significant average precision score of 0.74 . Moreover , it reduces the time taken by Latent Relational Analysis to process 374 word-analogy questions from 9 days to less than 6 hours , with an SAT score of 51 % . ',\n",
       " 13514543: 'SOFIE : a self-organizing framework for information extraction This paper presents SOFIE , a system for automated ontology extension . SOFIE can parse natural language documents , extract ontological facts from them and link the facts into an ontology . SOFIE uses logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate words to their most probable meaning , to reason on the meaning of text patterns and to take into account world knowledge axioms . This allows SOFIE to check the plausibility of hypotheses and to avoid inconsistencies with the ontology . The framework of SOFIE unites the paradigms of pattern matching , word sense disambiguation and ontological reasoning in one unified model . Our experiments show that SOFIE delivers high-quality output , even from unstructured Internet documents . ',\n",
       " 13514784: \"REST-based management of loosely coupled services Applications increasingly make use of the distributed platform that the World Wide Web provides - be it as a Software-as-a-Service such as salesforce.com , an application infrastructure such as facebook.com , or a computing infrastructure such as a `` cloud '' . A common characteristic of applications of this kind is that they are deployed on infrastructure or make use of components that reside in different management domains . Current service management approaches and systems , however , often rely on a centrally managed configuration management database ( CMDB ) , which is the basis for centrally orchestrated service management processes , in particular change management and incident management . The distribution of management responsibility of WWW based applications requires a decentralized approach to service management . This paper proposes an approach of decentralized service management based on distributed configuration management and service process co-ordination , making use RESTful access to configuration information and ATOM-based distribution of updates as a novel foundation for service management processes . \",\n",
       " 13515810: \"Retaining personal expression for social search Web is being extensively used for personal expression , which includes ratings , reviews , recommendations , blogs . This user created content , e.g. book review on Amazon.com , becomes the property of the website , and the user often does not have easy access to it . In some cases , user 's feedback may get averaged with feedback from other users e.g. ratings of a video . We argue that the creator of such content needs to be able to retain ( a link to ) her created content . We introduce the concept of MEB which is a user controlled store of such retained links . A MEB allows a user to access\\\\/share all the reviews she has given on different websites . With this capability users can allow their friends to search through their feedback . Searching through one 's social network allows harnessing the power of social networks where known relationships provide the context & trust necessary to interpret feedback . \",\n",
       " 13516419: 'Bucefalo : a tool for intelligent search and filtering for web-based personal health records In this poster , a tool named BUCEFALO is presented . This tool is specially designed to improve the information retrieval tasks in web-based Personal Health Records ( PHR ) . This tool implements semantic and multilingual query expansion techniques and information filtering algorithms in order to help users find the most valuable information about a specific clinical case . The filtering model is based on fuzzy prototypes based filtering , data quality measures , user profiles and healthcare ontologies . The first experimental results illustrate the feasibility of this tool . ',\n",
       " 13517074: 'Instance-based probabilistic reasoning in the semantic web Most of the approaches for dealing with uncertainty in the Semantic Web rely on the principle that this uncertainty is already asserted . In this paper , we propose a new approach to learn and reason about uncertainty in the Semantic Web . Using instance data , we learn the uncertainty of an OWL ontology , and use that information to perform probabilistic reasoning on it . For this purpose , we use Markov logic , a new representation formalism that combines logic with probabilistic graphical models . ',\n",
       " 13517135: 'Crawling English-Japanese person-name transliterations from the web Automatic compilation of lexicon is a dream of lexicon compilers as well as lexicon users . This paper proposes a system that crawls English-Japanese person-name transliterations from the Web , which works a back-end collector for automatic compilation of bilingual person-name lexicon . Our crawler collected 561K transliterations in five months . From them , an English-Japanese person-name lexicon with 406K entries has been compiled by an automatic post processing . This lexicon is much larger than other similar resources including English-Japanese lexicon of HeiNER obtained from Wikipedia . ',\n",
       " 13518145: \"Bid optimization for broad match ad auctions Ad auctions in sponsored search support `` broad match '' that allows an advertiser to target a large number of queries while bidding only on a limited number . While giving more expressiveness to advertisers , this feature makes it challenging to optimize bids to maximize their returns : choosing to bid on a query as a broad match because it provides high profit results in one bidding for related queries which may yield low or even negative profits . We abstract and study the complexity of the -LCB- \\\\ em bid optimization problem -RCB- which is to determine an advertiser 's bids on a subset of keywords ( possibly using broad match ) so that her profit is maximized . In the query language model when the advertiser is allowed to bid on all queries as broad match , we present a linear programming ( LP ) - based polynomial-time algorithm that gets the optimal profit . In the model in which an advertiser can only bid on keywords , ie. , a subset of keywords as an exact or broad match , we show that this problem is not approximable within any reasonable approximation factor unless P = NP . To deal with this hardness result , we present a constant-factor approximation when the optimal profit significantly exceeds the cost . This algorithm is based on rounding a natural LP formulation of the problem . Finally , we study a budgeted variant of the problem , and show that in the query language model , one can find two budget constrained ad campaigns in polynomial time that implement the optimal bidding strategy . Our results are the first to address bid optimization under the broad match feature which is common in ad auctions . \",\n",
       " 13518419: \"Matchbox : large scale online bayesian recommendations We present a probabilistic model for generating personalised recommendations of items to users of a web service . The Matchbox system makes use of content information in the form of user and item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user . Users and items are represented by feature vectors which are mapped into a low-dimensional ` trait space ' in which similarity is measured in terms of inner products . The model can be trained from different types of feedback in order to learn user-item preferences . Here we present three alternatives : direct observation of an absolute rating each user gives to some items , observation of a binary preference ( like \\\\/ do n't like ) and observation of a set of ordinal ratings on a user-specific scale . Efficient inference is achieved by approximate message passing involving a combination of Expectation Propagation ( EP ) and Variational Message Passing . We also include a dynamics model which allows an item 's popularity , a user 's taste or a user 's personal rating scale to drift over time . By using Assumed-Density Filtering ( ADF ) for training , the model requires only a single pass through the training data . This is an on-line learning algorithm capable of incrementally taking account of new data so the system can immediately reflect the latest user preferences . We evaluate the performance of the algorithm on the MovieLens and Netflix data sets consisting of approximately 1,000,000 and 100,000,000 ratings respectively . This demonstrates that training the model using the on-line ADF approach yields state-of-the-art performance with the option of improving performance further if computational resources are available by performing multiple EP passes over the training data . \",\n",
       " 13518634: 'A game based approach to assign geographical relevance to web images Geographical context is very important for images . Millions of images on the Web have been already assigned latitude and longitude information . Due to the rapid proliferation of such images with geographical context , it is still difficult to effectively search and browse them , since we do not have ways to decide their relevance . In this paper , we focus on the geographical relevance of images , which is defined as to what extent the main objects in an image match landmarks at the location where the image was taken . Recently , researchers have proposed to use game based approaches to label large scale data such as Web images . However , previous works have not shown the quality of collected game logs in detail and how the logs can improve existing applications . To answer these questions , we design and implement a Web-based and multi-player game to collect human knowledge while people are enjoying the game . Then we thoroughly analyze the game logs obtained during a three week study with 147 participants and propose methods to determine the image geographical relevance . In addition , we conduct an experiment to compare our methods with a commercial search engine . Experimental results show that our methods dramatically improve image search relevance . Furthermore , we show that we can derive geographically relevant objects and their salient portion in images , which is valuable for a number of applications such as image location recognition . ',\n",
       " 13518670: 'Combining anchor text categorization and graph analysis for paid link detection In order to artificially boost the rank of commercial pages in search engine results , search engine optimizers pay for links to these pages on other websites . Identifying paid links is important for a web search engine to produce highly relevant results . In this paper we introduce a novel method of identifying such links . We start with training a classifier of anchor text topics and analyzing web pages for diversity of their outgoing commercial links . Then we use this information and analyze link graph of the Russian Web to find pages that sell links and sites that buy links and to identify the paid links . Testing on manually marked samples showed high efficiency of the algorithm . ',\n",
       " 13519640: \"Predicting click through rate for job listings Click Through Rate ( CTR ) is an important metric for ad systems , job portals , recommendation systems . CTR impacts publisher 's revenue , advertiser 's bid amounts in `` pay for performance '' business models . We learn regression models using features of the job , optional click history of job , features of `` related '' jobs . We show that our models predict CTR much better than predicting avg . CTR for all job listings , even in absence of the click history for the job listing . \",\n",
       " 13520597: 'Unsupervised query categorization using automatically-built concept graphs Automatic categorization of user queries is an important component of general purpose ( Web ) search engines , particularly for triggering rich , query-specific content and sponsored links . We propose an unsupervised learning scheme that reduces dramatically the cost of setting up and maintaining such a categorizer , while retaining good categorization power . The model is stored as a graph of concepts where graph edges represent the cross-reference between the concepts . Concepts and relations are extracted from query logs by an offline Web mining process , which uses a search engine as a powerful summarizer for building a concept graph . Empirical evaluation indicates that the system compares favorably on publicly available data sets ( such as KDD Cup 2005 ) as well as on portions of the current query stream of Yahoo ! Search , where it is already changing the experience of millions of Web search users . ',\n",
       " 13520879: 'A P2P based distributed services network for next generation mobile internet communications In this poster , we present a novel P2P ( Peer to Peer ) based distributed services network ( DSN ) , which is a next generation operable and manageable distributed core network architecture and functional structure , proposed by China Mobile for telecommunication services and wireless Internet . Our preliminary implementations of P2P VoIP ( Voice over Internet Protocol ) system over DSN platform demonstrate its effectiveness and promising future . ',\n",
       " 13521214: 'Keyword extraction for social snippets Today , a huge amount of text is being generated for social purposes on social networking services on the Web . Unlike traditional documents , such text is usually extremely short and tends to be informal . Analysis of such text benefit many applications such as advertising , search , and content filtering . In this work , we study one traditional text mining task on such new form of text , that is extraction of meaningful keywords . We propose several intuitive yet useful features and experiment with various classification models . Evaluation is conducted on Facebook data . Performances of various features and models are reported and compared . ',\n",
       " 13521570: 'The slashdot zoo : mining a social network with negative edges We analyze the corpus of user relationships of the Slashdot technology news site . The data was collected from the Slashdot Zoo feature where users of the website can tag other users as friends and foes , providing positive and negative endorsements . We adapt social network analysis techniques to the problem of negative edge weights . In particular , we consider signed variants of global network characteristics such as the clustering coefficient , node-level characteristics such as centrality and popularity measures , and link-level characteristics such as distances and similarity measures . We evaluate these measures on the task of identifying unpopular users , as well as on the task of predicting the sign of links and show that the network exhibits multiplicative transitivity which allows algebraic methods based on matrix multiplication to be used . We compare our methods to traditional methods which are only suitable for positively weighted edges . ',\n",
       " 13522059: 'Web service derivatives Web service development and usage has shifted from simple information processing services to high-value business services that are crucial to productivity and success . In order to deal with an increasing risk of unavailability or failure of mission-critical Web services we argue the need for advanced reservation of services in the form of derivatives . The contribution of this paper is twofold : First we provide an abstract model of a market design that enables the trade of derivatives for mission-critical Web services . Our model satisfies requirements that result from service characteristics such as intangibility and the impossibility to inventor services in order to meet fluctuating demand . It comprehends principles from models of incomplete markets such as the absence of a tradeable underlying and consistent arbitrage-free derivative pricing . Furthermore we provide an architecture for a Web service market that implements our model and describes the strategy space and interaction of market participants in the trading process of service derivatives . We compare the underlying pricing processes to existing derivative models in energy exchanges , discuss eventual shortcomings , and apply Wavelets to analyze actual data and extract long - and short-term trends . ',\n",
       " 13522731: \"Answering approximate queries over autonomous web databases To deal with the problem of empty or too little answers returned from a Web database in response to a user query , this paper proposes a novel approach to provide relevant and ranked query results . Based on the user original query , we speculate how much the user cares about each specified attribute and assign a corresponding weight to it . This original query is then rewritten as an approximate query by relaxing the query criteria range . The relaxation order of all specified attributes and the relaxed degree on each specified attribute are varied with the attribute weights . For the approximate query results , we generate users ' contextual preferences from database workload and use them to create a priori orders of tuples in an off-line preprocessing step . Only a few representative orders are saved , each corresponding to a set of contexts . Then , these orders and associated contexts are used at query time to expeditiously provide ranked answers . Results of a preliminary user study demonstrate that our query relaxation and results ranking methods can capture the user 's preferences effectively . The efficiency and effectiveness of our approach is also demonstrated by experimental result . \",\n",
       " 13523265: 'RankCompete : simultaneous ranking and clustering of web photos With the explosive growth of digital cameras and online media , it has become crucial to design efficient methods that help users browse and search large image collections . The recent VisualRank algorithm ( 4 ) employs visual similarity to represent the link structure in a graph so that the classic PageRank algorithm can be applied to select the most relevant images . However , measuring visual similarity is difficult when there exist diversified semantics in the image collection , and the results from VisualRank can not supply good visual summarization with diversity . This paper proposes to rank the images in a structural fashion , which aims to discover the diverse structure embedded in photo collections , and rank the images according to their similarity among local neighborhoods instead of across the entire photo collection . We design a novel algorithm named RankCompete , which generalizes the PageRank algorithm for the task of simultaneous ranking and clustering . The experimental results show that RankCompete outperforms VisualRank and provides an efficient but effective tool for organizing web photos . ',\n",
       " 13525236: 'Detecting image spam using local invariant features and pyramid match kernel Image spam is a new obfuscating method which spammers invented to more effectively bypass conventional text based spam filters . In this paper , we extract local invariant features of images and run a one-class SVM classifier which uses the pyramid match kernel as the kernel function to detect image spam . Experimental results demonstrate that our algorithm is effective for fighting image spam . ',\n",
       " 13526048: 'Topic initiator detection on the world wide web In this paper we introduce a new Web mining and search technique - Topic Initiator Detection ( TID ) on the Web . Given a topic query on the Internet and the resulting collection of time-stamped web documents which contain the query keywords , the task of TID is to automatically return which web document ( or its author ) initiated the topic or was the first to discuss about the topic . To deal with the TID problem , we design a system framework and propose algorithm InitRank ( Initiator Ranking ) to rank the web documents by their possibility to be the topic initiator . We first extract features from the web documents and design several topic initiator indicators . Then , we propose a TCL graph which integrates the Time , Content and Link information and design an optimization framework over the graph to compute InitRank . Experiments show that compared with baseline methods , such as direct time sorting , well-known link based ranking algorithms PageRank and HITS , InitRank achieves the best overall performance with high effectiveness and robustness . In case studies , we successfully detected ( 1 ) the first web document related to a famous rumor of an Australia product banned in USA and ( 2 ) the pre-release of IBM and Google Cloud Computing collaboration before the official announcement . ',\n",
       " 13526508: \"User-centric content freshness metrics for search engines In order to return relevant search results , a search engine must keep its local repository synchronized to the Web , but it is usually impossible to attain perfect freshness . Hence , it is vital for a production search engine continually to monitor and improve repository freshness . Most previous freshness metrics , formulated in the context of developing better synchronization policies , focused on the web crawler while ignoring other parts of a search engine . But , the freshness of documents in a web crawler does not necessarily translate directly into the freshness of search results as seen by users . We propose metrics for measuring freshness from a user 's perspective , which take into account the latency between when documents are crawled and when they are viewed by users , as well as the variation in user click and view frequency among different documents . We also describe a practical implementation of these metrics that were used in a production search engine . \",\n",
       " 13526671: 'Semantic wiki aided business process specification This paper formulates a collaborative system for modeling business application . The system uses a Semantic Wiki to enable collaboration between the various stakeholders involved in the design of the system and translates the captured intelligence into business models which are used for designing a business system . ',\n",
       " 13526814: 'Mining cultural differences from a large number of geotagged photos We propose a novel method to detect cultural differences over the world automatically by using a large amount of geotagged images on the photo sharingWeb sites such as Flickr . We employ the state-of-the-art object recognition technique developed in the research community of computer vision to mine representative photos of the given concept for representative local regions from a large-scale unorganized collection of consumer-generated geotagged photos . The results help us understand how objects , scenes or events corresponding to the same given concept are visually different depending on local regions over the world . ',\n",
       " 13526913: \"Co-browsing dynamic web pages Collaborative browsing , or co-browsing , is the co-navigation of the web with other people at-a-distance , supported by software that takes care of synchronizing the browsers . Current state-of-the-art solutions are able to do co-browsing of `` static web pages '' , and do not support the synchronization of JavaScript interactions . However , currently many web pages use JavaScript and Ajax techniques to create highly dynamic and interactive web applications . In this paper , we describe two approaches for co-browsing that both support the synchronization of the JavaScript and Ajax interactions of dynamic web pages . One approach is based on synchronizing the output of the JavaScript engine by sending over the changes made on the DOM tree . The other approach is based on synchronizing the input of the JavaScript engine by synchronizing UI events and incoming data . Since the latter solution offers a better user experience and is more scalable , it is elaborated in more detail . An important aspect of both approaches is that they operate at the DOM level . Therefore , the client-side can be implemented in JavaScript and no browser extensions are required . To the best of the authors ' knowledge this is the first DOM-level co-browsing solution that also enables co-browsing of the dynamic interaction parts of web pages . The presented co-browsing solution has been implemented in a research demonstrator which allows users to do co-browsing of web-applications on browser-based networked televisions . \",\n",
       " 13527964: 'A messaging API for inter-widgets communication Widget containers are used everywhere on the Web , for instance as customizable start pages to Web desktops . In this poster , we describe the extension of a widget container with an inter-widgets communication layer , as well as the subsequent application programming interfaces ( APIs ) added to the Widget object to support this feature . We present the benefits of a drag and drop facility within widgets and conclude by a call for standardization of inter-widgets communication on the Web . ',\n",
       " 13528329: 'Towards lightweight and efficient DDOS attacks detection for web server In this poster , based on our previous work in building a lightweight DDoS ( Distributed Denial-of-Services ) attacks detection mechanism for web server using TCM-KNN ( Transductive Confidence Machines for K-Nearest Neighbors ) and genetic algorithm based instance selection methods , we further propose a more efficient and effective instance selection method , named E-FCM ( Extend Fuzzy C-Means ) . By using this method , we can obtain much cheaper training time for TCM-KNN while ensuring high detection performance . Therefore , the optimized mechanism is more suitable for lightweight DDoS attacks detection in real network environment . ',\n",
       " 13528523: 'Web image retrieval reranking with multi-view clustering General image retrieval is often carried out by a text-based search engine , such as Google Image Search . In this case , natural language queries are used as input to the search engine . Usually , the user queries are quite ambiguous and the returned results are not well-organized as the ranking often done by the popularity of an image . In order to address these problems , we propose to use both textual and visual contents of retrieved images to reRank web retrieved results . In particular , a machine learning technique , a multi-view clustering algorithm is proposed to reorganize the original results provided by the text-based search engine . Preliminary results validate the effectiveness of the proposed framework . ',\n",
       " 13528887: \"Sitemaps : above and beyond the crawl of duty Comprehensive coverage of the public web is crucial to web search engines . Search engines use crawlers to retrieve pages and then discover new ones by extracting the pages ' outgoing links . However , the set of pages reachable from the publicly linked web is estimated to be significantly smaller than the invisible web , the set of documents that have no incoming links and can only be retrieved through web applications and web forms . The Sitemaps protocol is a fast-growing web protocol supported jointly by major search engines to help content creators and search engines unlock this hidden data by making it available to search engines . In this paper , we perform a detailed study of how `` classic '' discovery crawling compares with Sitemaps , in key measures such as coverage and freshness over key representative websites as well as over billions of URLs seen at Google . We observe that Sitemaps and discovery crawling complement each other very well , and offer different tradeoffs . \",\n",
       " 13529859: 'Automatically filling form-based web interfaces with free text inputs On the web of today the most prevalent solution for users to interact with data-intensive applications is the use of form-based interfaces composed by several data input fields , such as text boxes , radio buttons , pull-down lists , check boxes , etc. . Although these interfaces are popular and effective , in many cases , free text interfaces are preferred over form-based ones . In this paper we discuss the proposal and the implementation of a novel IR-based method for using data rich free text to interact with form-based interfaces . Our solution takes a free text as input , extracts implicitly data values from it and fills appropriate fields using them . For this task , we rely on values of previous submissions for each field , which are freely obtained from the usage of form-based interfaces ',\n",
       " 13530223: \"Searching for events in the blogosphere Over the last few years , blogs ( web logs ) have gained massive popularity and have become one of the most influential web social media in our times . Every blog post in the Blogosphere has a well defined timestamp , which is not taken into account by search engines . By conducting research regarding this feature of the Blogosphere , we can attempt to discover bursty terms and correlations between them during a time interval . We apply Kleinberg 's automaton on extracted titles of blog posts to discover bursty terms , we introduce a novel representation of a term 's burstiness evolution called State Series and we employ a Euclidean-based distance metric to discover potential correlations between terms without taking into account their context . We evaluate the results trying to match them with real life events . Finally , we propose some ideas for further evaluation techniques and future research in the field . \",\n",
       " 13530633: \"Compressed web indexes Web search engines use indexes to efficiently retrieve pages containing specified query terms , as well as pages linking to specified pages . The problem of compressed indexes that permit such fast retrieval has a long history . We consider the problem : assuming that the terms in ( or links to ) a page are generated from a probability distribution , how well compactly can we build such indexes that allow fast retrieval ? Of particular interest is the case when the probability distribution is Zipfian ( or a similar power law ) , since these are the distributions that arise on the web . We obtain sharp bounds on the space requirement of Boolean indexes for text documents that follow Zipf 's law . In the process we develop a general technique that applies to any probability distribution , not necessarily a power law ; this is the first analysis of compression in indexes under arbitrary distributions . Our bounds lead to quantitative versions of rules of thumb that are folklore in indexing . Our experiments on several document collections show that the distribution of terms appears to follow a double-Pareto law rather than Zipf 's law . Despite widely varying sets of documents , the index sizes observed in the experiments conform well to our theoretical predictions . \",\n",
       " 13530974: 'Efficient overlap and content reuse detection in blogs and online news articles The use of blogs to track and comment on real world ( political , news , entertainment ) events is growing . Similarly , as more individuals start relying on the Web as their primary information source and as more traditional media outlets try reaching consumers through alternative venues , the number of news sites on the Web is also continuously increasing . Content-reuse , whether in the form of extensive quotations or content borrowing across media outlets , is very common in blogs and news entries outlets tracking the same real-world event . Knowledge about which web entries re-use content from which others can be an effective asset when organizing these entries for presentation . On the other hand , this knowledge is not cheap to acquire : considering the size of the related space web entries , it is essential that the techniques developed for identifying re-use are fast and scalable . Furthermore , the dynamic nature of blog and news entries necessitates incremental processing for reuse detection . In this paper , we develop a novel qSign algorithm that efficiently and effectively analyze the blogosphere for quotation and reuse identification . Experiment results show that with qSign processing time gains from 10X to 100X are possible while maintaining reuse detection rates of upto 90 % . Furthermore , processing time gains can be pushed multiple orders of magnitude ( from 100X to 1000X ) for 70 % recall . ',\n",
       " 13531284: \"SGPS : a semantic scheme for web service similarity Today 's Web becomes a platform for services to be dynamically interconnected to produce a desired outcome . It is important to formalize the semantics of the contextual elements of web services . In this paper , we propose a novel technique called Semantic Genome Propagation Scheme ( SGPS ) for measuring similarity between semantic concepts . We show how SGPS is used to compute a multi-dimensional similarity between two services . We evaluate the SGPS similarity measurement in terms of the similarity performance and scalability . \",\n",
       " 13532199: 'Dataplorer : a scalable search engine for the data web More and more structured information in the form of semantic data is nowadays available . It offers a wide range of new possibilities especially for semantic search and Web data integration . However , their effective exploitation still brings about a number of challenges , e.g. usability , scalability and uncertainty . In this paper , we present Dataplorer , a solution designed to address these challenges . We consider the usability through the use of hybrid queries and faceted search , while still preserving the scalability thanks to an extension of inverted index to support this type of query . Moreover , Dataplorer deals with uncertainty by means of a powerful ranking scheme to find relevant results . Our experimental results show that our proposed approach is promising and it makes us believe that it is possible to extend the current IR infrastructure to query and search the Web of data . ',\n",
       " 13533362: 'Spatio-temporal models for estimating click-through rate We propose novel spatio-temporal models to estimate click-through rates in the context of content recommendation . We track article CTR at a fixed location over time through a dynamic Gamma-Poisson model and combine information from correlated locations through dynamic linear regressions , significantly improving on per-location model . Our models adjust for user fatigue through an exponential tilt to the first-view CTR ( probability of click on first article exposure ) that is based only on user-specific repeat-exposure features . We illustrate our approach on data obtained from a module ( Today Module ) published regularly on Yahoo ! Front Page and demonstrate significant improvement over commonly used baseline methods . Large scale simulation experiments to study the performance of our models under different scenarios provide encouraging results . Throughout , all modeling assumptions are validated via rigorous exploratory data analysis . ',\n",
       " 13533459: 'OpenRuleBench : an analysis of the performance of rule engines The Semantic Web initiative has led to an upsurge of the interest in rules as a general and powerful way of processing , combining , and analyzing semantic information . Since several of the technologies underlying rule-based systems are already quite mature , it is important to understand how such systems might perform on the Web scale . OpenRuleBench is a suite of benchmarks for analyzing the performance and scalability of different rule engines . Currently the study spans five different technologies and eleven systems , but OpenRuleBench is an open community resource , and contributions from the community are welcome . In this paper , we describe the tested systems and technologies , the methodology used in testing , and analyze the results . ',\n",
       " 13533717: \"Smart Miner : a new framework for mining large scale web usage data In this paper , we propose a novel framework called Smart-Miner for web usage mining problem which uses link information for producing accurate user sessions and frequent navigation patterns . Unlike the simple session concepts in the time and navigation based approaches , where sessions are sequences of web pages requested from the server or viewed in the browser , Smart Miner sessions are set of paths traversed in the web graph that corresponds to users ' navigations among web pages . We have modeled session construction as a new graph problem and utilized a new algorithm , Smart-SRA , to solve this problem efficiently . For the pattern discovery phase , we have developed an efficient version of the Apriori-All technique which uses the structure of web graph to increase the performance . From the experiments that we have performed on both real and simulated data , we have observed that Smart-Miner produces at least 30 % more accurate web usage patterns than other approaches including previous session construction methods . We have also studied the effect of having the referrer information in the web server logs to show that different versions of Smart-SRA produce similar results . Our another contribution is that we have implemented distributed version of the Smart Miner framework by employing Map\\\\/Reduce Paradigm . We conclude that we can efficiently process terabytes of web server logs belonging to multiple web sites by our scalable framework . \",\n",
       " 13533950: 'Rapid prototyping of semantic mash-ups through semantic web pipes The use of RDF data published on the Web for applications is still a cumbersome and resource-intensive task due to the limited software support and the lack of standard programming paradigms to deal with everyday problems such as combination of RDF data from dierent sources , object identifier consolidation , ontology alignment and mediation , or plain querying and filtering tasks . In this paper we present a framework , Semantic Web Pipes , that supports fast implementation of Semantic data mash-ups while preserving desirable properties such as abstraction , encapsulation , component-orientation , code re-usability and maintainability which are common and well supported in other application areas . ',\n",
       " 13534577: 'Crosslanguage blog mining and trend visualisation People use weblogs to express thoughts , present ideas and share knowledge , therefore weblogs are extraordinarily valuable resources , amongs others , for trend analysis . Trends are derived from the chronological sequence of blog post count per topic . The comparison with a reference corpus allows qualitative statements over identified trends . We propose a crosslanguage blog mining and trend visualisation system to analyze blogs across languages and topics . The trend visualisation facilitates the identification of trends and the comparison with the reference news article corpus . To prove the correctness of our system we computed the correlation between trends in blogs and news articles for a subset of blogs and topics . The evaluation corroborated our hypothesis of a high correlation coefficient for these subsets and therefore the correctness of our system for different languages and topics is proven . ',\n",
       " 13534613: 'An experimental study of large-scale mobile social network Mobile social network is a typical social network where one or more individuals of similar interests or commonalities , conversing and connecting with one another using the mobile phone . Our works in this paper focus on the experimental study for this kind of social network with the support of large-scale real mobile call data . The main contributions can be summarized as three-fold : firstly , a large-scale real mobile phone call log of one city has been extracted from a mobile phone carrier in China to construct mobile social network ; secondly , common features of traditional social networks , such as power law distribution and small diameter etc , have been experimented , with which we confirm that the mobile social network is a typical scale-free network and has small-world phenomenon ; lastly , different from traditional analytical methods , important properties of the actors , such as gender and age , have been introduced into our experiments with some interesting findings about human behavior , for example , the middle-age people are more active than the young and old people , and the female is unusual more active than the male while in the old age . ',\n",
       " 13535450: 'Detecting soft errors by redirection classification A soft error redirection is a URL redirection to a page that returns the HTTP status code 200 ( OK ) but has actually no relevant content to the client request . Since such redirections degrade the performance of web search engines in many ways , it is highly desirable to remove as many of them as possible . We propose a novel approach to detect soft error redirections by analyzing redirection logs collected during crawling operation . Experimental results on huge crawl data show that our measure can classify soft error redirections effectively . ',\n",
       " 13536580: 'Nearest-neighbor caching for content-match applications Motivated by contextual advertising systems and other web applications involving efficiency-accuracy tradeoffs , we study similarity caching . Here , a cache hit is said to occur if the requested item is similar but not necessarily equal to some cached item . We study two objectives that dictate the efficiency-accuracy tradeoff and provide our caching policies for these objectives . By conducting extensive experiments on real data we show similarity caching can significantly improve the efficiency of contextual advertising systems , with minimal impact on accuracy . Inspired by the above , we propose a simple generative model that embodies two fundamental characteristics of page requests arriving to advertising systems , namely , long-range dependences and similarities . We provide theoretical bounds on the gains of similarity caching in this model and demonstrate these gains empirically by fitting the actual data to the model . ',\n",
       " 13536686: \"XQuery in the browser Since the invention of the Web , the browser has become more and more powerful . By now , it is a programming and execution environment in itself . The predominant language to program applications in the browser today is JavaScript . With browsers becoming more powerful , JavaScript has been extended and new layers have been added ( e.g. , DOM-Support and XPath ) . Today , JavaScript is very successful and applications and GUI features implemented in the browser have become increasingly complex . The purpose of this paper is to improve the programmability of Web browsers by enabling the execution of XQuery programs in the browser . Although it has the potential to ideally replace JavaScript , it is possible to run it in addition to JavaScript for more flexibility . Furthermore , it allows instant code migration from the server to the client and vice-versa . This enables a significant simplification of the technology stack . The intuition is that programming the browser involves mostly XML ( i.e. , DOM ) navigation and manipulation , and the XQuery family of W3C standards were designed exactly for that purpose . The paper proposes extensions to XQuery for Web browsers and gives a number of examples that demonstrate the usefulness of XQuery for the development of AJAX-style applications . Furthermore , the paper presents the design of an XQuery plug-in for Microsoft 's Internet Explorer . The paper also gives examples of applications which were developed with the help of this plug-in . \",\n",
       " 13536936: 'Raise semantics at the user level for dynamic and interactive SOA-based portals In this paper , we describe the fully dynamic semantic portal we implemented , integrating Semantic Web technologies and Service Oriented Architecture ( SOA ) . The goals of the portal are twofold : first it helps administrators to easily propose new features in the portal using semantics to ease the orchestration process ; secondly it automatically generates a customized user interface for these scenarios . This user interface takes into account different devices and assists end-users in the use of the portal taking benefit of context awareness . All the added-value of this portal is based on a core semantics defined by an ontology . We present here the main features of this portal and how it was implemented using state-of-the-art technologies and frameworks . ',\n",
       " 13537427: \"Search result re-ranking based on gap between search queries and social tags Both search engine click-through log and social annotation have been utilized as user feedback for search result re-ranking . However , to our best knowledge , no previous study has explored the correlation between these two factors for the task of search result ranking . In this paper , we show that the gap between search queries and social tags of the same Web page can well reflect its user preference score . Motivated by this observation , we propose a novel algorithm , called Query-Tag-Gap ( QTG ) , to re-rank search results for better user satisfaction . Intuitively , on one hand , the search users ' intentions are generally described by their queries before they read the search results . On the other hand , the Web annotators semantically tag Web pages after they read the content of the pages . The difference between users ' recognition of the same page before and after they read it is a good reflection of user satisfaction . In this extended abstract , we formally define the query set and tag set of the same page as users ' pre - and post - knowledge respectively . We empirically show the strong correlation between user satisfaction and user 's knowledge gap before and after reading the page . Based on this gap , experiments have shown outstanding performance of our proposed QTG algorithm in search result re-ranking . \",\n",
       " 13537435: \"Content hole search in community-type content In community-type content such as blogs and SNSs , we call the user 's unawareness of information as a `` content hole '' and the search for this information as a `` content hole search . '' A content hole search differs from similarity searching and has a variety of types . In this paper , we propose different types of content holes and define each type . We also propose an analysis of dialogue related to community-type content and introduce content hole search by using Wikipedia as an example . \",\n",
       " 13538141: 'Exploiting web search to generate synonyms for entities Tasks recognizing named entities such as products , people names , or locations from documents have recently received significant attention in the literature . Many solutions to these tasks assume the existence of reference entity tables . An important challenge that needs to be addressed in the entity extraction task is that of ascertaining whether or not a candidate string approximately matches with a named entity in a given reference table . Prior approaches have relied on string-based similarity which only compare a candidate string and an entity it matches with . In this paper , we exploit web search engines in order to define new similarity functions . We then develop efficient techniques to facilitate approximate matching in the context of our proposed similarity functions . In an extensive experimental evaluation , we demonstrate the accuracy and efficiency of our techniques . ',\n",
       " 13538176: \"Automated construction of web accessibility models from transaction click-streams Screen readers , the dominant assistive technology used by visually impaired people to access the Web , function by speaking out the content of the screen serially . Using screen readers for conducting online transactions can cause considerable information overload , because transactions , such as shopping and paying bills , typically involve a number of steps spanning several web pages . One can combat this overload by using a transaction model for web accessibility that presents only fragments of web pages that are needed for doing transactions . We can realize such a model by coupling a process automaton , encoding states of a transaction , with concept classifiers that identify page fragments `` relevant '' to a particular state of the transaction . In this paper we present a fully automated process that synergistically combines several techniques for transforming unlabeled click-stream data generated by transactions into a transactionmodel . These techniques include web content analysis to partition a web page into segments consisting of semantically related content , contextual analysis of data surrounding clickable objects in a page , and machine learning methods , such as clustering of page segments based on contextual analysis , statistical classification , and automata learning . The use of unlabeled click streams in building transaction models has important benefits : ( i ) visually impaired users do not have to depend on sighted users for creating manually labeled training data to construct the models ; ( ii ) it is possible to mine personalized models from unlabeled transaction click-streams associated with sites that visually impaired users visit regularly ; ( iii ) since unlabeled data is relatively easy to obtain , it is feasible to scale up the construction of domain-specific transaction models ( e.g. , separate models for shopping , airline reservations , bill payments , etc. ) ; ( iv ) adjusting the performance of deployed models over timtime with new training data is also doable . We provide preliminary experimental evidence of the practical effectiveness of both domain-specific , as well as personalized accessibility transaction models built using our approach . Finally , this approach is applicable for building transaction models for mobile devices with limited-size displays , as well as for creating wrappers for information extraction from web sites . \",\n",
       " 13538488: 'Rare item detection in e-commerce site As the largest online marketplace in the world , eBay has a huge inventory where there are plenty of great rare items with potentially large , even rapturous buyers . These items are obscured in long tail of eBay item listing and hard to find through existing searching or browsing methods . It is observed that there are great rarity demands from users according to eBay query log . To keep up with the demands , the paper proposes a method to automatically detect rare items in eBay online listing . A large set of features relevant to the task are investigated to filter items and further measure item rareness . The experiments on the most rarity-demand-intensitive domains show that the method may effectively detect rare items ( ) 90 % precision ) . ',\n",
       " 13539372: 'Fast dynamic reranking in large graphs In this paper we consider the problem of re-ranking search results by incorporating user feedback . We present a graph theoretic measure for discriminating irrelevant results from relevant results using a few labeled examples provided by the user . The key intuition is that nodes relatively closer ( in graph topology ) to the relevant nodes than the irrelevant nodes are more likely to be relevant . We present a simple sampling algorithm to evaluate this measure at specific nodes of interest , and an efficient branch and bound algorithm to compute the top k nodes from the entire graph under this measure . On quantifiable prediction tasks the introduced measure outperforms other diffusion-based proximity measures which take only the positive relevance feedback into account . On the Entity-Relation graph built from the authors and papers of the entire DBLP citation corpus ( 1.4 million nodes and 2.2 million edges ) our branch and bound algorithm takes about 1.5 seconds to retrieve the top 10 nodes w.r.t. this measure with 10 labeled nodes . ',\n",
       " 13540032: \"Community gravity : measuring bidirectional effects by trust and rating on online social networks Several attempts have been made to analyze customer behavior on online E-commerce sites . Some studies particularly emphasize the social networks of customers . Users ' reviews and ratings of a product exert effects on other consumers ' purchasing behavior . Whether a user refers to other users ' ratings depends on the trust accorded by a user to the reviewer . On the other hand , the trust that is felt by a user for another user correlates with the similarity of two users ' ratings . This bidirectional interaction that involves trust and rating is an important aspect of understanding consumer behavior in online communities because it suggests clustering of similar users and the evolution of strong communities . This paper presents a theoretical model along with analyses of an actual online E-commerce site . We analyzed a large community site in Japan : @cosme . The noteworthy characteristics of @cosme are that users can bookmark their trusted users ; in addition , they can post their own ratings of products , which facilitates our analyses of the ratings ' bidirectional effects on trust and ratings . We describe an overview of the data in @cosme , analyses of effects from trust to rating and vice versa , and our proposition of a measure of community gravity , which measures how strongly a user might be attracted to a community . Our study is based on the @cosme dataset in addition to the Epinions dataset . It elucidates important insights and proposes a potentially important measure for mining online social networks . \",\n",
       " 13540651: 'A flight meta-search engine with metamorph We demonstrate a flight meta-search engine that is based on the Metamorph framework . Metamorph provides mechanisms to model web forms together with the interactions which are needed to fulfil a request , and can generate interaction sequences that pose queries using these web forms and collect the results . In this paper , we discuss an interesting new feature that makes use of the forms themselves as an information source . We show how data can be extracted from web forms ( rather than the data behind web forms ) to generate a graph of flight connections between cities . The flight connection graph allows us to vastly reduce the number of queries that the engine sends to airline websites in the most interesting search scenarios ; those that involve the controversial practice of creative ticketing , in which agencies attempt to find lower price fares by using more than one airline for a journey . We describe a system which attains data from a number of websites to identify promising routes and prune the search tree . Heuristics that make use of geographical information and an estimation of cost based on historical data are employed . The results are then made available to improve the quality of future search requests . ',\n",
       " 13542825: 'The value of socially tagged urls for a search engine Social bookmarking has emerged as a growing source of human generated content on the web . In essence , bookmarking involves URLs and tags on them . In this paper , we perform a large scale study of the usefulness of bookmarked URLs from the top social bookmarking site Delicious . Instead of focusing on the dimension of tags , which has been covered in the previous work , we explore social bookmarking from the dimension of URLs . More specifically , we investigate the Delicious URLs and their content to quantify their value to a search engine . For their value in leading to good content , we show that the Delicious URLs have higher quality content and more external outlinks . For their value in satisfying users , we show that the Delicious URLs have more clicked URLs as well as get more clicks . We suggest that based on their value , the Delicious URLs should be used as another source of seed URLs for crawlers . ',\n",
       " 13543051: 'A geographical analysis of knowledge production in computer science We analyze knowledge production in Computer Science by means of coauthorship networks . For this , we consider 30 graduate programs of different regions of the world , being 8 programs in Brazil , 16 in North America ( 3 in Canada and 13 in the United States ) , and 6 in Europe ( 2 in France , 1 in Switzerland and 3 in the United Kingdom ) . We use a dataset that consists of 176,537 authors and 352,766 publication entries distributed among 2,176 publication venues . The results obtained for different metrics of collaboration social networks indicate the process of knowledge creation has changed differently for each region . Research is increasingly done in teams across different fields of Computer Science . The size of the giant component indicates the existence of isolated collaboration groups in the European network , contrasting to the degree of connectivity found in the Brazilian and North-American counterparts . We also analyzed the temporal evolution of the social networks representing the three regions . The number of authors per paper experienced an increase in a time span of 12 years . We observe that the number of collaborations between authors grows faster than the number of authors , benefiting from the existing network structure . The temporal evolution shows differences between well-established fields , such as Databases and Computer Architecture , and emerging fields , like Bioinformatics and Geoinformatics . The patterns of collaboration analyzed in this paper contribute to an overall understanding of Computer Science research in different geographical regions that could not be achieved without the use of complex networks and a large publication database . ',\n",
       " 13543053: 'Visual diversification of image search results Due to the reliance on the textual information associated with an image , image search engines on the Web lack the discriminative power to deliver visually diverse search results . The textual descriptions are key to retrieve relevant results for a given user query , but at the same time provide little information about the rich image content . In this paper we investigate three methods for visual diversification of image search results . The methods deploy lightweight clustering techniques in combination with a dynamic weighting function of the visual features , to best capture the discriminative aspects of the resulting set of images that is retrieved . A representative image is selected from each cluster , which together form a diverse result set . Based on a performance evaluation we find that the outcome of the methods closely resembles human perception of diversity , which was established in an extensive clustering experiment carried out by human assessors . ',\n",
       " 13543554: 'Two birds with one stone : a graph-based framework for disambiguating and tagging people names in web search The ever growing volume of Web data makes it increasingly challenging to accurately find relevant information about a specific person on the Web . To address the challenge caused by name ambiguity in Web people search , this paper explores a novel graph-based framework to both disambiguate and tag people entities in Web search results . Experimental results demonstrate the effectiveness of the proposed framework in tag discovery and name disambiguation . ',\n",
       " 13543822: 'The web of nations In this paper , we report on a large-scale study of structural differences among the national webs . The study is based on a web-scale crawl conducted in the summer 2008 . More specifically , we study two graphs derived from this crawl , the nation graph , with nodes corresponding to nations and edges - to links among nations , and the host graph , with nodes corresponding to hosts and edges - to hyperlinks among pages on the hosts . Contrary to some of the previous work ( 2 ) , our results show that webs of different nations are often very different from each other , both in terms of their internal structure , and in terms of their connectivity with other nations . ',\n",
       " 13544213: 'Discovering the staring people from social networks In this paper , we study a novel problem of staring people discovery from social networks , which is concerned with finding people who are not only authoritative but also sociable in the social network . We formalize this problem as an optimization programming problem . Taking the co-author network as a case study , we define three objective functions and propose two methods to combine these objective functions . A genetic algorithm based method is further presented to solve this problem . Experimental results show that the proposed solution can effectively find the staring people from social networks . ',\n",
       " 13544630: 'Reliability analysis using weighted combinational models for web-based software In the past , some researches suggested that engineers can use combined software reliability growth models ( SRGMs ) to obtain more accurate reliability prediction during testing . In this paper , three weighted combinational models , namely , equal , linear , and nonlinear weight , are proposed for reliability estimation of web-based software . We further investigate the estimation accuracy of using genetic algorithm to determine the weight assignment for the proposed models . Preliminary result shows that the linearly and nonlinearly weighted combinational models have better prediction capability than single SRGM and equally weighted combinational model for web-based software . ',\n",
       " 13545447: \"A general framework for adaptive and online detection of web attacks Detection of web attacks is an important issue in current defense-in-depth security framework . In this paper , we propose a novel general framework for adaptive and online detection of web attacks . The general framework can be based on any online clustering methods . A detection model based on the framework is able to learn online and deal with `` concept drift '' in web audit data streams . Str-DBSCAN that we extended DBSCAN to streaming data as well as StrAP are both used to validate the framework . The detection model based on the framework automatically labels the web audit data and adapts to normal behavior changes while identifies attacks through dynamical clustering of the streaming data . A very large size of real HTTP Log data collected in our institute is used to validate the framework and the model . The preliminary testing results demonstrated its effectiveness . \",\n",
       " 13601457: 'Mining for personal name aliases on the web We propose a novel approach to find aliases of a given name from the web . We exploit a set of known names and their aliases as training data and extract lexical patterns that convey information related to aliases of names from text snippets returned by a web search engine . The patterns are then used to find candidate aliases of a given name . We use anchor texts and hyperlinks to design a word co-occurrence model and define numerous ranking scores to evaluate the association between a name and its candidate aliases . The proposed method outperforms numerous baselines and previous work on alias extraction on a dataset of personal names , achieving a statistically significant mean reciprocal rank of 0.6718 . Moreover , the aliases extracted using the proposed method improve recall by 20 % in a relation-detection task . ',\n",
       " 13621384: 'Social and semantics analysis via non-negative matrix factorization Social media such as Web forum often have dense interactions between user and content where network models are often appropriate for analysis . Joint non-negative matrix factorization model of participation and content data can be viewed as a bipartite graph model between users and media and is proposed for analysis social media . The factorizations allow simultaneous automatic discovery of leaders and sub-communities in the Web forum as well as the core latent topics in the forum . Results on topic detection of Web forums and cluster analysis show that social features are highly effective for forum analysis . ',\n",
       " 13638975: \"Winner takes all : competing viruses or ideas on fair-play networks Given two competing products ( or memes , or viruses etc. ) spreading over a given network , can we predict what will happen at the end , that is , which product will ` win ' , in terms of highest market share ? One may naively expect that the better product ( stronger virus ) will just have a larger footprint , proportional to the quality ratio of the products ( or strength ratio of the viruses ) . However , we prove the surprising result that , under realistic conditions , for any graph topology , the stronger virus completely wipes-out the weaker one , thus not merely ` winning ' but ` taking it all ' . In addition to the proofs , we also demonstrate our result with simulations over diverse , real graph topologies , including the social-contact graph of the city of Portland OR ( about 31 million edges and 1 million nodes ) and internet AS router graphs . Finally , we also provide real data about competing products from Google-Insights , like Facebook-Myspace , and we show again that they agree with our analysis . \",\n",
       " 13714941: \"Merkle tree authentication of HTTP responses We propose extensions to existing web protocols that allow proofs of authenticity of HTTP server responses , whether or not the HTTP server is under the control of the publisher . These extensions protect users from content that may be substituted by malicious servers , and therefore have immediate applications in improving the security of web caching , mirroring , and relaying systems that rely on untrusted machines ( 2,4 ) . Our proposal relies on Merkle trees to support 200 and 404 response authentication while requiring only a single cryptographic hash of trusted data per repository . While existing web protocols such as HTTPS can provide authenticity guarantees ( in addition to confidentiality ) , HTTPS consumes significantly more computational resources , and requires that the hosting server act without malice in generating responses and in protecting the publisher 's private key . \",\n",
       " 13753027: 'Document hierarchies from text and links Hierarchical taxonomies provide a multi-level view of large document collections , allowing users to rapidly drill down to fine-grained distinctions in topics of interest . We show that automatically induced taxonomies can be made more robust by combining text with relational links . The underlying mechanism is a Bayesian generative model in which a latent hierarchical structure explains the observed data -- thus , finding hierarchical groups of documents with similar word distributions and dense network connections . As a nonparametric Bayesian model , our approach does not require pre-specification of the branching factor at each non-terminal , but finds the appropriate level of detail directly from the data . Unlike many prior latent space models of network structure , the complexity of our approach does not grow quadratically in the number of documents , enabling application to networks with more than ten thousand nodes . Experimental results on hypertext and citation network corpora demonstrate the advantages of our hierarchical , multimodal approach . ',\n",
       " 13756266: \"Trust analysis with clustering Web provides rich information about a variety of objects . Trustability is a major concern on the web . Truth establishment is an important task so as to provide the right information to the user from the most trustworthy source . Trustworthiness of information provider and the confidence of the facts it provides are inter-dependent on each other and hence can be expressed iteratively in terms of each other . However , a single information provider may not be the most trustworthy for all kinds of information . Every information provider has its own area of competence where it can perform better than others . We derive a model that can evaluate trustability on objects and information providers based on clusters ( groups ) . We propose a method which groups the set of objects for which similar set of providers provide `` good '' facts , and provides better accuracy in addition to high quality object clusters . \",\n",
       " 13778833: 'Design and implementation of contextual information portals This paper presents a system for enabling offline web use to satisfy the information needs of disconnected communities . We describe the design , implementation , evaluation , and pilot deployment of an automated mechanism to construct Contextual Information Portals ( CIPs ) . CIPs are large searchable information repositories of web pages tailored to the information needs of a target population . We combine an efficient classifier with a focused crawler to gather the web pages for the portal for any given topic . Given a set of topics of interest , our system constructs a CIP containing the most relevant pages from the web across these topics . Using several secondary school course syllabi , we demonstrate the effectiveness of our system for constructing CIPs for use as an education resource . We evaluate our system across several metrics : classification accuracy , crawl scalability , crawl accuracy and harvest rate . We describe the utility and usability of our system based on a preliminary deployment study at an after-school program in India , and also outline our ongoing larger-scale pilot deployment at five schools in Kenya . ',\n",
       " 13785303: 'Simplifying friendlist management Online social networks like Facebook allow users to connect , communicate , and share content . The popularity of these services has lead to an information overload for their users ; the task of simply keeping track of different interactions has become daunting . To reduce this burden , sites like Facebook allows the user to group friends into specific lists , known as friendlists , aggregating the interactions and content from all friends in each friendlist . While this approach greatly reduces the burden on the user , it still forces the user to create and populate the friendlists themselves and , worse , makes the user responsible for maintaining the membership of their friendlists over time . We show that friendlists often have a strong correspondence to the structure of the social network , implying that friendlists may be automatically inferred by leveraging the social network structure . We present a demonstration of Friendlist Manager , a Facebook application that proposes friendlists to the user based on the structure of their local social network , allows the user to tweak the proposed friendlists , and then automatically creates the friendlists for the user . ',\n",
       " 13788626: \"Measuring the web crawler ethics Web crawlers are highly automated and seldom regulated manually . The diversity of crawler activities often leads to ethical problems such as spam and service attacks . In this research , quantitative models are proposed to measure the web crawler ethics based on their behaviors on web servers . We investigate and define rules to measure crawler ethics , referring to the extent to which web crawlers respect the regulations set forth in robots . txt configuration files . We propose a vector space model to represent crawler behavior and measure the ethics of web crawlers based on the behavior vectors . The results show that ethicality scores vary significantly among crawlers . Most commercial web crawlers ' behaviors are ethical . However , many commercial crawlers still consistently violate or misinterpret certain robots . txt rules . We also measure the ethics of big search engine crawlers in terms of return on investment . The results show that Google has a higher score than other search engines for a US website but has a lower score than Baidu for Chinese websites . \",\n",
       " 13788838: 'Spotting fake reviewer groups in consumer reviews Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making . However , due to the reason of profit or fame , people try to game the system by opinion spamming ( e.g. , writing fake reviews ) to promote or demote some target products . For reviews to reflect genuine user experiences and opinions , such spam reviews should be detected . Prior works on opinion spam focused on detecting fake reviews and individual fake reviewers . However , a fake reviewer group ( a group of reviewers who work collaboratively to write fake reviews ) is even more damaging as they can take total control of the sentiment on the target product due to its size . This paper studies spam detection in the collaborative setting , i.e. , to discover fake reviewer groups . The proposed method first uses a frequent itemset mining method to find a set of candidate groups . It then uses several behavioral models derived from the collusion phenomenon among fake reviewers and relation models based on the relationships among groups , individual reviewers , and products they reviewed to detect fake reviewer groups . Additionally , we also built a labeled dataset of fake reviewer groups . Although labeling individual fake reviews and reviewers is very hard , to our surprise labeling fake reviewer groups is much easier . We also note that the proposed technique departs from the traditional supervised learning approach for spam detection because of the inherent nature of our problem which makes the classic supervised learning approach less effective . Experimental results show that the proposed method outperforms multiple strong baselines including the state-of-the-art supervised classification , regression , and learning to rank algorithms . ',\n",
       " 13801324: \"Understanding and combating link farming in the twitter social network Recently , Twitter has emerged as a popular platform for discovering real-time information on the Web , such as news stories and people 's reaction to them . Like the Web , Twitter has become a target for link farming , where users , especially spammers , try to acquire large numbers of follower links in the social network . Acquiring followers not only increases the size of a user 's direct audience , but also contributes to the perceived influence of the user , which in turn impacts the ranking of the user 's tweets by search engines . In this paper , we first investigate link farming in the Twitter network and then explore mechanisms to discourage the activity . To this end , we conducted a detailed analysis of links acquired by over 40,000 spammer accounts suspended by Twitter . We find that link farming is wide spread and that a majority of spammers ' links are farmed from a small fraction of Twitter users , the social capitalists , who are themselves seeking to amass social capital and links by following back anyone who follows them . Our findings shed light on the social dynamics that are at the root of the link farming problem in Twitter network and they have important implications for future designs of link spam defenses . In particular , we show that a simple user ranking scheme that penalizes users for connecting to spammers can effectively address the problem by disincentivizing users from linking with other users simply to gain influence . \",\n",
       " 13819642: \"On measuring the quality of Wikipedia articles This paper discusses an approach to modeling and measuring information quality of Wikipedia articles . The approach is based on the idea that the quality of Wikipedia articles with distinctly different profiles needs to be measured using different information quality models . We report on our initial study , which involved two categories of Wikipedia articles : `` stabilized '' ( those , whose content has not undergone major changes for a significant period of time ) and `` controversial '' ( the articles , which have undergone vandalism , revert wars , or whose content is subject to internal discussions between Wikipedia editors ) . We present simple information quality models and compare their performance on a subset of Wikipedia articles with the information quality evaluations provided by human users . Our experiment shows , that using special-purpose models for information quality captures user sentiment about Wikipedia articles better than using a single model for both categories of articles . \",\n",
       " 13820155: 'Consideration set generation in commerce search In commerce search , the set of products returned by a search engine often forms the basis for all user interactions leading up to a potential transaction on the web . Such a set of products is known as the consideration set . In this study , we consider the problem of generating consideration set of products in commerce search so as to maximize user satisfaction . One of the key features of commerce search that we exploit in our study is the association of a set of important attributes with the products and a set of specified attributes with the user queries . Those important attributes not used in the query are treated as unspecified . The attribute space admits a natural definition of user satisfaction via user preferences on the attributes and their values , viz . require that the surfaced products be close to the specified attribute values in the query , and diverse with respect to the unspecified attributes . We model this as a general Max-Sum Dispersion problem wherein we are given a set of n nodes in a metric space and the objective is to select a subset of nodes with total cost at most a given budget , and maximize the sum of the pairwise distances between the selected nodes . In our setting , each node denotes a product , the cost of a node being inversely proportional to its relevance with respect to specified attributes . The distance between two nodes quantifies the diversity with respect to the unspecified attributes . The problem is NP-hard and a 2-approximation was previously known only when all the nodes have unit cost . In our setting , we do not make any assumptions on the cost . We label this problem as the General Max-Sum Dispersion problem . We give the first constant factor approximation algorithm for this problem , achieving an approximation ratio of 2 . Further , we perform extensive empirical analysis on real-world data to show the effectiveness of our algorithm . ',\n",
       " 13833456: 'Modeling the temporal dynamics of social rating networks using bidirectional effects of social relations and rating patterns A social rating network ( SRN ) is a social network in which edges represent social relationships and users ( nodes ) express ratings on some of the given items . Such networks play an increasingly important role in reviewing websites such as Epinions.com or online sharing websites like Flickr.com . In this paper , we first observe and analyze the temporal behavior of users in a social rating network , who express ratings and create social relations . Then , we model the temporal dynamics of an SRN based on our observations , using the bidirectional effects of ratings and social relations . While existing models for other types of social networks have captured some of the effects , our model is the first one to represent all four effects , i.e. social relations-on-ratings ( social influence ) , social relations-on-social relations ( transitivity ) , ratings-on-social relations ( selection ) , and ratings-on-ratings ( correlational influence ) . Existing works consider these effects as static and constant throughout the evolution of an SRN , however our observations reveal that these effects are actually dynamic . We propose a probabilistic generative model for SRNs , which models the strength and dynamics of each effect throughout the network evolution . This model can serve for the prediction of future links , ratings or community structures . Due to the sensitive nature of SRNs , another motivation for our work is the generation of synthetic SRN data sets for research purposes . Our experimental studies on two real life datasets ( Epinions and Flickr ) demonstrate that the proposed model produces social rating networks that agree with real world data on a comprehensive set of evaluation criteria . ',\n",
       " 13875907: \"Economics of BitTorrent communities Over the years , private file-sharing communities built on the BitTorrent protocol have developed their own policies and mechanisms for motivating members to share content and contribute resources . By requiring members to maintain a minimum ratio between uploads and downloads , private communities effectively establish credit systems , and with them full-fledged economies . We report on a half-year-long measurement study of DIME -- a community for sharing live concert recordings -- that sheds light on the economic forces affecting users in such communities . A key observation is that while the download of files is priced only according to the size of the file , the rate of return for seeding new files is significantly greater than for seeding old files . We find via a natural experiment that users react to such differences in resale value by preferentially consuming older files during a ` free leech ' period . We consider implications of these finding on a user 's ability to earn credits and meet ratio enforcements , focusing in particular on the relationship between visitation frequency and wealth and on low bandwidth users . We then share details from an interview with DIME moderators , which highlights the goals of the community based on which we make suggestions for possible improvement . \",\n",
       " 13896241: \"Periodic transfers in mobile applications : network-wide origin , impact , and optimization Cellular networks employ a specific radio resource management policy distinguishing them from wired and Wi-Fi networks . A lack of awareness of this important mechanism potentially leads to resource-inefficient mobile applications . We perform the first network-wide , large-scale investigation of a particular type of application traffic pattern called periodic transfers where a handset periodically exchanges some data with a remote server every t seconds . Using packet traces containing 1.5 billion packets collected from a commercial cellular carrier , we found that periodic transfers are very prevalent in today 's smartphone traffic . However , they are extremely resource-inefficient for both the network and end-user devices even though they predominantly generate very little traffic . This somewhat counter-intuitive behavior is a direct consequence of the adverse interaction between such periodic transfer patterns and the cellular network radio resource management policy . For example , for popular smartphone applications such as Facebook , periodic transfers account for only 1.7 % of the overall traffic volume but contribute to 30 % of the total handset radio energy consumption . We found periodic transfers are generated for various reasons such as keep-alive , polling , and user behavior measurements . We further investigate the potential of various traffic shaping and resource control algorithms . Depending on their traffic patterns , applications exhibit disparate responses to optimization strategies . Jointly using several strategies with moderate aggressiveness can eliminate almost all energy impact of periodic transfers for popular applications such as Facebook and Pandora . \",\n",
       " 13904998: 'A unified approach to learning task-specific bit vector representations for fast nearest neighbor search Fast nearest neighbor search is necessary for a variety of large scale web applications such as information retrieval , nearest neighbor classification and nearest neighbor regression . Recently a number of machine learning algorithms have been proposed for representing the data to be searched as ( short ) bit vectors and then using hashing to do rapid search . These algorithms have been limited in their applicability in that they are suited for only one type of task -- e.g. Spectral Hashing learns bit vector representations for retrieval , but not say , classification . In this paper we present a unified approach to learning bit vector representations for many applications that use nearest neighbor search . The main contribution is a single learning algorithm that can be customized to learn a bit vector representation suited for the task at hand . This broadens the usefulness of bit vector representations to tasks beyond just conventional retrieval . We propose a learning-to-rank formulation to learn the bit vector representation of the data . LambdaRank algorithm is used for learning a function that computes a task-specific bit vector from an input data vector . Our approach outperforms state-of-the-art nearest neighbor methods on a number of real world text and image classification and retrieval datasets . It is scalable and learns a 32-bit representation on 1.46 million training cases in two days . ',\n",
       " 13982483: \"Estimating the impressionrank of web pages The ImpressionRank of a web page ( or , more generally , of a web site ) is the number of times users viewed the page while browsing search results . ImpressionRank captures the visibility of pages and sites in search engines and is thus an important measure , which is of interest to web site owners , competitors , market analysts , and end users . All previous approaches to estimating the ImpressionRank of a page rely on privileged access to private data sources , like the search engine 's query log . In this paper we present the first external algorithm for estimating the ImpressionRank of a web page . This algorithm relies on access to three public data sources : the search engine , the query suggestion service of the search engine , and the web . In addition , the algorithm is local and uses modest resources . It can therefore be used by almost any party to estimate the ImpressionRank of any page on any search engine . En route to estimating the ImpressionRank of a page , our algorithm solves a novel variant of the keyword extraction problem : it finds the most popular search keywords that drive impressions of a page . Empirical analysis of the algorithm on the Google and Yahoo ! search engines indicates that it is accurate and provides interesting insights about sites and search queries . \",\n",
       " 14005107: 'Classifying web sites In this paper , we present a novel method for the classification of Web sites . This method exploits both structure and content of Web sites in order to discern their functionality . It allows for distinguishing between eight of the most relevant functional classes of Web sites . We show that a pre-classification of Web sites utilizing structural properties considerably improves a subsequent textual classification with standard techniques . We evaluate this approach on a dataset comprising more than 16,000 Web sites with about 20 million crawled and 100 million known Web pages . Our approach achieves an accuracy of 92 % for the coarse-grained classification of these Web sites . ',\n",
       " 14005258: 'Human wayfinding in information networks Navigating information spaces is an essential part of our everyday lives , and in order to design efficient and user-friendly information systems , it is important to understand how humans navigate and find the information they are looking for . We perform a large-scale study of human wayfinding , in which , given a network of links between the concepts of Wikipedia , people play a game of finding a short path from a given start to a given target concept by following hyperlinks . What distinguishes our setup from other studies of human Web-browsing behavior is that in our case people navigate a graph of connections between concepts , and that the exact goal of the navigation is known ahead of time . We study more than 30,000 goal-directed human search paths and identify strategies people use when navigating information spaces . We find that human wayfinding , while mostly very efficient , differs from shortest paths in characteristic ways . Most subjects navigate through high-degree hubs in the early phase , while their search is guided by content features thereafter . We also observe a trade-off between simplicity and efficiency : conceptually simple solutions are more common but tend to be less efficient than more complex ones . Finally , we consider the task of predicting the target a user is trying to reach . We design a model and an efficient learning algorithm . Such predictive models of human wayfinding can be applied in intelligent browsing interfaces . ',\n",
       " 14007948: 'Generalized link suggestions via web site clustering Proactive link suggestion leads to improved user experience by allowing users to reach relevant information with fewer clicks , fewer pages to read , or simply faster because the right pages are prefetched just in time . In this paper we tackle two new scenarios for link suggestion , which were not covered in prior work owing to scarcity of historical browsing data . In the web search scenario , we propose a method for generating quick links - additional entry points into Web sites , which are shown for top search results for navigational queries - for tail sites , for which little browsing statistics is available . Beyond Web search , we also propose a method for link suggestion in general web browsing , effectively anticipating the next link to be followed by the user . Our approach performs clustering of Web sites in order to aggregate information across multiple sites , and enables relevant link suggestion for virtually any site , including tail sites and brand new sites for which little historical data is available . Empirical evaluation confirms the validity of our method using editorially labeled data as well as real-life search and browsing data from a major US search engine . ',\n",
       " 14026193: \"A data-driven sketch of Wikipedia editors Who edits Wikipedia ? We attempt to shed light on this question by using aggregated log data from Yahoo ! 's browser toolbar in order to analyze Wikipedians ' editing behavior in the context of their online lives beyond Wikipedia . We broadly characterize editors by investigating how their online behavior differs from that of other users ; e.g. , we find that Wikipedia editors search more , read more news , play more games , and , perhaps surprisingly , are more immersed in pop culture . Then we inspect how editors ' general interests relate to the articles to which they contribute ; e.g. , we confirm the intuition that editors show more expertise in their active domains than average users . Our results are relevant as they illuminate novel aspects of what has become many Web users ' prevalent source of information and can help in recruiting new editors . \",\n",
       " 14027370: \"Framework and algorithms for network bucket testing Bucket testing , also known as split testing , A\\\\/B testing , or 0\\\\/1 testing , is a widely used method for evaluating users ' satisfaction with new features , products , or services . In order not to expose the whole user base to the new service , the mean user satisfaction rate is estimated by exposing the service only to a few uniformly chosen random users . In a recent work , Backstrom and Kleinberg , defined the notion of network bucket testing for social services . In this context , users ' interactions are only valid for measurement if some minimal number of their friends are also given the service . The goal is to estimate the mean user satisfaction rate while providing the service to the least number of users . This constraint makes uniform sampling , which is optimal for the traditional case , grossly inefficient . In this paper we introduce a simple general framework for designing and evaluating sampling techniques for network bucket testing . The framework is constructed in a way that sampling algorithms are only required to generate sets of users to which the service should be provided . Given an algorithm , the framework produces an unbiased user satisfaction rate estimator and a corresponding variance bound for any network and any user satisfaction function . Furthermore , we present several simple sampling algorithms that are evaluated using both synthetic and real social networks . Our experiments corroborate the theoretical results and demonstrate the effectiveness of the proposed framework and algorithms . \",\n",
       " 14045611: 'Online team formation in social networks We study the problem of online team formation . We consider a setting in which people possess different skills and compatibility among potential team members is modeled by a social network . A sequence of tasks arrives in an online fashion , and each task requires a specific set of skills . The goal is to form a new team upon arrival of each task , so that ( i ) each team possesses all skills required by the task , ( ii ) each team has small communication overhead , and ( iii ) the workload of performing the tasks is balanced among people in the fairest possible way . We propose efficient algorithms that address all these requirements : our algorithms form teams that always satisfy the required skills , provide approximation guarantees with respect to team communication overhead , and they are online-competitive with respect to load balancing . Experiments performed on collaboration networks among film actors and scientists , confirm that our algorithms are successful at balancing these conflicting requirements . This is the first paper that simultaneously addresses all these aspects . Previous work has either focused on minimizing coordination for a single task or balancing the workload neglecting coordination costs . ',\n",
       " 14122983: 'Trains of thought : generating information maps When information is abundant , it becomes increasingly difficult to fit nuggets of knowledge into a single coherent picture . Complex stories spaghetti into branches , side stories , and intertwining narratives . In order to explore these stories , one needs a map to navigate unfamiliar territory . We propose a methodology for creating structured summaries of information , which we call metro maps . Our proposed algorithm generates a concise structured set of documents maximizing coverage of salient pieces of information . Most importantly , metro maps explicitly show the relations among retrieved pieces in a way that captures story development . We first formalize characteristics of good maps and formulate their construction as an optimization problem . Then we provide efficient methods with theoretical guarantees for generating maps . Finally , we integrate user interaction into our framework , allowing users to alter the maps to better reflect their interests . Pilot user studies with a real-world dataset demonstrate that the method is able to produce maps which help users acquire knowledge efficiently . ',\n",
       " 14123171: 'Active objects : actions for entity-centric search We introduce an entity-centric search experience , called Active Objects , in which entity-bearing queries are paired with actions that can be performed on the entities . For example , given a query for a specific flashlight , we aim to present actions such as reading reviews , watching demo videos , and finding the best price online . In an annotation study conducted over a random sample of user query sessions , we found that a large proportion of queries in query logs involve actions on entities , calling for an automatic approach to identifying relevant actions for entity-bearing queries . In this paper , we pose the problem of finding actions that can be performed on entities as the problem of probabilistic inference in a graphical model that captures how an entity bearing query is generated . We design models of increasing complexity that capture latent factors such as entity type and intended actions that determine how a user writes a query in a search box , and the URL that they click on . Given a large collection of real-world queries and clicks from a commercial search engine , the models are learned efficiently through maximum likelihood estimation using an EM algorithm . Given a new query , probabilistic inference enables recommendation of a set of pertinent actions and hosts . We propose an evaluation methodology for measuring the relevance of our recommended actions , and show empirical evidence of the quality and the diversity of the discovered actions . ',\n",
       " 14128558: \"Using proximity to predict activity in social networks The structure of a social network contains information useful for predicting its evolution . We show that structural information also helps predict activity . People who are `` close '' in some sense in a social network are more likely to perform similar actions than more distant people . We use network proximity to capture the degree to which people are `` close '' to each other . In addition to standard proximity metrics used in the link prediction task , such as neighborhood overlap , we introduce new metrics that model different types of interactions that take place between people . We study this claim empirically using data about URL forwarding activity on the social media sites Digg and Twitter . We show that structural proximity of two users in the follower graph is related to similarity of their activity , i.e. , how many URLs they both forward . We also show that given friends ' activity , knowing their proximity to the user can help better predict which URLs the user will forward . We compare the performance of different proximity metrics on the activity prediction task and find that metrics that take into account the attention-limited nature of interactions in social media lead to substantially better predictions . \",\n",
       " 14146914: 'Optimizing budget allocation among channels and influencers Brands and agencies use marketing as a tool to influence customers . One of the major decisions in a marketing plan deals with the allocation of a given budget among media channels in order to maximize the impact on a set of potential customers . A similar situation occurs in a social network , where a marketing budget needs to be distributed among a set of potential influencers in a way that provides high-impact . We introduce several probabilistic models to capture the above scenarios . The common setting of these models consists of a bipartite graph of source and target nodes . The objective is to allocate a fixed budget among the source nodes to maximize the expected number of influenced target nodes . The concrete way in which source nodes influence target nodes depends on the underlying model . We primarily consider two models : a source-side influence model , in which a source node that is allocated a budget of k makes k independent trials to influence each of its neighboring target nodes , and a target-side influence model , in which a target node becomes influenced according to a specified rule that depends on the overall budget allocated to its neighbors . Our main results are an optimal ( 1-1 \\\\/ e ) - approximation algorithm for the source-side model , and several inapproximability results for the target-side model , establishing that influence maximization in the latter model is provably harder . ',\n",
       " 14149355: 'On revenue in the generalized second price auction The Generalized Second Price ( GSP ) auction is the primary auction used for selling sponsored search advertisements . In this paper we consider the revenue of this auction at equilibrium . We prove that if agent values are drawn from identical regular distributions , then the GSP auction paired with an appropriate reserve price generates a constant fraction ( 1\\\\/6th ) of the optimal revenue . In the full-information game , we show that at any Nash equilibrium of the GSP auction obtains at least half of the revenue of the VCG mechanism excluding the payment of a single participant . This bound holds also with any reserve price , and is tight . Finally , we consider the tradeoff between maximizing revenue and social welfare . We introduce a natural convexity assumption on the click-through rates and show that it implies that the revenue-maximizing equilibrium of GSP in the full information model will necessarily be envy-free . In particular , it is always possible to maximize revenue and social welfare simultaneously when click-through rates are convex . Without this convexity assumption , however , we demonstrate that revenue may be maximized at a non-envy-free equilibrium that generates a socially inefficient allocation . ',\n",
       " 14169498: 'Document recommendation in social tagging services Social tagging services allow users to annotate various online resources with freely chosen keywords ( tags ) . They not only facilitate the users in finding and organizing online resources , but also provide meaningful collaborative semantic data which can potentially be exploited by recommender systems . Traditional studies on recommender systems focused on user rating data , while recently social tagging data is becoming more and more prevalent . How to perform resource recommendation based on tagging data is an emerging research topic . In this paper we consider the problem of document ( e.g. Web pages , research papers ) recommendation using purely tagging data . That is , we only have data containing users , tags , documents and the relationships among them . We propose a novel graph-based representation learning algorithm for this purpose . The users , tags and documents are represented in the same semantic space in which two related objects are close to each other . For a given user , we recommend those documents that are sufficiently close to him\\\\/her . Experimental results on two data sets crawled from Del. icio . us and CiteULike show that our algorithm can generate promising recommendations and outperforms traditional recommendation algorithms . ',\n",
       " 14173486: 'From actors , politicians , to CEOs : domain adaptation of relational extractors using a latent relational mapping We propose a method to adapt an existing relation extraction system to extract new relation types with minimum supervision . Our proposed method comprises two stages : learning a lower-dimensional projection between different relations , and learning a relational classifier for the target relation type with instance sampling . We evaluate the proposed method using a dataset that contains 2000 instances for 20 different relation types . Our experimental results show that the proposed method achieves a statistically significant macro-average F-score of 62.77 . Moreover , the proposed method outperforms numerous baselines and a previously proposed weakly-supervised relation extraction method . ',\n",
       " 14185350: \"Modeling and predicting behavioral dynamics on the web User behavior on the Web changes over time . For example , the queries that people issue to search engines , and the underlying informational goals behind the queries vary over time . In this paper , we examine how to model and predict this temporal user behavior . We develop a temporal modeling framework adapted from physics and signal processing that can be used to predict time-varying user behavior using smoothing and trends . We also explore other dynamics of Web behaviors , such as the detection of periodicities and surprises . We develop a learning procedure that can be used to construct models of users ' activities based on features of current and historical behaviors . The results of experiments indicate that by using our framework to predict user behavior , we can achieve significant improvements in prediction compared to baseline models that weight historical evidence the same for all queries . We also develop a novel learning algorithm that explicitly learns when to apply a given prediction model among a set of such models . Our improved temporal modeling of user behavior can be used to enhance query suggestions , crawling policies , and result ranking . \",\n",
       " 14186402: 'k-Centralities : local approximations of global measures based on shortest paths A lot of centrality measures have been developed to analyze different aspects of importance . Some of the most popular centrality measures ( e.g. betweenness centrality , closeness centrality ) are based on the calculation of shortest paths . This characteristic limits the applicability of these measures for larger networks . In this article we elaborate on the idea of bounded-distance shortest paths calculations . We claim criteria for k-centrality measures and we introduce one algorithm for calculating both betweenness and closeness based centralities . We also present normalizations for these measures . We show that k-centrality measures are good approximations for the corresponding centrality measures by achieving a tremendous gain of calculation time and also having linear calculation complexity O ( n ) for networks with constant average degree . This allows researchers to approximate centrality measures based on shortest paths for networks with millions of nodes or with high frequency in dynamically changing networks . ',\n",
       " 14200731: 'An expressive mechanism for auctions on the web Auctions are widely used on the Web . Applications range from internet advertising to platforms such as eBay . In most of these applications the auctions in use are single\\\\/multi-item auctions with unit demand . The main drawback of standard mechanisms for this type of auctions , such as VCG and GSP , is the limited expressiveness that they offer to the bidders . The General Auction Mechanism ( GAM ) of ( 1 ) is taking a first step towards addressing the problem of limited expressiveness by computing a bidder optimal , envy free outcome for linear utility functions with identical slopes and a single discontinuity per bidder-item pair . We show that in many practical situations this does not suffice to adequately model the preferences of the bidders , and we overcome this problem by presenting the first mechanism for piece-wise linear utility functions with non-identical slopes and multiple discontinuities . Our mechanism runs in polynomial time . Like GAM it is incentive compatible for inputs that fulfill a certain non-degeneracy requirement , but our requirement is more general than the requirement of GAM . For discontinuous utility functions that are non-degenerate as well as for continuous utility functions the outcome of our mechanism is a competitive equilibrium . We also show how our mechanism can be used to compute approximately bidder optimal , envy free outcomes for a general class of continuous utility functions via piece-wise linear approximation . Finally , we prove hardness results for even more expressive settings . ',\n",
       " 14209920: \"The million song dataset challenge We introduce the Million Song Dataset Challenge : a large-scale , personalized music recommendation challenge , where the goal is to predict the songs that a user will listen to , given both the user 's listening history and full information ( including meta-data and content analysis ) for all songs . We explain the taste profile data , our goals and design choices in creating the challenge , and present baseline results using simple , off-the-shelf recommendation algorithms . \",\n",
       " 14237951: 'The social honeypot project : protecting online communities from spammers We present the conceptual framework of the Social Honeypot Project for uncovering social spammers who target online communities and initial empirical results from Twitter and MySpace . Two of the key components of the Social Honeypot Project are : ( 1 ) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities ; and ( 2 ) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers . ',\n",
       " 14249: 'Design of a crawler with bounded bandwidth This paper presents an algorithm to bound the bandwidth of a Web crawler . The crawler collects statistics on the transfer rate of each server to predict the expected bandwidth use for future downloads . The prediction allows us to activate the optimal number of fetcher threads in order to exploit the assigned bandwidth . The experimental results show the effectiveness of the proposed technique . ',\n",
       " 14255423: \"Serf and turf : crowdturfing for fun and profit Popular Internet services in recent years have shown that remarkable things can be achieved by harnessing the power of the masses using crowd-sourcing systems . However , crowd-sourcing systems can also pose a real challenge to existing security mechanisms deployed to protect Internet services . Many of these security techniques rely on the assumption that malicious activity is generated automatically by automated programs . Thus they would perform poorly or be easily bypassed when attacks are generated by real users working in a crowd-sourcing system . Through measurements , we have found surprising evidence showing that not only do malicious crowd-sourcing systems exist , but they are rapidly growing in both user base and total revenue . We describe in this paper a significant effort to study and understand these `` crowdturfing '' systems in today 's Internet . We use detailed crawls to extract data about the size and operational structure of these crowdturfing systems . We analyze details of campaigns offered and performed in these sites , and evaluate their end-to-end effectiveness by running active , benign campaigns of our own . Finally , we study and compare the source of workers on crowdturfing sites in different countries . Our results suggest that campaigns on these systems are highly effective at reaching users , and their continuing growth poses a concrete threat to online communities both in the US and elsewhere . \",\n",
       " 14265822: 'b-Bit minwise hashing This paper establishes the theoretical framework of b-bit minwise hashing . The original minwise hashing method has become a standard technique for estimating set similarity ( e.g. , resemblance ) with applications in information retrieval , data management , computational advertising , etc. . By only storing b bits of each hashed value ( e.g. , b = 1 or 2 ) , we gain substantial advantages in terms of storage space . We prove the basic theoretical results and provide an unbiased estimator of the resemblance for any b. We demonstrate that , even in the least favorable scenario , using b = 1 may reduce the storage space at least by a factor of 21.3 ( or 10.7 ) compared to b = 64 ( or b = 32 ) , if one is interested in resemblance ) 0.5 . ',\n",
       " 14316716: 'Crowdsourcing with endogenous entry We investigate the design of mechanisms to incentivize high quality outcomes in crowdsourcing environments with strategic agents , when entry is an endogenous , strategic choice . Modeling endogenous entry in crowdsourcing markets is important because there is a nonzero cost to making a contribution of any quality which can be avoided by not participating , and indeed many sites based on crowdsourced content do not have adequate participation . We use a mechanism with monotone , rank-based , rewards in a model where agents strategically make participation and quality choices to capture a wide variety of crowdsourcing environments , ranging from conventional crowdsourcing contests with monetary rewards such as TopCoder , to crowdsourced content as in online Q&A forums . We begin by explicitly constructing the unique mixed-strategy equilibrium for such monotone rank-order mechanisms , and use the participation probability and distribution of qualities from this construction to address the question of designing incentives for two kinds of rewards that arise in the context of crowdsourcing . We first show that for attention rewards that arise in the crowdsourced content setting , the entire equilibrium distribution and therefore every increasing statistic including the maximum and average quality ( accounting for participation ) , improves when the rewards for every rank but the last are as high as possible . In particular , when the cost of producing the lowest possible quality content is low , the optimal mechanism displays all but the poorest contribution . We next investigate how to allocate rewards in settings where there is a fixed total reward that can be arbitrarily distributed amongst participants , as in crowdsourcing contests . Unlike models with exogenous entry , here the expected number of participants can be increased by subsidizing entry , which could potentially improve the expected value of the best contribution . However , we show that subsidizing entry does not improve the expected quality of the best contribution , although it may improve the expected quality of the average contribution . In fact , we show that free entry is dominated by taxing entry -- making all entrants pay a small fee , which is rebated to the winner along with whatever rewards were already assigned , can improve the quality of the best contribution over a winner-take-all contest with no taxes . ',\n",
       " 14316718: \"Implementing optimal outcomes in social computing : a game-theoretic approach In many social computing applications such as online Q&A forums , the best contribution for each task receives some high reward , while all remaining contributions receive an identical , lower reward irrespective of their actual qualities . Suppose a mechanism designer ( site owner ) wishes to optimize an objective that is some function of the number and qualities of received contributions . When potential contributors are -LCB- \\\\ em strategic -RCB- agents , who decide whether to contribute or not to selfishly maximize their own utilities , is such a `` best contribution '' mechanism , Mb , adequate to implement an outcome that is optimal for the mechanism designer ? We first show that in settings where a contribution 's value is determined primarily by an agent 's expertise , and agents only strategically choose whether to contribute or not , contests can implement optimal outcomes : for any reasonable objective , the rewards for the best and remaining contributions in Mb can always be chosen so that the outcome in the unique symmetric equilibrium of Mb maximizes the mechanism designer 's utility . We also show how the mechanism designer can learn these optimal rewards when she does not know the parameters of the agents ' utilities , as might be the case in practice . We next consider settings where a contribution 's value depends on both the contributor 's expertise as well as her effort , and agents endogenously choose how much effort to exert in addition to deciding whether to contribute . Here , we show that optimal outcomes can never be implemented by contests if the system can rank the qualities of contributions perfectly . However , if there is noise in the contributions ' rankings , then the mechanism designer can again induce agents to follow strategies that maximize his utility . Thus imperfect rankings can actually help achieve implementability of optimal outcomes when effort is endogenous and influences quality . \",\n",
       " 14322308: 'New objective functions for social collaborative filtering This paper examines the problem of social collaborative filtering ( CF ) to recommend items of interest to users in a social network setting . Unlike standard CF algorithms using relatively simple user and item features , recommendation in social networks poses the more complex problem of learning user preferences from a rich and complex set of user profile and interaction information . Many existing social CF methods have extended traditional CF matrix factorization , but have overlooked important aspects germane to the social setting . We propose a unified framework for social CF matrix factorization by introducing novel objective functions for training . Our new objective functions have three key features that address main drawbacks of existing approaches : ( a ) we fully exploit feature-based user similarity , ( b ) we permit direct learning of user-to-user information diffusion , and ( c ) we leverage co-preference ( dis ) agreement between two users to learn restricted areas of common interest . We evaluate these new social CF objectives , comparing them to each other and to a variety of ( social ) CF baselines , and analyze user behavior on live user trials in a custom-developed Facebook App involving data collected over five months from over 100 App users and their 37,000 + friends . ',\n",
       " 14347434: 'Intelligent crawling of web applications for web archiving The steady growth of the World Wide Web raises challenges regarding the preservation of meaningful Web data . Tools used currently by Web archivists blindly crawl and store Web pages found while crawling , disregarding the kind of Web site currently accessed ( which leads to suboptimal crawling strategies ) and whatever structured content is contained in Web pages ( which results in page-level archives whose content is hard to exploit ) . We focus in this PhD work on the crawling and archiving of publicly accessible Web applications , especially those of the social Web . A Web application is any application that uses Web standards such as HTML and HTTP to publish information on the Web , accessible by Web browsers . Examples include Web forums , social networks , geolocation services , etc. . We claim that the best strategy to crawl these applications is to make the Web crawler aware of the kind of application currently processed , allowing it to refine the list of URLs to process , and to annotate the archive with information about the structure of crawled content . We add adaptive characteristics to an archival Web crawler : being able to identify when a Web page belongs to a given Web application and applying the appropriate crawling and content extraction methodology . ',\n",
       " 14348154: \"Leveraging interlingual classification to improve web search In this paper we address the problem of improving accuracy of web search in a smaller , data-limited search market ( search language ) using behavioral data from a larger , data-rich market ( assist language ) . Specifically , we use interlingual classification to infer the search language query 's intent using the assist language click-through data . We use these improved estimates of query intent , along with the query intent based on the search language data , to compute features that encode the similarity between a search result ( URL ) and the query . These features are subsequently fed into the ranking model to improve the relevance ranking of the documents . Our experimental results on German and French languages show the effectiveness of using assist language behavioral data especially , when the search language queries have small click-through data . \",\n",
       " 14353464: 'Modeling click-through based word-pairs for web search Statistical translation models and latent semantic analysis ( LSA ) are two effective approaches to exploit click-through data for web search ranking . This paper presents two document ranking models that combine both approaches by explicitly modeling word-pairs . The first model , called PairModel , is a monolingual ranking model based on word pairs that are derived from click-through data . It maps queries and documents into a concept space spanned by these word pairs . The second model , called Bilingual Paired Topic Model ( BPTM ) , uses bilingual word pairs and jointly models a bilingual query-document collection . This model maps queries and documents in multiple languages into a lower dimensional semantic subspace . Experimental results on web search task show that they significantly outperform the state-of-the-art baseline models , and the best result is obtained by interpolating PairModel and BPTM . ',\n",
       " 14371347: \"Information integration over time in unreliable and uncertain environments Often an interesting true value such as a stock price , sports score , or current temperature is only available via the observations of noisy and potentially conflicting sources . Several techniques have been proposed to reconcile these conflicts by computing a weighted consensus based on source reliabilities , but these techniques focus on static values . When the real-world entity evolves over time , the noisy sources can delay , or even miss , reporting some of the real-world updates . This temporal aspect introduces two key challenges for consensus-based approaches : ( i ) due to delays , the mapping between a source 's noisy observation and the real-world update it observes is unknown , and ( ii ) missed updates may translate to missing values for the consensus problem , even if the mapping is known . To overcome these challenges , we propose a formal approach that models the history of updates of the real-world entity as a hidden semi-Markovian process ( HSMM ) . The noisy sources are modeled as observations of the hidden state , but the mapping between a hidden state ( i.e. real-world update ) and the observation ( i.e. source value ) is unknown . We propose algorithms based on Gibbs Sampling and EM to jointly infer both the history of real-world updates as well as the unknown mapping between them and the source values . We demonstrate using experiments on real-world datasets how our history-based techniques improve upon history-agnostic consensus-based approaches . \",\n",
       " 14372945: \"Handling forecast errors while bidding for display advertising Most of the online advertising today is sold via an auction , which requires the advertiser to respond with a valid bid within a fraction of a second . As such , most advertisers employ bidding agents to submit bids on their behalf . The architecture of such agents typically has ( 1 ) an offline optimization phase which incorporates the bidder 's knowledge about the market and ( 2 ) an online bidding strategy which simply executes the offline strategy . The online strategy is typically highly dependent on both supply and expected price distributions , both of which are forecast using traditional machine learning methods . In this work we investigate the optimum strategy of the bidding agent when faced with incorrect forecasts . At a high level , the agent can invest resources in improving the forecasts , or can tighten the loop between successive offline optimization cycles in order to detect errors more quickly . We show analytically that the latter strategy , while simple , is extremely effective in dealing with forecast errors , and confirm this finding with experimental evaluations . \",\n",
       " 14373644: 'Max algorithms in crowdsourcing environments Our work investigates the problem of retrieving the maximum item from a set in crowdsourcing environments . We first develop parameterized families of max algorithms , that take as input a set of items and output an item from the set that is believed to be the maximum . Such max algorithms could , for instance , select the best Facebook profile that matches a given person or the best photo that describes a given restaurant . Then , we propose strategies that select appropriate max algorithm parameters . Our framework supports various human error and cost models and we consider many of them for our experiments . We evaluate under many metrics , both analytically and via simulations , the tradeoff between three quantities : ( 1 ) quality , ( 2 ) monetary cost , and ( 3 ) execution time . Also , we provide insights on the effectiveness of the strategies in selecting appropriate max algorithm parameters and guidelines for choosing max algorithms and strategies for each application . ',\n",
       " 14373837: \"The role of social networks in information diffusion Online social networking technologies enable individuals to simultaneously share information with any number of peers . Quantifying the causal effect of these mediums on the dissemination of information requires not only identification of who influences whom , but also of whether individuals would still propagate information in the absence of social signals about that information . We examine the role of social networks in online information diffusion with a large-scale field experiment that randomizes exposure to signals about friends ' information sharing among 253 million subjects in situ . Those who are exposed are significantly more likely to spread information , and do so sooner than those who are not exposed . We further examine the relative role of strong and weak ties in information propagation . We show that , although stronger ties are individually more influential , it is the more abundant weak ties who are responsible for the propagation of novel information . This suggests that weak ties may play a more dominant role in the dissemination of information online than currently believed . \",\n",
       " 14374135: \"Targeted disambiguation of ad-hoc , homogeneous sets of named entities In many entity extraction applications , the entities to be recognized are constrained to be from a list of `` target entities '' . In many cases , these target entities are ( i ) ad-hoc , i.e. , do not exist in a knowledge base and ( ii ) homogeneous ( e.g. , all the entities are IT companies ) . We study the following novel disambiguation problem in this unique setting : given the candidate mentions of all the target entities , determine which ones are true mentions of a target entity . Prior techniques only consider target entities present in a knowledge base and\\\\/or having a rich set of attributes . In this paper , we develop novel techniques that require no knowledge about the entities except their names . Our main insight is to leverage the homogeneity constraint and disambiguate the candidate mentions collectively across all documents . We propose a graph-based model , called MentionRank , for that purpose . Furthermore , if additional knowledge is available for some or all of the entities , our model can leverage it to further improve quality . Our experiments demonstrate the effectiveness of our model . To the best of our knowledge , this is the first work on targeted entity disambiguation for ad-hoc entities . \",\n",
       " 14375309: 'Vertex collocation profiles : subgraph counting for link analysis and prediction We introduce the concept of a vertex collocation profile ( VCP ) for the purpose of topological link analysis and prediction . VCPs provide nearly complete information about the surrounding local structure of embedded vertex pairs . The VCP approach offers a new tool for domain experts to understand the underlying growth mechanisms in their networks and to analyze link formation mechanisms in the appropriate sociological , biological , physical , or other context . The same resolution that gives VCP its analytical power also enables it to perform well when used in supervised models to discriminate potential new links . We first develop the theory , mathematics , and algorithms underlying VCPs . Then we demonstrate VCP methods performing link prediction competitively with unsupervised and supervised methods across several different network families . We conclude with timing results that introduce the comparative performance of several existing algorithms and the practicability of VCP computations on large networks . ',\n",
       " 14375496: \"Partitioned multi-indexing : bringing order to social search To answer search queries on a social network rich with user-generated content , it is desirable to give a higher ranking to content that is closer to the individual issuing the query . Queries occur at nodes in the network , documents are also created by nodes in the same network , and the goal is to find the document that matches the query and is closest in network distance to the node issuing the query . In this paper , we present the `` Partitioned Multi-Indexing '' scheme , which provides an approximate solution to this problem . With m links in the network , after an offline ~ O ( m ) pre-processing time , our scheme allows for social index operations ( i.e. , social search queries , as well as insertion and deletion of words into and from a document at any node ) , all in time ~ O ( 1 ) . Further , our scheme can be implemented on open source distributed streaming systems such as Yahoo ! S4 or Twitter 's Storm so that every social index operation takes ~ O ( 1 ) processing time and network queries in the worst case , and just two network queries in the common case where the reverse index corresponding to the query keyword is much smaller than the memory available at any distributed compute node . Building on Das Sarma et al. 's approximate distance oracle , the worst-case approximation ratio of our scheme is ~ O ( 1 ) for undirected networks . Our simulations on the social network Twitter as well as synthetic networks show that in practice , the approximation ratio is actually close to 1 for both directed and undirected networks . We believe that this work is the first demonstration of the feasibility of social search with real-time text updates at large scales . \",\n",
       " 14375712: 'Learning causality for news events prediction The problem we tackle in this work is , given a present news event , to generate a plausible future event that can be caused by the given event . We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques . Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor . To obtain precise labeled causality examples , we mine 150 years of news articles , and apply semantic natural language modeling techniques to titles containing certain predefined causality patterns . For generalization , the model uses a vast amount of world knowledge ontologies mined from LinkedData , containing ~ 200 datasets with approximately 20 billion relations . Empirical evaluation on real news articles shows that our Pundit algorithm reaches a human-level performance . ',\n",
       " 14375718: 'On the analysis of cascading style sheets Developing and maintaining cascading style sheets ( CSS ) is an important issue to web developers as they suffer from the lack of rigorous methods . Most existing means rely on validators that check syntactic rules , and on runtime debuggers that check the behavior of a CSS style sheet on a particular document instance . However , the aim of most style sheets is to be applied to an entire set of documents , usually defined by some schema . To this end , a CSS style sheet is usually written w.r.t. a given schema . While usual debugging tools help reducing the number of bugs , they do not ultimately allow to prove properties over the whole set of documents to which the style sheet is intended to be applied . We propose a novel approach to fill this lack . We introduce ideas borrowed from the fields of logic and compile-time verification for the analysis of CSS style sheets . We present an original tool based on recent advances in tree logics . The tool is capable of statically detecting a wide range of errors ( such as empty CSS selectors and semantically equivalent selectors ) , as well as proving properties related to sets of documents ( such as coverage of styling information ) , in the presence or absence of schema information . This new tool can be used in addition to existing runtime debuggers to ensure a higher level of quality of CSS style sheets . ',\n",
       " 14376091: \"Discovering geographical topics in the twitter stream Micro-blogging services have become indispensable communication tools for online users for disseminating breaking news , eyewitness accounts , individual expression , and protest groups . Recently , Twitter , along with other online social networking services such as Foursquare , Gowalla , Facebook and Yelp , have started supporting location services in their messages , either explicitly , by letting users choose their places , or implicitly , by enabling geo-tagging , which is to associate messages with latitudes and longitudes . This functionality allows researchers to address an exciting set of questions : 1 ) How is information created and shared across geographical locations , 2 ) How do spatial and linguistic characteristics of people vary across regions , and 3 ) How to model human mobility . Although many attempts have been made for tackling these problems , previous methods are either complicated to be implemented or oversimplified that can not yield reasonable performance . It is a challenge task to discover topics and identify users ' interests from these geo-tagged messages due to the sheer amount of data and diversity of language variations used on these location sharing services . In this paper we focus on Twitter and present an algorithm by modeling diversity in tweets based on topical diversity , geographical diversity , and an interest distribution of the user . Furthermore , we take the Markovian nature of a user 's location into account . Our model exploits sparse factorial coding of the attributes , thus allowing us to deal with a large and diverse set of covariates efficiently . Our approach is vital for applications such as user profiling , content recommendation and topic tracking . We show high accuracy in location estimation based on our model . Moreover , the algorithm identifies interesting topics based on location and language . \",\n",
       " 14376543: 'Information transfer in social media Recent research has explored the increasingly important role of social media by examining the dynamics of individual and group behavior , characterizing patterns of information diffusion , and identifying influential individuals . In this paper we suggest a measure of causal relationships between nodes based on the information -- theoretic notion of transfer entropy , or information transfer . This theoretically grounded measure is based on dynamic information , captures fine -- grain notions of influence , and admits a natural , predictive interpretation . Networks inferred by transfer entropy can differ significantly from static friendship networks because most friendship links are not useful for predicting future dynamics . We demonstrate through analysis of synthetic and real-world data that transfer entropy reveals meaningful hidden network structures . In addition to altering our notion of who is influential , transfer entropy allows us to differentiate between weak influence over large groups and strong influence over small groups . ',\n",
       " 14377120: \"Echoes of power : language effects and power differences in social interaction Understanding social interaction within groups is key to analyzing online communities . Most current work focuses on structural properties : who talks to whom , and how such interactions form larger network structures . The interactions themselves , however , generally take place in the form of natural language -- either spoken or written -- and one could reasonably suppose that signals manifested in language might also provide information about roles , status , and other aspects of the group 's dynamics . To date , however , finding domain-independent language-based signals has been a challenge . Here , we show that in group discussions , power differentials between participants are subtly revealed by how much one individual immediately echoes the linguistic style of the person they are responding to . Starting from this observation , we propose an analysis framework based on linguistic coordination that can be used to shed light on power relationships and that works consistently across multiple types of power -- including a more `` static '' form of power based on status differences , and a more `` situational '' form of power in which one individual experiences a type of dependence on another . Using this framework , we study how conversational behavior can reveal power relationships in two very different settings : discussions among Wikipedians and arguments before the U. S. Supreme Court . \",\n",
       " 14377272: 'Mining photo-sharing websites to study ecological phenomena The popularity of social media websites like Flickr and Twitter has created enormous collections of user-generated content online . Latent in these content collections are observations of the world : each photo is a visual snapshot of what the world looked like at a particular point in time and space , for example , while each tweet is a textual expression of the state of a person and his or her environment . Aggregating these observations across millions of social sharing users could lead to new techniques for large-scale monitoring of the state of the world and how it is changing over time . In this paper we step towards that goal , showing that by analyzing the tags and image features of geo-tagged , time-stamped photos we can measure and quantify the occurrence of ecological phenomena including ground snow cover , snow fall and vegetation density . We compare several techniques for dealing with the large degree of noise in the dataset , and show how machine learning can be used to reduce errors caused by misleading tags and ambiguous visual content . We evaluate the accuracy of these techniques by comparing to ground truth data collected both by surface stations and by Earth-observing satellites . Besides the immediate application to ecology , our study gives insight into how to accurately crowd-source other types of information from large , noisy social sharing datasets . ',\n",
       " 14377562: 'Towards robust service compositions in the context of functionally diverse services Web service composition provides a means of customized and flexible integration of service functionalities . Quality-of-Service ( QoS ) optimization algorithms select services in order to adapt workflows to the non-functional requirements of the user . With increasing number of services in a workflow , previous approaches fail to achieve a sufficient reliability . Moreover , expensive ad-hoc replanning is required to deal with service failures . The major problem with such sequential application of planning and replanning is that it ignores the potential costs during the initial planning and they consequently are hidden from the decision maker . Our basic idea to overcome this substantial problem is to compute a QoS optimized selection of service clusters that includes a sufficient number of backup services for each service employed . To support the human decision maker in the service selection task , our approach considers the possible repair costs directly in the initial composition . On the basis of a multi-objective approach and using a suitable service selection interface , the decision maker can select compositions in line with his\\\\/her personal risk preferences . ',\n",
       " 14380234: \"Learning from the past : answering new questions with past answers Community-based Question Answering sites , such as Yahoo ! Answers or Baidu Zhidao , allow users to get answers to complex , detailed and personal questions from other users . However , since answering a question depends on the ability and willingness of users to address the asker 's needs , a significant fraction of the questions remain unanswered . We measured that in Yahoo ! Answers , this fraction represents 15 % of all incoming English questions . At the same time , we discovered that around 25 % of questions in certain categories are recurrent , at least at the question-title level , over a period of one year . We attempt to reduce the rate of unanswered questions in Yahoo ! Answers by reusing the large repository of past resolved questions , openly available on the site . More specifically , we estimate the probability whether certain new questions can be satisfactorily answered by a best answer from the past , using a statistical model specifically trained for this task . We leverage concepts and methods from query-performance prediction and natural language processing in order to extract a wide range of features for our model . The key challenge here is to achieve a level of quality similar to the one provided by the best human answerers . We evaluated our algorithm on offline data extracted from Yahoo ! Answers , but more interestingly , also on online data by using three `` live '' answering robots that automatically provide past answers to new questions when a certain degree of confidence is reached . We report the success rate of these robots in three active Yahoo ! Answers categories in terms of both accuracy , coverage and askers ' satisfaction . This work presents a first attempt , to the best of our knowledge , of automatic question answering to questions of social nature , by reusing past answers of high quality . \",\n",
       " 14380564: \"Structured query suggestion for specialization and parallel movement : effect on search behaviors Query suggestion , which enables the user to revise a query with a single click , has become one of the most fundamental features of Web search engines . However , it is often difficult for the user to choose from a list of query suggestions , and to understand the relation between an input query and suggested ones . In this paper , we propose a new method to present query suggestions to the user , which has been designed to help two popular query reformulation actions , namely , specialization ( e.g. from `` nikon '' to `` nikon camera '' ) and parallel movement ( e.g. from `` nikon camera '' to `` canon camera '' ) . Using a query log collected from a popular commercial Web search engine , our prototype called SParQS classifies query suggestions into automatically generated categories and generates a label for each category . Moreover , SParQS presents some new entities as alternatives to the original query ( e.g. `` canon '' in response to the query `` nikon '' ) , together with their query suggestions classified in the same way as the original query 's suggestions . We conducted a task-based user study to compare SParQS with a traditional `` flat list '' query suggestion interface . Our results show that the SParQS interface enables subjects to search more successfully than the flat list case , even though query suggestions presented were exactly the same in the two interfaces . In addition , the subjects found the query suggestions more helpful when they were presented in the SParQS interface rather than in a flat list . \",\n",
       " 14382235: \"Are web users really Markovian ? User modeling on the Web has rested on the fundamental assumption of Markovian behavior -- a user 's next action depends only on her current state , and not the history leading up to the current state . This forms the underpinning of PageRank web ranking , as well as a number of techniques for targeting advertising to users . In this work we examine the validity of this assumption , using data from a number of Web settings . Our main result invokes statistical order estimation tests for Markov chains to establish that Web users are not , in fact , Markovian . We study the extent to which the Markovian assumption is invalid , and derive a number of avenues for further research . \",\n",
       " 14383928: 'Lightweight automatic face annotation in media pages Labeling human faces in images contained in Web media stories enables enriching the user experience offered by media sites . We propose a lightweight framework for automatic image annotation that exploits named entities mentioned in the article to significantly boost the accuracy of face recognition . While previous works in the area labor to train comprehensive offline visual models for a pre-defined universe of candidates , our approach models the people mentioned in a given story on the y , using a standard Web image search engine as an image sampling mechanism . We overcome multiple sources of noise introduced by this ad-hoc process , to build a fast and robust end-to-end system from off-the-shelf error-prone text analysis and machine vision components . In experiments conducted on approximately 900 faces depicted in 500 stories from a major celebrity news website , we were able to correctly label 81.5 % of the faces while mislabeling 14.8 % of them . ',\n",
       " 14385094: 'Community detection in incomplete information networks With the recent advances in information networks , the problem of community detection has attracted much attention in the last decade . While network community detection has been ubiquitous , the task of collecting complete network data remains challenging in many real-world applications . Usually the collected network is incomplete with most of the edges missing . Commonly , in such networks , all nodes with attributes are available while only the edges within a few local regions of the network can be observed . In this paper , we study the problem of detecting communities in incomplete information networks with missing edges . We first learn a distance metric to reproduce the link-based distance between nodes from the observed edges in the local information regions . We then use the learned distance metric to estimate the distance between any pair of nodes in the network . A hierarchical clustering approach is proposed to detect communities within the incomplete information networks . Empirical studies on real-world information networks demonstrate that our proposed method can effectively detect community structures within incomplete information networks . ',\n",
       " 14386517: 'Semantic navigation on the web of data : specification of routes , web fragments and actions The massive semantic data sources linked in the Web of Data give new meaning to old features like navigation ; introduce new challenges like semantic specification of Web fragments ; and make it possible to specify actions relying on semantic data . In this paper we introduce a declarative language to face these challenges . Based on navigational features , it is designed to specify fragments of the Web of Data and actions to be performed based on these data . We implement it in a centralized fashion , and show its power and performance . Finally , we explore the same ideas in a distributed setting , showing their feasibility , potentialities and challenges . ',\n",
       " 14386992: 'Counting beyond a Yottabyte , or how SPARQL 1.1 property paths will prevent adoption of the standard SPARQL - the standard query language for querying RDF - provides only limited navigational functionalities , although these features are of fundamental importance for graph data formats such as RDF . This has led the W3C to include the property path feature in the upcoming version of the standard , SPARQL 1.1 . We tested several implementations of SPARQL 1.1 handling property path queries , and we observed that their evaluation methods for this class of queries have a poor performance even in some very simple scenarios . To formally explain this fact , we conduct a theoretical study of the computational complexity of property paths evaluation . Our results imply that the poor performance of the tested implementations is not a problem of these particular systems , but of the specification itself . In fact , we show that any implementation that adheres to the SPARQL 1.1 specification ( as of November 2011 ) is doomed to show the same behavior , the key issue being the need for counting solutions imposed by the current specification . We provide several intractability results , that together with our empirical results , provide strong evidence against the current semantics of SPARQL 1.1 property paths . Finally , we put our results in perspective , and propose a natural alternative semantics with tractable evaluation , that we think may lead to a wide adoption of the language by practitioners , developers and theoreticians . ',\n",
       " 14387519: 'Joint relevance and freshness learning from clickthroughs for news search In contrast to traditional Web search , where topical relevance is often the main selection criterion , news search is characterized by the increased importance of freshness . However , the estimation of relevance and freshness , and especially the relative importance of these two aspects , are highly specific to the query and the time when the query was issued . In this work , we propose a unified framework for modeling the topical relevance and freshness , as well as their relative importance , based on click logs . We use click statistics and content analysis techniques to define a set of temporal features , which predict the right mix of freshness and relevance for a given query . Experimental results on both historical click data and editorial judgments demonstrate the effectiveness of the proposed approach . ',\n",
       " 14387989: \"Multi-objective ranking of comments on web With the explosion of information on any topic , the need for ranking is becoming very critical . Ranking typically depends on several aspects . Products , for example , have several aspects like price , recency , rating , etc. . Product ranking has to bring the `` best '' product which is recent and highly rated . Hence ranking has to satisfy multiple objectives . In this paper , we explore multi-objective ranking of comments using Hodge decomposition . While Hodge decomposition produces a globally consistent ranking , a globally inconsistent component is also present . We propose an active learning strategy for the reduction of this component . Finally , we develop techniques for online Hodge decomposition . We experimentally validate the ideas presented in this paper . \",\n",
       " 14388642: \"Unsupervised extraction of template structure in web search queries Web search queries are an encoding of the user 's search intent and extracting structured information from them can facilitate central search engine operations like improving the ranking of search results and advertisements . Not surprisingly , this area has attracted a lot of attention in the research community in the last few years . The problem is , however , made challenging by the fact that search queries tend to be extremely succinct ; a condensation of user search needs to the bare-minimum set of keywords . In this paper we consider the problem of extracting , with no manual intervention , the hidden structure behind the observed search queries in a domain : the origins of the constituent keywords as well as the manner the individual keywords are assembled together . We formalize important properties of the problem and then give a principled solution based on generative models that satisfies these properties . Using manually labeled data we show that the query templates extracted by our solution are superior to those discovered by strong baseline methods . The query templates extracted by our approach have potential uses in many search engine tasks ; query answering , advertisement matching and targeting , to name a few . In this paper we study one such task , estimating Query-Advertisability , and empirically demonstrate that using extracted template information can improve performance over and above the current state-of-the-art . \",\n",
       " 14388703: \"Actions speak as loud as words : predicting relationships from social behavior data In recent years , new studies concentrating on analyzing user personality and finding credible content in social media have become quite popular . Most such work augments features from textual content with features representing the user 's social ties and the tie strength . Social ties are crucial in understanding the network the people are a part of . However , textual content is extremely useful in understanding topics discussed and the personality of the individual . We bring a new dimension to this type of analysis with methods to compute the type of ties individuals have and the strength of the ties in each dimension . We present a new genre of behavioral features that are able to capture the `` function '' of a specific relationship without the help of textual features . Our novel features are based on the statistical properties of communication patterns between individuals such as reciprocity , assortativity , attention and latency . We introduce a new methodology for determining how such features can be compared to textual features , and show , using Twitter data , that our features can be used to capture contextual information present in textual features very accurately . Conversely , we also demonstrate how textual features can be used to determine social attributes related to an individual . \",\n",
       " 14388913: 'Declarative platform for data sourcing games Harnessing a crowd of users for the collection of mass data ( data sourcing ) has recently become a wide-spread practice . One effective technique is based on games as a tool that attracts the crowd to contribute useful facts . We focus here on the data management layer of such games , and observe that the development of this layer involves challenges such as dealing with probabilistic data , combined with recursive manipulation of this data . These challenges are difficult to address using current declarative data management framework works , and we thus propose here a novel such framework , and demonstrate its usefulness in expressing different aspects in the data management of Trivia-like games . We have implemented a system prototype with our novel data management framework at its core , and we highlight key issues in the system design , as well as our experimentations that indicate the usefulness and scalability of the approach . ',\n",
       " 14388926: 'Distributed graph pattern matching Graph simulation has been adopted for pattern matching to reduce the complexity and capture the need of novel applications . With the rapid development of the Web and social networks , data is typically distributed over multiple machines . Hence a natural question raised is how to evaluate graph simulation on distributed data . To our knowledge , no such distributed algorithms are in place yet . This paper settles this question by providing evaluation algorithms and optimizations for graph simulation in a distributed setting . ( 1 ) We study the impacts of components and data locality on the evaluation of graph simulation . ( 2 ) We give an analysis of a large class of distributed algorithms , captured by a message-passing model , for graph simulation . We also identify three complexity measures : visit times , makespan and data shipment , for analyzing the distributed algorithms , and show that these measures are essentially controversial with each other . ( 3 ) We propose distributed algorithms and optimization techniques that exploit the properties of graph simulation and the analyses of distributed algorithms . ( 4 ) We experimentally verify the effectiveness and efficiency of these algorithms , using both real-life and synthetic data . ',\n",
       " 14389008: 'Mr. LDA : a flexible large scale topic modeling package using variational inference in MapReduce Latent Dirichlet Allocation ( LDA ) is a popular topic modeling technique for exploring document collections . Because of the increasing prevalence of large datasets , there is a need to improve the scalability of inference for LDA . In this paper , we introduce a novel and flexible large scale topic modeling package in MapReduce ( Mr. LDA ) . As opposed to other techniques which use Gibbs sampling , our proposed framework uses variational inference , which easily fits into a distributed environment . More importantly , this variational implementation , unlike highly tuned and specialized implementations based on Gibbs sampling , is easily extensible . We demonstrate two extensions of the models possible with this scalable framework : informed priors to guide topic discovery and extracting topics from a multilingual corpus . We compare the scalability of Mr. LDA against Mahout , an existing large scale topic modeling package . Mr. LDA out-performs Mahout both in execution speed and held-out likelihood . ',\n",
       " 14389159: 'QUBE : a quick algorithm for updating betweenness centrality The betweenness centrality of a vertex in a graph is a measure for the participation of the vertex in the shortest paths in the graph . The Betweenness centrality is widely used in network analyses . Especially in a social network , the recursive computation of the betweenness centralities of vertices is performed for the community detection and finding the influential user in the network . Since a social network graph is frequently updated , it is necessary to update the betweenness centrality efficiently . When a graph is changed , the betweenness centralities of all the vertices should be recomputed from scratch using all the vertices in the graph . To the best of our knowledge , this is the first work that proposes an efficient algorithm which handles the update of the betweenness centralities of vertices in a graph . In this paper , we propose a method that efficiently reduces the search space by finding a candidate set of vertices whose betweenness centralities can be updated and computes their betweenness centeralities using candidate vertices only . As the cost of calculating the betweenness centrality mainly depends on the number of vertices to be considered , the proposed algorithm significantly reduces the cost of calculation . The proposed algorithm allows the transformation of an existing algorithm which does not consider the graph update . Experimental results on large real datasets show that the proposed algorithm speeds up the existing algorithm 2 to 2418 times depending on the dataset . ',\n",
       " 14389185: 'On directly mapping relational databases to RDF and OWL Mapping relational databases to RDF is a fundamental problem for the development of the Semantic Web . We present a solution , inspired by draft methods defined by the W3C where relational databases are directly mapped to RDF and OWL . Given a relational database schema and its integrity constraints , this direct mapping produces an OWL ontology , which , provides the basis for generating RDF instances . The semantics of this mapping is defined using Datalog . Two fundamental properties are information preservation and query preservation . We prove that our mapping satisfies both conditions , even for relational databases that contain null values . We also consider two desirable properties : monotonicity and semantics preservation . We prove that our mapping is monotone and also prove that no monotone mapping , including ours , is semantic preserving . We realize that monotonicity is an obstacle for semantic preservation and thus present a non-monotone direct mapping that is semantics preserving . ',\n",
       " 14389220: \"Strategic formation of credit networks Credit networks are an abstraction for modeling trust between agents in a network . Agents who do not directly trust each other can transact through exchange of IOUs ( obligations ) along a chain of trust in the network . Credit networks are robust to intrusion , can enable transactions between strangers in exchange economies , and have the liquidity to support a high rate of transactions . We study the formation of such networks when agents strategically decide how much credit to extend each other . When each agent trusts a fixed set of other agents , and transacts directly only with those it trusts , the formation game is a potential game and all Nash equilibria are social optima . Moreover , the Nash equilibria of this game are equivalent in a very strong sense : the sequences of transactions that can be supported from each equilibrium credit network are identical . When we allow transactions over longer paths , the game may not admit a Nash equilibrium , and even when it does , the price of anarchy may be unbounded . Hence , we study two special cases . First , when agents have a shared belief about the trustworthiness of each agent , the networks formed in equilibrium have a star-like structure . Though the price of anarchy is unbounded , myopic best response quickly converges to a social optimum . Similar star-like structures are found in equilibria of heuristic strategies found via simulation . In addition , we simulate a second case where agents may have varying information about each others ' trustworthiness based on their distance in a social network . Empirical game analysis of these scenarios suggests that star structures arise only when defaults are relatively rare , and otherwise , credit tends to be issued over short social distances conforming to the locality of information . \",\n",
       " 14389382: \"Answering search queries with CrowdSearcher Web users are increasingly relying on social interaction to complete and validate the results of their search activities . While search systems are superior machines to get world-wide information , the opinions collected within friends and expert\\\\/local communities can ultimately determine our decisions : human curiosity and creativity is often capable of going much beyond the capabilities of search systems in scouting `` interesting '' results , or suggesting new , unexpected search directions . Such personalized interaction occurs in most times aside of the search systems and processes , possibly instrumented and mediated by a social network ; when such interaction is completed and users resort to the use of search systems , they do it through new queries , loosely related to the previous search or to the social interaction . In this paper we propose CrowdSearcher , a novel search paradigm that embodies crowds as first-class sources for the information seeking process . CrowdSearcher aims at filling the gap between generalized search systems , which operate upon world-wide information - including facts and recommendations as crawled and indexed by computerized systems - with social systems , capable of interacting with real people , in real time , to capture their opinions , suggestions , emotions . The technical contribution of this paper is the discussion of a model and architecture for integrating computerized search with human interaction , by showing how search systems can drive and encapsulate social systems . In particular we show how social platforms , such as Facebook , LinkedIn and Twitter , can be used for crowdsourcing search-related tasks ; we demonstrate our approach with several prototypes and we report on our experiment upon real user communities . \",\n",
       " 14392043: 'CloudGenius : decision support for web server cloud migration Cloud computing is the latest computing paradigm that delivers hardware and software resources as virtualized services in which users are free from the burden of worrying about the low-level system administration details . Migrating Web applications to Cloud services and integrating Cloud services into existing computing infrastructures is non-trivial . It leads to new challenges that often require innovation of paradigms and practices at all levels : technical , cultural , legal , regulatory , and social . The key problem in mapping Web applications to virtualized Cloud services is selecting the best and compatible mix of software images ( e.g. , Web server image ) and infrastructure services to ensure that Quality of Service ( QoS ) targets of an application are achieved . The fact that , when selecting Cloud services , engineers must consider heterogeneous sets of criteria and complex dependencies between infrastructure services and software images , which are impossible to resolve manually , is a critical issue . To overcome these challenges , we present a framework ( called CloudGenius ) which automates the decision-making process based on a model and factors specifically for Web server migration to the Cloud . CloudGenius leverages a well known multi-criteria decision making technique , called Analytic Hierarchy Process , to automate the selection process based on a model , factors , and QoS parameters related to an application . An example application demonstrates the applicability of the theoretical CloudGenius approach . Moreover , we present an implementation of CloudGenius that has been validated through experiments . ',\n",
       " 14392151: 'Towards network-aware service composition in the cloud Service-Oriented Computing ( SOC ) enables the composition of loosely coupled services provided with varying Quality of Service ( QoS ) levels . Selecting a ( near - ) optimal set of services for a composition in terms of QoS is crucial when many functionally equivalent services are available . With the advent of Cloud Computing , both the number of such services and their distribution across the network are rising rapidly , increasing the impact of the network on the QoS of such compositions . Despite this , current approaches do not differentiate between the QoS of services themselves and the QoS of the network . Therefore , the computed latency differs substantially from the actual latency , resulting in suboptimal QoS for service compositions in the cloud . Thus , we propose a network-aware approach that handles the QoS of services and the QoS of the network independently . First , we build a network model in order to estimate the network latency between arbitrary services and potential users . Our selection algorithm then leverages this model to find compositions that will result in a low latency given an employed execution policy . In our evaluation , we show that our approach efficiently computes compositions with much lower latency than current approaches . ',\n",
       " 14394291: \"Practical end-to-end web content integrity Widespread growth of open wireless hotspots has made it easy to carry out man-in-the-middle attacks and impersonate web sites . Although HTTPS can be used to prevent such attacks , its universal adoption is hindered by its performance cost and its inability to leverage caching at intermediate servers ( such as CDN servers and caching proxies ) while maintaining end-to-end security . To complement HTTPS , we revive an old idea from SHTTP , a protocol that offers end-to-end web integrity without confidentiality . We name the protocol HTTPi and give it an efficient design that is easy to deploy for today 's web . In particular , we tackle several previously-unidentified challenges , such as supporting progressive page loading on the client 's browser , handling mixed content , and defining access control policies among HTTP , HTTPi , and HTTPS content from the same domain . Our prototyping and evaluation experience show that HTTPi incurs negligible performance overhead over HTTP , can leverage existing web infrastructure such as CDNs or caching proxies without any modifications to them , and can make many of the mixed-content problems in existing HTTPS web sites easily go away . Based on this experience , we advocate browser and web server vendors to adopt HTTPi . \",\n",
       " 14394643: 'Recommendations to boost content spread in social networks Content sharing in social networks is a powerful mechanism for discovering content on the Internet . The degree to which content is disseminated within the network depends on the connectivity relationships among network nodes . Existing schemes for recommending connections in social networks are based on the number of common neighbors , similarity of user profiles , etc. . However , such similarity-based connections do not consider the amount of content discovered . In this paper , we propose novel algorithms for recommending connections that boost content propagation in a social network without compromising on the relevance of the recommendations . Unlike existing work on influence propagation , in our environment , we are looking for edges instead of nodes , with a bound on the number of incident edges per node . We show that the content spread function is not submodular , and develop approximation algorithms for computing a near-optimal set of edges . Through experiments on real-world social graphs such as Flickr and Twitter , we show that our approximation algorithms achieve content spreads that are as much as 90 times higher compared to existing heuristics for recommending connections . ',\n",
       " 14395405: 'Leveraging user comments for aesthetic aware image search reranking The increasing number of images available online has created a growing need for efficient ways to search for relevant content . Text-based query search is the most common approach to retrieve images from the Web . In this approach , the similarity between the input query and the metadata of images is used to find relevant information . However , as the amount of available images grows , the number of relevant images also increases , all of them sharing very similar metadata but differing in other visual characteristics . This paper studies the influence of visual aesthetic quality in search results as a complementary attribute to relevance . By considering aesthetics , a new ranking parameter is introduced aimed at improving the quality at the top ranks when large amounts of relevant results exist . Two strategies for aesthetic rating inference are proposed : one based on visual content , another based on the analysis of user comments to detect opinions about the quality of images . The results of a user study with $ 58 $ participants show that the comment-based aesthetic predictor outperforms the visual content-based strategy , and reveals that aesthetic-aware rankings are preferred by users searching for photographs on the Web . ',\n",
       " 14395695: 'Beyond dwell time : estimating document relevance from cursor movements and other post-click searcher behavior Result clickthrough statistics and dwell time on clicked results have been shown valuable for inferring search result relevance , but the interpretation of these signals can vary substantially for different tasks and users . This paper shows that that post-click searcher behavior , such as cursor movement and scrolling , provides additional clues for better estimating document relevance . To this end , we identify patterns of examination and interaction behavior that correspond to viewing a relevant or non-relevant document , and design a new Post-Click Behavior ( PCB ) model to capture these patterns . To our knowledge , PCB is the first to successfully incorporate post-click searcher interactions such as cursor movements and scrolling on a landing page for estimating document relevance . We evaluate PCB on a dataset collected from a controlled user study that contains interactions gathered from hundreds of unique queries , result clicks , and page examinations . The experimental results show that PCB is significantly more effective than using page dwell time information alone , both for estimating the explicit judgments of each user , and for re-ranking the results using the estimated relevance . ',\n",
       " 14395704: \"A dual-mode user interface for accessing 3D content on the world wide web The Web evolved from a text-based system to the current rich and interactive medium that supports images , 2D graphics , audio and video . The major media type that is still missing is 3D graphics . Although various approaches have been proposed ( most notably VRML\\\\/X3D ) , they have not been widely adopted . One reason for the limited acceptance is the lack of 3D interaction techniques that are optimal for the hypertext-based Web interface . We present a novel strategy for accessing integrated information spaces , where hypertext and 3D graphics data are simultaneously available and linked . We introduce a user interface that has two modes between which a user can switch anytime : the driven by simple hypertext-based interactions `` do n't - make-me-think '' mode , where a 3D scene is embedded in hypertext and the more immersive 3D `` take-me-to-the-Wonderland '' mode , which immerses the hypertextual annotations into the 3D scene . A user study is presented , which characterizes the user interface in terms of its efficiency and usability . \",\n",
       " 14396486: 'Collective context-aware topic models for entity disambiguation A crucial step in adding structure to unstructured data is to identify references to entities and disambiguate them . Such disambiguated references can help enhance readability and draw similarities across different pieces of running text in an automated fashion . Previous research has tackled this problem by first forming a catalog of entities from a knowledge base , such as Wikipedia , and then using this catalog to disambiguate references in unseen text . However , most of the previously proposed models either do not use all text in the knowledge base , potentially missing out on discriminative features , or do not exploit word-entity proximity to learn high-quality catalogs . In this work , we propose topic models that keep track of the context of every word in the knowledge base ; so that words appearing within the same context as an entity are more likely to be associated with that entity . Thus , our topic models utilize all text present in the knowledge base and help learn high-quality catalogs . Our models also learn groups of co-occurring entities thus enabling collective disambiguation . Unlike most previous topic models , our models are non-parametric and do not require the user to specify the exact number of groups present in the knowledge base . In experiments performed on an extract of Wikipedia containing almost 60,000 references , our models outperform SVM-based baselines by as much as 18 % in terms of disambiguation accuracy translating to an increment of almost 11,000 correctly disambiguated references . ',\n",
       " 14397937: 'Online modeling of proactive moderation system for auction fraud detection We consider the problem of building online machine-learned models for detecting auction frauds in e-commence web sites . Since the emergence of the world wide web , online shopping and online auction have gained more and more popularity . While people are enjoying the benefits from online trading , criminals are also taking advantages to conduct fraudulent activities against honest parties to obtain illegal profit . Hence proactive fraud-detection moderation systems are commonly applied in practice to detect and prevent such illegal and fraud activities . Machine-learned models , especially those that are learned online , are able to catch frauds more efficiently and quickly than human-tuned rule-based systems . In this paper , we propose an online probit model framework which takes online feature selection , coefficient bounds from human knowledge and multiple instance learning into account simultaneously . By empirical experiments on a real-world online auction fraud detection data we show that this model can potentially detect more frauds and significantly reduce customer complaints compared to several baseline models and the human-tuned rule-based system . ',\n",
       " 14426843: 'D2RQ\\\\/update : updating relational data via virtual RDF D2RQ is a popular RDB-to-RDF mapping platform that supports mapping relational databases to RDF and posing SPARQL queries to these relational databases . However , D2RQ merely provides a read-only RDF view on relational databases . Thus , we introduce D2RQ\\\\/Update -- an extension of D2RQ to enable executing SPARQL\\\\/Update statements on the mapped data , and to facilitate the creation of a read-write Semantic Web . ',\n",
       " 14435710: 'LINDEN : linking named entities with knowledge base via semantic knowledge Integrating the extracted facts with an existing knowledge base has raised an urgent need to address the problem of entity linking . Specifically , entity linking is the task to link the entity mention in text with the corresponding real world entity in the existing knowledge base . However , this task is challenging due to name ambiguity , textual inconsistency , and lack of world knowledge in the knowledge base . Several methods have been proposed to tackle this problem , but they are largely based on the co-occurrence statistics of terms between the text around the entity mention and the document associated with the entity . In this paper , we propose LINDEN , a novel framework to link named entities in text with a knowledge base unifying Wikipedia and WordNet , by leveraging the rich semantic knowledge embedded in the Wikipedia and the taxonomy of the knowledge base . We extensively evaluate the performance of our proposed LINDEN over two public data sets and empirical results show that LINDEN significantly outperforms the state-of-the-art methods in terms of accuracy . ',\n",
       " 14445422: \"Evaluating the effectiveness of search task trails In this paper , we introduce `` task trail '' as a new concept to understand user search behaviors . We define task to be an atomic user information need . Web search logs have been studied mainly at session or query level where users may submit several queries within one task and handle several tasks within one session . Although previous studies have addressed the problem of task identification , little is known about the advantage of using task over session and query for search applications . In this paper , we conduct extensive analyses and comparisons to evaluate the effectiveness of task trails in three search applications : determining user satisfaction , predicting user search interests , and query suggestion . Experiments are conducted on large scale datasets from a commercial search engine . Experimental results show that : ( 1 ) Sessions and queries are not as precise as tasks in determining user satisfaction . ( 2 ) Task trails provide higher web page utilities to users than other sources . ( 3 ) Tasks represent atomic user information needs , and therefore can preserve topic similarity between query pairs . ( 4 ) Task-based query suggestion can provide complementary results to other models . The findings in this paper verify the need to extract task trails from web search logs and suggest potential applications in search and recommendation systems . \",\n",
       " 14445546: 'WISER : a web-based interactive route search system for smartphones Many smartphones , nowadays , use GPS to detect the location of the user , and can use the Internet to interact with remote location-based services . These two capabilities support online navigation that incorporates search . In this demo we presents WISER -- a system for Web-based Interactive Search en Route . In the system , users perform route search by providing ( 1 ) a target location , and ( 2 ) search terms that specify types of geographic entities to be visited . The task is to find a route that minimizes the travel distance from the initial location of the user to the target , via entities of the specified types . However , planning a route under conditions of uncertainty requires the system to take into account the possibility that some visited entities will not satisfy the search requirements , so that the route may need to go via several entities of the same type . In an interactive search , the user provides feedback regarding her satisfaction with entities she visits during the travel , and the system changes the route , in real time , accordingly . The goal is to use the interaction for computing a route that is more effective than a route that is computed in a non-interactive fashion . ',\n",
       " 14449751: 'A flexible generative model for preference aggregation Many areas of study , such as information retrieval , collaborative filtering , and social choice face the preference aggregation problem , in which multiple preferences over objects must be combined into a consensus ranking . Preferences over items can be expressed in a variety of forms , which makes the aggregation problem difficult . In this work we formulate a flexible probabilistic model over pairwise comparisons that can accommodate all these forms . Inference in the model is very fast , making it applicable to problems with hundreds of thousands of preferences . Experiments on benchmark datasets demonstrate superior performance to existing methods ',\n",
       " 14453157: \"Evaluation with informational and navigational intents Given an ambiguous or underspecified query , search result diversification aims at accomodating different user intents within a single `` entry-point '' result page . However , some intents are informational , for which many relevant pages may help , while others are navigational , for which only one web page is required . We propose new evaluation metrics for search result diversification that considers this distinction , as well as a simple method for comparing the intuitiveness of a given pair of metrics quantitatively . Our main experimental findings are : ( a ) In terms of discriminative power which reflects statistical reliability , the proposed metrics , DIN #_# - nDCG and P+Q #_# , are comparable to intent recall and D #_# - nDCG , and possibly superior to α-nDCG ; ( b ) In terms of preference agreement with intent recall , P+Q #_# is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises diversity ; and ( c ) In terms of preference agreement with effective precision , DIN #_# - nDCG is superior to other diversity metrics and therefore may be the most intuitive as a metric that emphasises relevance . Moreover , DIN #_# - nDCG may be the most intuitive as a metric that considers both diversity and relevance . In addition , we demonstrate that the randomised Tukey 's Honestly Significant Differences test that takes the entire set of available runs into account is substantially more conservative than the paired bootstrap test that only considers one run pair at a time , and therefore recommend the former approach for significance testing when a set of runs is available for evaluation . \",\n",
       " 14453752: 'Template-based question answering over RDF data As an increasing amount of RDF data is published as Linked Data , intuitive ways of accessing this data become more and more important . Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity . Most question answering systems translate questions into triples which are matched against the RDF data to retrieve an answer , typically relying on some similarity metric . However , in many cases , triples do not represent a faithful representation of the semantic structure of the natural language question , with the result that more expressive queries can not be answered . To circumvent this problem , we present a novel approach that relies on a parse of the question to produce a SPARQL template that directly mirrors the internal structure of the question . This template is then instantiated using statistical entity identification and predicate detection . We show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches . ',\n",
       " 14454508: 'ZenCrowd : leveraging probabilistic reasoning and crowdsourcing techniques for large-scale entity linking We tackle the problem of entity linking for large collections of online pages ; Our system , ZenCrowd , identifies entities from natural language text using state of the art techniques and automatically connects them to the Linked Open Data cloud . We show how one can take advantage of human intelligence to improve the quality of the links by dynamically generating micro-tasks on an online crowdsourcing platform . We develop a probabilistic framework to make sensible decisions about candidate links and to identify unreliable human workers . We evaluate ZenCrowd in a real deployment and show how a combination of both probabilistic reasoning and crowdsourcing techniques can significantly improve the quality of the links , while limiting the amount of work performed by the crowd . ',\n",
       " 14475693: \"Understanding task-driven information flow in collaborative networks Collaborative networks are a special type of social network formed by members who collectively achieve specific goals , such as fixing software bugs and resolving customers ' problems . In such networks , information flow among members is driven by the tasks assigned to the network , and by the expertise of its members to complete those tasks . In this work , we analyze real-life collaborative networks to understand their common characteristics and how information is routed in these networks . Our study shows that collaborative networks exhibit significantly different properties compared with other complex networks . Collaborative networks have truncated power-law node degree distributions and other organizational constraints . Furthermore , the number of steps along which information is routed follows a truncated power-law distribution . Based on these observations , we developed a network model that can generate synthetic collaborative networks subject to certain structure constraints . Moreover , we developed a routing model that emulates task-driven information routing conducted by human beings in a collaborative network . Together , these two models can be used to study the efficiency of information routing for different types of collaborative networks -- a problem that is important in practice yet difficult to solve without the method proposed in this paper . \",\n",
       " 1454933: 'Three theses of representation in the semantic web The Semantic Web is vitally dependent on a formal meaning for the constructs of its languages . For Semantic Web languages to work well together their formal meanings must employ a common view ( or thesis ) of representation , otherwise it will not be possible to reconcile documents written in different languages . The thesis of representation underlying RDF and RDFS is particularly troublesome in this regard , as it has several unusual aspects , both semantic and syntactic . A more-standard thesis of representation would result in the ability to reuse existing results and tools in the Semantic Web . ',\n",
       " 1605069: 'Law-governed peer-to-peer auctions This paper proposes a flexible architecture for the creation of Internet auctions . It allows the custom definition of the auction parameters , and provides a decentralized control of the auction process . Auction policies are defined as laws in the Law Governed Interaction ( LGI ) paradigm . Each of these laws specifies not only the auction algorithm itself ( e.g. open-cry , dutch , etc. ) but also how to handle the other parameters usually involved in the online auctions , such as certification , auditioning , and treatment of complaints . LGI is used to enforce the rules established in the auction policy within the agents involved in the process . After the agents find out about the actions , they interact in a peer-to-peer communication protocol , reducing the role of the centralized auction room to an advertising registry , and taking profit of the distributed nature of the Internet to conduct the auction . The paper presents an example of an auction law , illustrating the use of the proposed architecture . ',\n",
       " 163229: 'Hunter gatherer : interaction support for the creation and management of within-web-page collections Hunter Gatherer is an interface that lets Web users carry out three main tasks : ( 1 ) collect components from within Web pages ; ( 2 ) represent those components in a collection ; ( 3 ) edit those component collections . Our research shows that while the practice of making collections of content from within Web pages is common , it is not frequent , due in large part to poor interaction support in existing tools . We engaged with users in task analysis as well as iterative design reviews in order to understand the interaction issues that are part of within-Web-page collection making and to design an interaction that would support that process . We report here on that design development , as well as on the evaluations of the tool that evolved from that process , and the future work stemming from these results , in which our critical question is : what happens to users perceptions and expectations of web-based information ( their web-based information management practices ) when they can treat this information as harvestable , recontextualizable data , rather than as fixed pages ? ',\n",
       " 16334: \"Searching the workplace web The social impact from the World Wide Web can not be underestimated , but technologies used to build the Web are also revolutionizing the sharing of business and government information within intranets . In many ways the lessons learned from the Internet carry over directly to intranets , but others do not apply . In particular , the social forces that guide the development of intranets are quite different , and the determination of a `` good answer '' for intranet search is quite different than on the Internet . In this paper we study the problem of intranet search . Our approach focuses on the use of rank aggregation , and allows us to examine the effects of different heuristics on ranking of search results . \",\n",
       " 16381: 'SemTag and seeker : bootstrapping the semantic web via automated semantic annotation This paper describes Seeker , a platform for large-scale text analytics , and SemTag , an application written on the platform to perform automated semantic tagging of large corpora . We apply SemTag to a collection of approximately 264 million web pages , and generate approximately 434 million automatically disambiguated semantic tags , published to the web as a label bureau providing metadata regarding the 434 million annotations . To our knowledge , this is the largest scale semantic tagging effort to date . We describe the Seeker platform , discuss the architecture of the SemTag application , describe a new disambiguation algorithm specialized to support ontological disambiguation of large-scale data , evaluate the algorithm , and present our final results with information about acquiring and making use of the semantic tags . We argue that automated large scale semantic tagging of ambiguous content can bootstrap and accelerate the creation of the semantic web . ',\n",
       " 16387: 'A software framework for matchmaking based on semantic web technology An important objective of the Semantic Web is to make Electronic Commerce interactions more flexible and automated . To achieve this , standardization of ontologies , message content and message protocols will be necessary . In this paper we investigate how Semantic and Web Services technologies can be used to support service advertisement and discovery in e-commerce . In particular , we describe the design and implementation of a service matchmaking prototype which uses a DAML-S based ontology and a Description Logic reasoner to compare ontology based service descriptions . We also present the results of initial experiments testing the performance of this prototype implementation in a realistic agent based e-commerce scenario . ',\n",
       " 1659738: 'Accelerated focused crawling through online relevance feedback The organization of HTML into a tag tree structure , which is rendered by browsers as roughly rectangular regions with embedded text and HREF links , greatly helps surfers locate and click on links that best satisfy their information need . Can an automatic program emulate this human behavior and thereby learn to predict the relevance of an unseen HREF target page w.r.t. an information need , based on information limited to the HREF source page ? Such a capability would be of great interest in focused crawling and resource discovery , because it can fine-tune the priority of unvisited URLs in the crawl frontier , and reduce the number of irrelevant pages which are fetched and discarded . ',\n",
       " 16706: \"Implementing physical hyperlinks using ubiquitous identifier resolution Identifier resolution is presented as a way to link the physical world with virtual Web resources . In this paradigm , designed to support nomadic users , the user employs a handheld , wirelessly connected , sensor-equipped device to read identifiers associated with physical entities . The identifiers are resolved into virtual resources or actions related to the physical entities - as though the user ` clicked on a physical hyperlink ' . We have integrated identifier resolution with the Web so that it can be deployed as ubiquitously as the Web , in the infrastructure and on wirelessly connected handheld devices . We enable users to capture resolution services and applications as Web resources in their local context . We use the Web to invoke resolution services , with a model of ` physical ' Web form-filling . We propose a scheme for binding identifiers to resources , to promote services and applications linking the physical and virtual worlds . \",\n",
       " 173979: 'Probabilistic question answering on the web Web-based search engines such as Google and NorthernLight return documents that are relevant to a user query , not answers to user questions . We have developed an architecture that augments existing search engines so that they support natural language question answering . The process entails five steps : query modulation , document retrieval , passage extraction , phrase extraction , and answer ranking . In this paper we describe some probabilistic approaches to the last three of these stages . We show how our techniques apply to a number of existing search engines and we also present results contrasting three different methods for question answering . Our algorithm , probabilistic phrase reranking ( PPR ) using proximity and question type features achieves a total reciprocal document rank of .20 on the TREC 8 corpus . Our techniques have been implemented as a Web-accessible system , called NSIR . ',\n",
       " 178204: 'Application specific data replication for edge services The emerging edge services architecture promises to improve the availability and performance of web services by replicating servers at geographically distributed sites . A key challenge in such systems is data replication and consistency so that edge server code can manipulate shared data without incurring the availability and performance penalties that would be incurred by accessing a traditional centralized database . This paper explores using a distributed object architecture to build an edge service system for an e-commerce application , an online bookstore represented by the TPC-W benchmark . We take advantage of application specific semantics to design distributed objects to manage a specific subset of shared information using simple and effective consistency models . Our experimental results show that by slightly relaxing consistency within individual distributed objects , we can build an edge service system that is highly available and efficient . For example , in one experiment we find that our object-based edge server system provides a factor of five improvement in response time over a traditional centralized cluster architecture and a factor of nine improvement over an edge service system that distributes code but retains a centralized database . ',\n",
       " 178963: 'Learning to tag Social tagging provides valuable and crucial information for large-scale web image retrieval . It is ontology-free and easy to obtain ; however , irrelevant tags frequently appear , and users typically will not tag all semantic objects in the image , which is also called semantic loss . To avoid noises and compensate for the semantic loss , tag recommendation is proposed in literature . However , current recommendation simply ranks the related tags based on the single modality of tag co-occurrence on the whole dataset , which ignores other modalities , such as visual correlation . This paper proposes a multi-modality recommendation based on both tag and visual correlation , and formulates the tag recommendation as a learning problem . Each modality is used to generate a ranking feature , and Rankboost algorithm is applied to learn an optimal combination of these ranking features from different modalities . Experiments on Flickr data demonstrate the effectiveness of this learning-based multi-modality recommendation strategy . ',\n",
       " 1810492: 'Clustering for opportunistic communication We describe ongoing work on I2I , a system aimed at fostering opportunistic communication among users viewing or manipulating content on the Web and in productivity applications . Unlike previous work in which the URLs of Web resources are used to group users visiting the same resource , we present a more general framework for clustering work contexts to group users together that accounts for dynamic content and distributional properties of Web accesses which can limit the utility URL based systems . In addition , we describe a method for scaffolding asynchronous communication in the context of an ongoing task that takes into account the ephemeral nature of the location of content on the Web . The techniques we describe also nicely cover local files in progress , in addition to publicly available Web content . We present the results of several evaluations that indicate systems that use the techniques we employ may be more useful than systems that are strictly URL based . ',\n",
       " 183: \"The Eigentrust algorithm for reputation management in P2P networks Peer-to-peer file-sharing networks are currently receiving much attention as a means of sharing and distributing information . However , as recent experience shows , the anonymous , open nature of these networks offers an almost ideal environment for the spread of self-replicating inauthentic files . We describe an algorithm to decrease the number of downloads of inauthentic files in a peer-to-peer file-sharing network that assigns each peer a unique global trust value , based on the peer 's history of uploads . We present a distributed and secure method to compute global trust values , based on Power iteration . By having peers use these global trust values to choose the peers from whom they download , the network effectively identifies malicious peers and isolates them from the network . In simulations , this reputation system , called EigenTrust , has been shown to significantly decrease the number of inauthentic files on the network , even under a variety of conditions where malicious peers cooperate in an attempt to deliberately subvert the system . \",\n",
       " 1833006: 'A smart hill-climbing algorithm for application server configuration The overwhelming success of the Web as a mechanism for facilitating information retrieval and for conducting business transactions has ledto an increase in the deployment of complex enterprise applications . These applications typically run on Web Application Servers , which assume the burden of managing many tasks , such as concurrency , memory management , database access , etc. , required by these applications . The performance of an Application Server depends heavily on appropriate configuration . Configuration is a difficult and error-prone task dueto the large number of configuration parameters and complex interactions between them . We formulate the problem of finding an optimal configuration for a given application as a black-box optimization problem . We propose a smart hill-climbing algorithm using ideas of importance sampling and Latin Hypercube Sampling ( LHS ) . The algorithm is efficient in both searching and random sampling . It consists of estimating a local function , and then , hill-climbing in the steepest descent direction . The algorithm also learns from past searches and restarts in a smart and selective fashion using the idea of importance sampling . We have carried out extensive experiments with an on-line brokerage application running in a WebSphere environment . Empirical results demonstrate that our algorithm is more efficient than and superior to traditional heuristic methods . ',\n",
       " 1833066: \"Post-processing inkml for random-access navigation of voluminous handwritten ink documents The goal of this research is the improvement of browsing voluminous InkML data in two areas : ease of rendering continuous ink-flow for replay-browsing , and ease of random access navigation in eLearning domains . The notion of real-time random access navigation in ink documents has not yet been fully exploited . Users of existing eLearning browsers are restricted to viewing static annotated slides that are inferior in quality when compared to actively replaying the same slides with sequenced ink-flow of the annotated freehand writings . We are developing a tool to investigate ways of managing massive InkML data for efficient `` active visible scrolling '' of recorded freehand writings in ink documents . This work will also develop and evaluate new post-processing techniques that take advantage of the relationship between ink volumes and active-rendering times for real-time random access navigation . \",\n",
       " 1839820: \"TCOZ approach to semantic web services design Complex Semantic Web ( SW ) services may have intricate data state , autonomous process behavior and concurrent interactions . The design of such SW service systems requires precise and powerful modelling techniques to capture not only the ontology domain properties but also the services ' process behavior and functionalities . In this paper we apply an integrated formal modeling language , Timed Communicating Object Z ( TCOZ ) , to design SW services . Furthermore , the paper presents the development of the systematic translation rules and tools which can automatically extract the SW ontology and services semantic markup from the formal TCOZ design model . \",\n",
       " 1840610: 'Analyzing client interactivity in streaming media This paper provides an extensive analysis of pre-stored streaming media workloads , focusing on the client interactive behavior . We analyze four workloads that fall into three different domains , namely , education , entertainment video and entertainment audio . Our main goals are : ( a ) to identify qualitative similarities and differences in the typical client behavior for the three workload classes and ( b ) to provide data for generating realistic synthetic workloads . ',\n",
       " 1845167: 'Lessons from a Gnutella-web gateway We present a gateway between the WWW and the Gnutella peer-to-peer network that permits searchers on one side to be able to search and retrieve files on the other side of the gateway . This work improvesthe accessibility of files across different delivery platforms , making it possible to use a single search modality . We outline our design and implementation , present access statistics from a test deployment and discuss lessons learned . ',\n",
       " 1848435: 'Structuring and presenting annotated media repositories We generate hypermedia presentations from annotated media repositories using simple document structure as an intermediate phase . This poster applies Web style technologies to this process . Results include style specification for accessing semantically annotated media repositories , for determining document structure from semantic structure and for applying this document structure to the final presentation . ',\n",
       " 1880971: 'Towards the self-annotating web The success of the Semantic Web depends on the availability of ontologies as well as on the proliferation of web pages annotated with metadata conforming to these ontologies . Thus , a crucial question is where to acquire these metadata from . In this paper wepropose PANKOW ( Pattern-based Annotation through Knowledge on theWeb ) , a method which employs an unsupervised , pattern-based approach to categorize instances with regard to an ontology . The approach is evaluated against the manual annotations of two human subjects . The approach is implemented in OntoMat , an annotation tool for the Semantic Web and shows very promising results . ',\n",
       " 1888132: 'Price modeling in standards for electronic product catalogs based on XML The fast spreading of electronic business-to-business procurement systems has led to the development of new standards for the exchange of electronic product catalogs ( e-catalogs ) . E-catalogs contain various information about products , essential is price information . Prices are used for buying decisions and following order transactions . While simple price models are often sufficient for the description of indirect goods ( e.g. office supplies ) , other goods and lines of business make higher demands . In this paper we examine what price information is contained in commercial XML standards for the exchange of product catalog data . For that purpose we bring the different implicit price models of the examined catalog standards together and provide a generalized model . ',\n",
       " 204579: \"Aliasing on the world wide web : prevalence and performance implications Aliasing occurs in Web transactions when requests containing different URLs elicit replies containing identical data payloads . Conventional caches associate stored data with URLs and can therefore suffer redundant payload transfers due to aliasing and other causes . Existing research literature , however , says little about the prevalence of aliasing in user-initiated transactions , or about redundant payload transfers in conventional Web cache hierarchies . This paper quantifies the extent of aliasing and the performance impact of URL-indexed cache management using a large client trace from WebTV Networks . Fewer than 5 % of reply payloads are aliased ( referenced via multiple URLs ) but over 54 % of successful transactions involve aliased payloads . Aliased payloads account for under 3.1 % of the trace 's `` working set size '' ( sum of payload sizes ) but over 36 % of bytes transferred . For the WebTV workload , roughly 10 % of payload transfers to browser caches and 23 % of payload transfers to a shared proxy are redundant , assuming infinite-capacity conventional caches . Our analysis of a large proxy trace from Compaq Corporation yields similar results.URL-indexed caching does not entirely explain the large number of redundant proxy-to-browser payload transfers previously reported in the WebTV system . We consider other possible causes of redundant transfers ( e.g. , reply metadata and browser cache management policies ) and discuss a simple hop-by-hop protocol extension that completely eliminates all redundant transfers , regardless of cause . \",\n",
       " 204586: 'Value-based web caching Despite traditional web caching techniques , redundant data is often transferred over HTTP links . These redundant transfers result from both resource modification and aliasing . Resource modification causes the data represented by a single URI to change ; often , in transferring the new data , some old data is retransmitted . Aliasing , in contrast , occurs when the same data is named by multiple URIs , often in the context of dynamic or advertising content . Traditional web caching techniques index data by its name and thus often fail to recognize and take advantage of aliasing . Despite traditional web caching techniques , redundant data is often transferred over HTTP links . These redundant transfers result from both resource modification and aliasing . Resource modification causes the data represented by a single URI to change ; often , in transferring the new data , some old data is retransmitted . Aliasing , in contrast , occurs when the same data is named by multiple URIs , often in the context of dynamic or advertising content . Traditional web caching techniques index data by its name and thus often fail to recognize and take advantage of aliasing . ',\n",
       " 21580: \"Jena : implementing the semantic web recommendations The new Semantic Web recommendations for RDF , RDFS and OWL have , at their heart , the RDF graph . Jena2 , a second-generation RDF toolkit , is similarly centered on the RDF graph . RDFS and OWL reasoning are seen as graph-to-graph transforms , producing graphs of virtual triples . Rich APIs are provided . The Model API includes support for other aspects of the RDF recommendations , such as containers and reification . The Ontology API includes support for RDFS and OWL , including advanced OWL Full support . Jena includes the de facto reference RDF\\\\/XML parser , and provides RDF\\\\/XML output using the full range of the rich RDF\\\\/XML grammar . N3 I\\\\/O is supported . RDF graphs can be stored in-memory or in databases . Jena 's query language , RDQL , and the Web API are both offered for the next round of standardization . \",\n",
       " 21587: 'Unparsing RDF\\\\/XML It is difficult to serialize an RDF graph as a humanly readable RDF\\\\/XML document . This paper describes the approach taken in Jena 1.2 , in which a design pattern of guarded procedures invoked using top down recursive descent is used . Each procedure corresponds to a grammar rule ; the guard makes the choice about the applicability of the production . This approach is seen to correspond closely to the design of an LL ( k ) parser , and a theoretical justification of this correspondence is found in universal algebra . ',\n",
       " 217637: 'Personalization in distributed e-learning environments Personalized support for learners becomes even more important , when e-Learning takes place in open and dynamic learning and information networks . This paper shows how to realize personalized learning support in distributed learning environments based on Semantic Web technologies . Our approach fills the existing gap between current adaptive educational systems with well-established personalization functionality , and open , dynamic learning repository networks . We propose a service-based architecture for establishing personalized e-Learning , where personalization functionality is provided by various web-services . A Personal Learning Assistant integrates personalization services and other supporting services , and provides the personalized access to learning resources in an e-Learning network . ',\n",
       " 230498: 'Mining the peanut gallery : opinion extraction and semantic classification of product reviews The web contains a wealth of product reviews , but sifting through them is a daunting task . Ideally , an opinion mining tool would process a set of search results for a given item , generating a list of product attributes ( quality , features , etc. ) and aggregating opinions about each of them ( poor , mixed , good ) . We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews . Our classifier draws on information retrieval techniques for feature extraction and scoring , and the results for various metrics and heuristics vary depending on the testing situation . The best methods work as well as or better than traditional machine learning . When operating on individual sentences collected from web searches , performance is limited due to noise and ambiguity . But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes , the results are qualitatively quite useful . ',\n",
       " 238458: \"A convenient method for securely managing passwords Computer users are asked to generate , keep secret , and recall an increasing number of passwords for uses including host accounts , email servers , e-commerce sites , and online financial services . Unfortunately , the password entropy that users can comfortably memorize seems insufficient to store unique , secure passwords for all these accounts , and it is likely to remain constant as the number of passwords ( and the adversary 's computational power ) increases into the future . In this paper , we propose a technique that uses a strengthened cryptographic hash function to compute secure passwords for arbitrarily many accounts while requiring the user to memorize only a single short password . This mechanism functions entirely on the client ; no server-side changes are needed . Unlike previous approaches , our design is both highly resistant to brute force attacks and nearly stateless , allowing users to retrieve their passwords from any location so long as they can execute our program and remember a short secret . This combination of security and convenience will , we believe , entice users to adopt our scheme . We discuss the construction of our algorithm in detail , compare its strengths and weaknesses to those of related approaches , and present Password Multiplier , an implementation in the form of an extension to the Mozilla Firefox web browser . \",\n",
       " 239996: 'XQuery containment in presence of variable binding dependencies Semantic caching is an important technology for improving the response time of future user queries specified over remote servers . This paper deals with the fundamental query containment problem in an XQuery-based semantic caching system . To our best knowledge , the impact of subtle differences in XQuery semantics caused by different ways of specifying variables on query containment has not yet been studied . We introduce the concept of variable binding dependencies for representing the hierarchical element dependencies preserved by an XQuery . We analyze the problem of XQuery containment in the presence of such dependencies . We propose a containment mapping technique for nested XQuery in presence of variable binding dependencies . The implication of the nested block structure on XQuery containment is also considered . We mention the performance gains achieved by a semantic caching system we build based on the proposed technique . ',\n",
       " 246813: 'A method for modeling uncertainty in semantic web taxonomies We present a method for representing and reasoning with uncertainty in RDF ( S ) and OWL ontologies based on Bayesian networks . ',\n",
       " 247234: 'The indexable web is more than 11.5 billion pages In this short paper we estimate the size of the public indexable web at 11.5 billion pages . We also estimate the overlap and the index size of Google , MSN , Ask\\\\/Teoma and Yahoo ! ',\n",
       " 251906: \"DiTaBBu : automating the production of time-based hypermedia content We present DiTaBBu , Digital Talking Books Builder , a framework for automatic production of time-based hypermedia for the Web , focusing on the Digital Talking Books domain . Delivering Digital Talking Books collections to a wide range of users is an expensive task , as it must take into account each user profile 's different needs , therefore authoring should be dismissed in favor of automation . With DiTaBBu , we enable automated content delivery in several playback platforms , targeted to specific user needs , featuring powerful navigation capabilities over the content . DiTaBBu can also be used as testbed for prototyping novel capabilities , through its flexible extension mechanisms . \",\n",
       " 251909: 'Accessibility : a Web engineering approach Currently , the vast majority of web sites do not support accessibility for visually impaired users . Usually , these users have to rely on screen readers : applications that sequentially read the content of a web page in audio . Unfortunately , screen readers are not able to detect the meaning of the different page objects , and thus the implicit semantic knowledge conveyed in the presentation of the page is lost . One approach described in literature to tackle this problem , is the Dante approach , which allows semantic annotation of web pages to provide screen readers with extra ( semantic ) knowledge to better facilitate the audio presentation of a web page . Until now , such annotations were done manually , and failed for dynamic pages . In this paper , we combine the Dante approach with a web design method , WSDM , to fully automate the generation of the semantic annotation for visually impaired users . To do so , the semantic knowledge gathered during the design process is exploited , and the annotations are generated as a by-product of the design process , requiring no extra effort from the designer . ',\n",
       " 254542: 'Learning domain ontologies for Web service descriptions : an experiment in bioinformatics The reasoning tasks that can be performed with semantic web service descriptions depend on the quality of the domain ontologies used to create these descriptions . However , building such domain ontologies is a time consuming and difficult task . We describe an automatic extraction method that learns domain ontologies for web service descriptions from textual documentations attached to web services . We conducted our experiments in the field of bioinformatics by learning an ontology from the documentation of the web services used in myGrid , a project that supports biology experiments on the Grid . Based on the evaluation of the extracted ontology in the context of the project , we conclude that the proposed extraction method is a helpful tool to support the process of building domain ontologies for web service descriptions . ',\n",
       " 254899: 'Securing web application code by static analysis and runtime protection Security remains a major roadblock to universal acceptance of the Web for many kinds of transactions , especially since the recent sharp increase in remotely exploitable vulnerabilities have been attributed to Web application bugs . Many verification tools are discovering previously unknown vulnerabilities in legacy C programs , raising hopes that the same success can be achieved with Web applications . In this paper , we describe a sound and holistic approach to ensuring Web application security . Viewing Web application vulnerabilities as a secure information flow problem , we created a lattice-based static analysis algorithm derived from type systems and typestate , and addressed its soundness . During the analysis , sections of code considered vulnerable are instrumented with runtime guards , thus securing Web applications in the absence of user intervention . With sufficient annotations , runtime overhead can be reduced to zero . We also created a tool named . WebSSARI ( Web application Security by Static Analysis and Runtime Inspection ) to test our algorithm , and used it to verify 230 open-source Web application projects on SourceForge.net , which were selected to represent projects of different maturity , popularity , and scale . 69 contained vulnerabilities . After notifying the developers , 38 acknowledged our findings and stated their plans to provide patches . Our statistics also show that static analysis reduced potential runtime overhead by 98.4 % . ',\n",
       " 259989: 'Staging transformations for multimodal web interaction management Multimodal interfaces are becoming increasingly ubiquitous with the advent of mobile devices , accessibility considerations , and novel software technologies that combine diverse interaction media . In addition to improving access and delivery capabilities , such interfaces enable flexible and personalized dialogs with websites , much like a conversation between humans . In this paper , we present a software framework for multimodal web interaction management that supports mixed-initiative dialogs between users and websites . A mixed-initiative dialog is one where the user and the website take turns changing the flow of interaction . The framework supports the functional specification and realization of such dialogs using staging transformations -- a theory for representing and reasoning about dialogs based on partial input . It supports multiple interaction interfaces , and offers sessioning , caching , and co-ordination functions through the use of an interaction manager . Two case studies are presented to illustrate the promise of this approach . ',\n",
       " 259993: 'Dynamic coordination of information management services for processing dynamic web content Dynamic Web content provides us with time-sensitive and continuously changing data . To glean up-to-date information , users need to regularly browse , collect and analyze this Web content . Without proper tool support this information management task is tedious , time-consuming and error prone , especially when the quantity of the dynamic Web content is large , when many information management services are needed to analyze it , and when underlying services\\\\/network are not completely reliable . This paper describes a multi-level , lifecycle ( design-time and run-time ) coordination mechanism that enables rapid , efficient development and execution of information management applications that are especially useful for processing dynamic Web content . Such a coordination mechanism brings dynamism to coordinating independent , distributed information management services . Dynamic parallelism spawns\\\\/merges multiple execution service branches based on available data , and dynamic run-time reconfiguration coordinates service execution to overcome faulty services and bottlenecks . These features enable information management applications to be more efficient in handling content and format changes in Web resources , and enable the applications to be evolved and adapted to process dynamic Web content . ',\n",
       " 275063: 'To randomize or not to randomize : space optimal summaries for hyperlink analysis Personalized PageRank expresses link-based page quality around user selected pages . The only previous personalized PageRank algorithm that can serve on-line queries for an unrestricted choice of pages on large graphs is our Monte Carlo algorithm ( WAW 2004 ) . In this paper we achieve unrestricted personalization by combining rounding and randomized sketching techniques in the dynamic programming algorithm of Jeh and Widom ( WWW 2003 ) . We evaluate the precision of approximation experimentally on large scale real-world data and find significant improvement over previous results . As a key theoretical contribution we show that our algorithms use an optimal amount of space by also improving earlier asymptotic worst-case lower bounds . Our lower bounds and algorithms apply to the SimRank as well ; of independent interest is the reduction of the SimRank computation to personalized PageRank . ',\n",
       " 278400: 'An XPath-based preference language for P3P The Platform for Privacy Preferences ( P3P ) is the most significant effort currently underway to enable web users to gain control over their private information . The designers of P3P simultaneously designed a preference language called APPEL to allow users to express their privacy preferences , thus enabling automatic matching of privacy preferences against P3P policies . Unfortunately subtle interactions between P3P and APPEL result in serious problems when using APPEL : Users can only directly specify what is unacceptable in a policy , not what is acceptable ; simple preferences are hard to express ; and writing APPEL preferences is error prone . We show that these problems follow from a fundamental design choice made by APPEL , and can not be solved without completely redesigning the language . Therefore we explore alternatives to APPEL that can overcome these problems . In particular , we show that XPath serves quite nicely as a preference language and solves all the above problems . We identify the minimal subset of XPath that is needed , thus allowing matching programs to potentially use a smaller memory footprint . We also give an APPEL to XPath translator that shows that XPath is as expressive as APPEL . ',\n",
       " 281466: 'XJ : facilitating XML processing in Java The increased importance of XML as a data representation format has led to several proposals for facilitating the development of applications that operate on XML data . These proposals range from runtime API-based interfaces to XML-based programming languages . The subject of this paper is XJ , a research language that proposes novel mechanisms for the integration of XML as a first-class construct into Java ™ . The design goals of XJ distinguish it from past work on integrating XML support into programming languages -- specifically , the XJ design adheres to the XML Schema and XPath standards . Moreover , it supports in-place updates of XML data thereby keeping with the imperative nature of Java . We have built a prototype compiler for XJ , and our preliminary experiments demonstrate that the performance of XJ programs can approach that of traditional low-level API-based interfaces , while providing a higher level of abstraction . ',\n",
       " 281471: 'An adaptive , fast , and safe XML parser based on byte sequences memorization XML ( Extensible Markup Language ) processing can incur significant runtime overhead in XML-based infrastructural middleware such as Web service application servers . This paper proposes a novel mechanism for efficiently processing similar XML documents . Given a new XML document as a byte sequence , the XML parser proposed in this paper normally avoids syntactic analysis but simply matches the document with previously processed ones , reusing those results . Our parser is adaptive since it partially parses and then remembers XML document fragments that it has not met before . Moreover , it processes safely since its partial parsing correctly checks the well-formedness of documents . Our implementation of the proposed parser complies with the JSR 63 standard of the Java API for XML Processing ( JAXP ) 1.1 specification . We evaluated Deltarser performance with messages using Google Web services . Comparing to Piccolo ( and Apache Xerces ) , it effectively parses 35 % ( 106 % ) faster in a server-side use-case scenario , and 73 % ( 126 % ) faster in a client-side use-case scenario . ',\n",
       " 285729: \"Topic-sensitive PageRank In the original PageRank algorithm for improving the ranking of search-query results , a single PageRank vector is computed , using the link structure of the Web , to capture the relative `` importance '' of Web pages , independent of any particular search query . To yield more accurate search results , we propose computing a set of PageRank vectors , biased using a set of representative topics , to capture more accurately the notion of importance with respect to a particular topic . By using these ( precomputed ) biased PageRank vectors to generate query-specific importance scores for pages at query time , we show that we can generate more accurate rankings than with a single , generic PageRank vector . For ordinary keyword search queries , we compute the topic-sensitive PageRank scores for pages satisfying the query using the topic of the query keywords . For searches done in context ( e.g. , when the search query is performed by highlighting words in a Web page ) , we compute the topic-sensitive PageRank scores using the topic of the context in which the query appeared . \"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-balloon",
   "metadata": {},
   "source": [
    "### Extracting the keywords from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "invisible-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "grateful-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : 10023569.txt\n",
      "\n",
      "Processing : 10030413.txt\n",
      "\n",
      "Processing : 10048650.txt\n",
      "\n",
      "Processing : 10057110.txt\n",
      "\n",
      "Processing : 10057112.txt\n",
      "\n",
      "Processing : 100585.txt\n",
      "\n",
      "Processing : 100717.txt\n",
      "\n",
      "Processing : 10071923.txt\n",
      "\n",
      "Processing : 10074391.txt\n",
      "\n",
      "Processing : 1008874.txt\n",
      "\n",
      "Processing : 10119.txt\n",
      "\n",
      "Processing : 101488.txt\n",
      "\n",
      "Processing : 10153001.txt\n",
      "\n",
      "Processing : 10172411.txt\n",
      "\n",
      "Processing : 10178588.txt\n",
      "\n",
      "Processing : 1021018.txt\n",
      "\n",
      "Processing : 10210475.txt\n",
      "\n",
      "Processing : 10239456.txt\n",
      "\n",
      "Processing : 1025710.txt\n",
      "\n",
      "Processing : 10260371.txt\n",
      "\n",
      "Processing : 102655.txt\n",
      "\n",
      "Processing : 10267504.txt\n",
      "\n",
      "Processing : 10272014.txt\n",
      "\n",
      "Processing : 10274995.txt\n",
      "\n",
      "Processing : 10278781.txt\n",
      "\n",
      "Processing : 1029161.txt\n",
      "\n",
      "Processing : 10323033.txt\n",
      "\n",
      "Processing : 1033807.txt\n",
      "\n",
      "Processing : 10348116.txt\n",
      "\n",
      "Processing : 10358404.txt\n",
      "\n",
      "Processing : 10365987.txt\n",
      "\n",
      "Processing : 10377441.txt\n",
      "\n",
      "Processing : 10378963.txt\n",
      "\n",
      "Processing : 10389546.txt\n",
      "\n",
      "Processing : 10414735.txt\n",
      "\n",
      "Processing : 10460295.txt\n",
      "\n",
      "Processing : 10460575.txt\n",
      "\n",
      "Processing : 10460577.txt\n",
      "\n",
      "Processing : 10461284.txt\n",
      "\n",
      "Processing : 10464296.txt\n",
      "\n",
      "Processing : 10477594.txt\n",
      "\n",
      "Processing : 104778.txt\n",
      "\n",
      "Processing : 1050874.txt\n",
      "\n",
      "Processing : 10514579.txt\n",
      "\n",
      "Processing : 10524156.txt\n",
      "\n",
      "Processing : 10525045.txt\n",
      "\n",
      "Processing : 10525233.txt\n",
      "\n",
      "Processing : 10563630.txt\n",
      "\n",
      "Processing : 10565631.txt\n",
      "\n",
      "Processing : 105768.txt\n",
      "\n",
      "Processing : 10581378.txt\n",
      "\n",
      "Processing : 10582583.txt\n",
      "\n",
      "Processing : 10582584.txt\n",
      "\n",
      "Processing : 10589971.txt\n",
      "\n",
      "Processing : 1059530.txt\n",
      "\n",
      "Processing : 10599039.txt\n",
      "\n",
      "Processing : 10603174.txt\n",
      "\n",
      "Processing : 10606830.txt\n",
      "\n",
      "Processing : 10631238.txt\n",
      "\n",
      "Processing : 10631834.txt\n",
      "\n",
      "Processing : 10633510.txt\n",
      "\n",
      "Processing : 10633621.txt\n",
      "\n",
      "Processing : 10643874.txt\n",
      "\n",
      "Processing : 10659466.txt\n",
      "\n",
      "Processing : 10754076.txt\n",
      "\n",
      "Processing : 1077174.txt\n",
      "\n",
      "Processing : 10772351.txt\n",
      "\n",
      "Processing : 10777275.txt\n",
      "\n",
      "Processing : 10795381.txt\n",
      "\n",
      "Processing : 10814222.txt\n",
      "\n",
      "Processing : 10844303.txt\n",
      "\n",
      "Processing : 1084549.txt\n",
      "\n",
      "Processing : 10901561.txt\n",
      "\n",
      "Processing : 10924972.txt\n",
      "\n",
      "Processing : 10946910.txt\n",
      "\n",
      "Processing : 10950133.txt\n",
      "\n",
      "Processing : 11006514.txt\n",
      "\n",
      "Processing : 11015761.txt\n",
      "\n",
      "Processing : 11026417.txt\n",
      "\n",
      "Processing : 11034828.txt\n",
      "\n",
      "Processing : 11052867.txt\n",
      "\n",
      "Processing : 1107868.txt\n",
      "\n",
      "Processing : 11081679.txt\n",
      "\n",
      "Processing : 11108002.txt\n",
      "\n",
      "Processing : 11134936.txt\n",
      "\n",
      "Processing : 11134938.txt\n",
      "\n",
      "Processing : 11204787.txt\n",
      "\n",
      "Processing : 11212270.txt\n",
      "\n",
      "Processing : 11227602.txt\n",
      "\n",
      "Processing : 11275612.txt\n",
      "\n",
      "Processing : 11275613.txt\n",
      "\n",
      "Processing : 11277694.txt\n",
      "\n",
      "Processing : 11338040.txt\n",
      "\n",
      "Processing : 11338703.txt\n",
      "\n",
      "Processing : 11394824.txt\n",
      "\n",
      "Processing : 11403481.txt\n",
      "\n",
      "Processing : 11418806.txt\n",
      "\n",
      "Processing : 1142421.txt\n",
      "\n",
      "Processing : 11485543.txt\n",
      "\n",
      "Processing : 11498123.txt\n",
      "\n",
      "Processing : 1150148.txt\n",
      "\n",
      "Processing : 11556066.txt\n",
      "\n",
      "Processing : 11570239.txt\n",
      "\n",
      "Processing : 11616857.txt\n",
      "\n",
      "Processing : 11619605.txt\n",
      "\n",
      "Processing : 11620085.txt\n",
      "\n",
      "Processing : 11624616.txt\n",
      "\n",
      "Processing : 11628441.txt\n",
      "\n",
      "Processing : 11631340.txt\n",
      "\n",
      "Processing : 11631791.txt\n",
      "\n",
      "Processing : 11633625.txt\n",
      "\n",
      "Processing : 11638887.txt\n",
      "\n",
      "Processing : 11639816.txt\n",
      "\n",
      "Processing : 11643476.txt\n",
      "\n",
      "Processing : 11657105.txt\n",
      "\n",
      "Processing : 11659004.txt\n",
      "\n",
      "Processing : 1167068.txt\n",
      "\n",
      "Processing : 1168781.txt\n",
      "\n",
      "Processing : 11701167.txt\n",
      "\n",
      "Processing : 11703421.txt\n",
      "\n",
      "Processing : 11708130.txt\n",
      "\n",
      "Processing : 11714006.txt\n",
      "\n",
      "Processing : 11749957.txt\n",
      "\n",
      "Processing : 11756683.txt\n",
      "\n",
      "Processing : 11759742.txt\n",
      "\n",
      "Processing : 11782911.txt\n",
      "\n",
      "Processing : 11785.txt\n",
      "\n",
      "Processing : 11788753.txt\n",
      "\n",
      "Processing : 11792499.txt\n",
      "\n",
      "Processing : 11796084.txt\n",
      "\n",
      "Processing : 11798933.txt\n",
      "\n",
      "Processing : 11806298.txt\n",
      "\n",
      "Processing : 11807460.txt\n",
      "\n",
      "Processing : 11811654.txt\n",
      "\n",
      "Processing : 11820735.txt\n",
      "\n",
      "Processing : 1182832.txt\n",
      "\n",
      "Processing : 11835370.txt\n",
      "\n",
      "Processing : 11836329.txt\n",
      "\n",
      "Processing : 11839803.txt\n",
      "\n",
      "Processing : 11840142.txt\n",
      "\n",
      "Processing : 11840967.txt\n",
      "\n",
      "Processing : 11841922.txt\n",
      "\n",
      "Processing : 11848994.txt\n",
      "\n",
      "Processing : 11850677.txt\n",
      "\n",
      "Processing : 11858468.txt\n",
      "\n",
      "Processing : 11859214.txt\n",
      "\n",
      "Processing : 11867634.txt\n",
      "\n",
      "Processing : 11876537.txt\n",
      "\n",
      "Processing : 11878695.txt\n",
      "\n",
      "Processing : 11881756.txt\n",
      "\n",
      "Processing : 11931844.txt\n",
      "\n",
      "Processing : 1194360.txt\n",
      "\n",
      "Processing : 1194809.txt\n",
      "\n",
      "Processing : 11981850.txt\n",
      "\n",
      "Processing : 12003689.txt\n",
      "\n",
      "Processing : 12008191.txt\n",
      "\n",
      "Processing : 12092784.txt\n",
      "\n",
      "Processing : 12102.txt\n",
      "\n",
      "Processing : 12127726.txt\n",
      "\n",
      "Processing : 12191750.txt\n",
      "\n",
      "Processing : 1225527.txt\n",
      "\n",
      "Processing : 1227358.txt\n",
      "\n",
      "Processing : 1229683.txt\n",
      "\n",
      "Processing : 1229684.txt\n",
      "\n",
      "Processing : 1247804.txt\n",
      "\n",
      "Processing : 1247888.txt\n",
      "\n",
      "Processing : 125048.txt\n",
      "\n",
      "Processing : 1267363.txt\n",
      "\n",
      "Processing : 1275176.txt\n",
      "\n",
      "Processing : 12995751.txt\n",
      "\n",
      "Processing : 13005831.txt\n",
      "\n",
      "Processing : 13009292.txt\n",
      "\n",
      "Processing : 13023730.txt\n",
      "\n",
      "Processing : 13058954.txt\n",
      "\n",
      "Processing : 13058990.txt\n",
      "\n",
      "Processing : 13059029.txt\n",
      "\n",
      "Processing : 13059042.txt\n",
      "\n",
      "Processing : 13059044.txt\n",
      "\n",
      "Processing : 13059085.txt\n",
      "\n",
      "Processing : 13059173.txt\n",
      "\n",
      "Processing : 13059270.txt\n",
      "\n",
      "Processing : 13059318.txt\n",
      "\n",
      "Processing : 13059382.txt\n",
      "\n",
      "Processing : 13059388.txt\n",
      "\n",
      "Processing : 13059401.txt\n",
      "\n",
      "Processing : 13059585.txt\n",
      "\n",
      "Processing : 13059663.txt\n",
      "\n",
      "Processing : 13059781.txt\n",
      "\n",
      "Processing : 13059898.txt\n",
      "\n",
      "Processing : 13059932.txt\n",
      "\n",
      "Processing : 13060051.txt\n",
      "\n",
      "Processing : 13060083.txt\n",
      "\n",
      "Processing : 13060159.txt\n",
      "\n",
      "Processing : 13060226.txt\n",
      "\n",
      "Processing : 13060384.txt\n",
      "\n",
      "Processing : 13060390.txt\n",
      "\n",
      "Processing : 13060588.txt\n",
      "\n",
      "Processing : 13060608.txt\n",
      "\n",
      "Processing : 13060664.txt\n",
      "\n",
      "Processing : 13060669.txt\n",
      "\n",
      "Processing : 13060679.txt\n",
      "\n",
      "Processing : 13060796.txt\n",
      "\n",
      "Processing : 13060825.txt\n",
      "\n",
      "Processing : 13060918.txt\n",
      "\n",
      "Processing : 13061041.txt\n",
      "\n",
      "Processing : 13061054.txt\n",
      "\n",
      "Processing : 13061186.txt\n",
      "\n",
      "Processing : 13061206.txt\n",
      "\n",
      "Processing : 13061384.txt\n",
      "\n",
      "Processing : 13061550.txt\n",
      "\n",
      "Processing : 13061552.txt\n",
      "\n",
      "Processing : 13061595.txt\n",
      "\n",
      "Processing : 13061601.txt\n",
      "\n",
      "Processing : 13061675.txt\n",
      "\n",
      "Processing : 13061698.txt\n",
      "\n",
      "Processing : 13061707.txt\n",
      "\n",
      "Processing : 13061722.txt\n",
      "\n",
      "Processing : 13061737.txt\n",
      "\n",
      "Processing : 13061804.txt\n",
      "\n",
      "Processing : 13061836.txt\n",
      "\n",
      "Processing : 13061839.txt\n",
      "\n",
      "Processing : 13061918.txt\n",
      "\n",
      "Processing : 13061924.txt\n",
      "\n",
      "Processing : 13061972.txt\n",
      "\n",
      "Processing : 13061989.txt\n",
      "\n",
      "Processing : 13062025.txt\n",
      "\n",
      "Processing : 13062026.txt\n",
      "\n",
      "Processing : 13062112.txt\n",
      "\n",
      "Processing : 13062121.txt\n",
      "\n",
      "Processing : 13062191.txt\n",
      "\n",
      "Processing : 13062261.txt\n",
      "\n",
      "Processing : 13062305.txt\n",
      "\n",
      "Processing : 13062337.txt\n",
      "\n",
      "Processing : 13062396.txt\n",
      "\n",
      "Processing : 13062406.txt\n",
      "\n",
      "Processing : 13062409.txt\n",
      "\n",
      "Processing : 13062436.txt\n",
      "\n",
      "Processing : 13062801.txt\n",
      "\n",
      "Processing : 13062845.txt\n",
      "\n",
      "Processing : 13062852.txt\n",
      "\n",
      "Processing : 13062911.txt\n",
      "\n",
      "Processing : 13062962.txt\n",
      "\n",
      "Processing : 13063216.txt\n",
      "\n",
      "Processing : 13063239.txt\n",
      "\n",
      "Processing : 13063418.txt\n",
      "\n",
      "Processing : 13063481.txt\n",
      "\n",
      "Processing : 13063484.txt\n",
      "\n",
      "Processing : 13063564.txt\n",
      "\n",
      "Processing : 13063596.txt\n",
      "\n",
      "Processing : 13063612.txt\n",
      "\n",
      "Processing : 13063679.txt\n",
      "\n",
      "Processing : 13063697.txt\n",
      "\n",
      "Processing : 13063699.txt\n",
      "\n",
      "Processing : 13063758.txt\n",
      "\n",
      "Processing : 13063830.txt\n",
      "\n",
      "Processing : 13063949.txt\n",
      "\n",
      "Processing : 13063954.txt\n",
      "\n",
      "Processing : 13064078.txt\n",
      "\n",
      "Processing : 13064160.txt\n",
      "\n",
      "Processing : 13064222.txt\n",
      "\n",
      "Processing : 13064271.txt\n",
      "\n",
      "Processing : 13064294.txt\n",
      "\n",
      "Processing : 13064296.txt\n",
      "\n",
      "Processing : 13064323.txt\n",
      "\n",
      "Processing : 13064431.txt\n",
      "\n",
      "Processing : 13064442.txt\n",
      "\n",
      "Processing : 13064535.txt\n",
      "\n",
      "Processing : 13064551.txt\n",
      "\n",
      "Processing : 13064602.txt\n",
      "\n",
      "Processing : 13064675.txt\n",
      "\n",
      "Processing : 13064722.txt\n",
      "\n",
      "Processing : 13064730.txt\n",
      "\n",
      "Processing : 13087531.txt\n",
      "\n",
      "Processing : 13089030.txt\n",
      "\n",
      "Processing : 13106832.txt\n",
      "\n",
      "Processing : 13109.txt\n",
      "\n",
      "Processing : 13152279.txt\n",
      "\n",
      "Processing : 13185533.txt\n",
      "\n",
      "Processing : 13190545.txt\n",
      "\n",
      "Processing : 13193305.txt\n",
      "\n",
      "Processing : 13240315.txt\n",
      "\n",
      "Processing : 13251515.txt\n",
      "\n",
      "Processing : 13386371.txt\n",
      "\n",
      "Processing : 13395437.txt\n",
      "\n",
      "Processing : 13403801.txt\n",
      "\n",
      "Processing : 13501869.txt\n",
      "\n",
      "Processing : 13502389.txt\n",
      "\n",
      "Processing : 13502390.txt\n",
      "\n",
      "Processing : 13502392.txt\n",
      "\n",
      "Processing : 13502500.txt\n",
      "\n",
      "Processing : 13502857.txt\n",
      "\n",
      "Processing : 13502912.txt\n",
      "\n",
      "Processing : 13503261.txt\n",
      "\n",
      "Processing : 13504156.txt\n",
      "\n",
      "Processing : 13505677.txt\n",
      "\n",
      "Processing : 13506151.txt\n",
      "\n",
      "Processing : 13507253.txt\n",
      "\n",
      "Processing : 13507543.txt\n",
      "\n",
      "Processing : 13507714.txt\n",
      "\n",
      "Processing : 13509697.txt\n",
      "\n",
      "Processing : 13510800.txt\n",
      "\n",
      "Processing : 13512950.txt\n",
      "\n",
      "Processing : 13513557.txt\n",
      "\n",
      "Processing : 13514543.txt\n",
      "\n",
      "Processing : 13514784.txt\n",
      "\n",
      "Processing : 13515810.txt\n",
      "\n",
      "Processing : 13516419.txt\n",
      "\n",
      "Processing : 13517074.txt\n",
      "\n",
      "Processing : 13517135.txt\n",
      "\n",
      "Processing : 13518145.txt\n",
      "\n",
      "Processing : 13518419.txt\n",
      "\n",
      "Processing : 13518634.txt\n",
      "\n",
      "Processing : 13518670.txt\n",
      "\n",
      "Processing : 13519640.txt\n",
      "\n",
      "Processing : 13520597.txt\n",
      "\n",
      "Processing : 13520879.txt\n",
      "\n",
      "Processing : 13521214.txt\n",
      "\n",
      "Processing : 13521570.txt\n",
      "\n",
      "Processing : 13522059.txt\n",
      "\n",
      "Processing : 13522731.txt\n",
      "\n",
      "Processing : 13523265.txt\n",
      "\n",
      "Processing : 13525236.txt\n",
      "\n",
      "Processing : 13526048.txt\n",
      "\n",
      "Processing : 13526508.txt\n",
      "\n",
      "Processing : 13526671.txt\n",
      "\n",
      "Processing : 13526814.txt\n",
      "\n",
      "Processing : 13526913.txt\n",
      "\n",
      "Processing : 13527964.txt\n",
      "\n",
      "Processing : 13528329.txt\n",
      "\n",
      "Processing : 13528523.txt\n",
      "\n",
      "Processing : 13528887.txt\n",
      "\n",
      "Processing : 13529859.txt\n",
      "\n",
      "Processing : 13530223.txt\n",
      "\n",
      "Processing : 13530633.txt\n",
      "\n",
      "Processing : 13530974.txt\n",
      "\n",
      "Processing : 13531284.txt\n",
      "\n",
      "Processing : 13532199.txt\n",
      "\n",
      "Processing : 13533362.txt\n",
      "\n",
      "Processing : 13533459.txt\n",
      "\n",
      "Processing : 13533717.txt\n",
      "\n",
      "Processing : 13533950.txt\n",
      "\n",
      "Processing : 13534577.txt\n",
      "\n",
      "Processing : 13534613.txt\n",
      "\n",
      "Processing : 13535450.txt\n",
      "\n",
      "Processing : 13536580.txt\n",
      "\n",
      "Processing : 13536686.txt\n",
      "\n",
      "Processing : 13536936.txt\n",
      "\n",
      "Processing : 13537427.txt\n",
      "\n",
      "Processing : 13537435.txt\n",
      "\n",
      "Processing : 13538141.txt\n",
      "\n",
      "Processing : 13538176.txt\n",
      "\n",
      "Processing : 13538488.txt\n",
      "\n",
      "Processing : 13539372.txt\n",
      "\n",
      "Processing : 13540032.txt\n",
      "\n",
      "Processing : 13540651.txt\n",
      "\n",
      "Processing : 13542825.txt\n",
      "\n",
      "Processing : 13543051.txt\n",
      "\n",
      "Processing : 13543053.txt\n",
      "\n",
      "Processing : 13543554.txt\n",
      "\n",
      "Processing : 13543822.txt\n",
      "\n",
      "Processing : 13544213.txt\n",
      "\n",
      "Processing : 13544630.txt\n",
      "\n",
      "Processing : 13545447.txt\n",
      "\n",
      "Processing : 13601457.txt\n",
      "\n",
      "Processing : 13621384.txt\n",
      "\n",
      "Processing : 13638975.txt\n",
      "\n",
      "Processing : 13714941.txt\n",
      "\n",
      "Processing : 13753027.txt\n",
      "\n",
      "Processing : 13756266.txt\n",
      "\n",
      "Processing : 13778833.txt\n",
      "\n",
      "Processing : 13785303.txt\n",
      "\n",
      "Processing : 13788626.txt\n",
      "\n",
      "Processing : 13788838.txt\n",
      "\n",
      "Processing : 13801324.txt\n",
      "\n",
      "Processing : 13819642.txt\n",
      "\n",
      "Processing : 13820155.txt\n",
      "\n",
      "Processing : 13833456.txt\n",
      "\n",
      "Processing : 13875907.txt\n",
      "\n",
      "Processing : 13896241.txt\n",
      "\n",
      "Processing : 13904998.txt\n",
      "\n",
      "Processing : 13982483.txt\n",
      "\n",
      "Processing : 14005107.txt\n",
      "\n",
      "Processing : 14005258.txt\n",
      "\n",
      "Processing : 14007948.txt\n",
      "\n",
      "Processing : 14026193.txt\n",
      "\n",
      "Processing : 14027370.txt\n",
      "\n",
      "Processing : 14045611.txt\n",
      "\n",
      "Processing : 14122983.txt\n",
      "\n",
      "Processing : 14123171.txt\n",
      "\n",
      "Processing : 14128558.txt\n",
      "\n",
      "Processing : 14146914.txt\n",
      "\n",
      "Processing : 14149355.txt\n",
      "\n",
      "Processing : 14169498.txt\n",
      "\n",
      "Processing : 14173486.txt\n",
      "\n",
      "Processing : 14185350.txt\n",
      "\n",
      "Processing : 14186402.txt\n",
      "\n",
      "Processing : 14200731.txt\n",
      "\n",
      "Processing : 14209920.txt\n",
      "\n",
      "Processing : 14237951.txt\n",
      "\n",
      "Processing : 14249.txt\n",
      "\n",
      "Processing : 14255423.txt\n",
      "\n",
      "Processing : 14265822.txt\n",
      "\n",
      "Processing : 14316716.txt\n",
      "\n",
      "Processing : 14316718.txt\n",
      "\n",
      "Processing : 14322308.txt\n",
      "\n",
      "Processing : 14347434.txt\n",
      "\n",
      "Processing : 14348154.txt\n",
      "\n",
      "Processing : 14353464.txt\n",
      "\n",
      "Processing : 14371347.txt\n",
      "\n",
      "Processing : 14372945.txt\n",
      "\n",
      "Processing : 14373644.txt\n",
      "\n",
      "Processing : 14373837.txt\n",
      "\n",
      "Processing : 14374135.txt\n",
      "\n",
      "Processing : 14375309.txt\n",
      "\n",
      "Processing : 14375496.txt\n",
      "\n",
      "Processing : 14375712.txt\n",
      "\n",
      "Processing : 14375718.txt\n",
      "\n",
      "Processing : 14376091.txt\n",
      "\n",
      "Processing : 14376543.txt\n",
      "\n",
      "Processing : 14377120.txt\n",
      "\n",
      "Processing : 14377272.txt\n",
      "\n",
      "Processing : 14377562.txt\n",
      "\n",
      "Processing : 14380234.txt\n",
      "\n",
      "Processing : 14380564.txt\n",
      "\n",
      "Processing : 14382235.txt\n",
      "\n",
      "Processing : 14383928.txt\n",
      "\n",
      "Processing : 14385094.txt\n",
      "\n",
      "Processing : 14386517.txt\n",
      "\n",
      "Processing : 14386992.txt\n",
      "\n",
      "Processing : 14387519.txt\n",
      "\n",
      "Processing : 14387989.txt\n",
      "\n",
      "Processing : 14388642.txt\n",
      "\n",
      "Processing : 14388703.txt\n",
      "\n",
      "Processing : 14388913.txt\n",
      "\n",
      "Processing : 14388926.txt\n",
      "\n",
      "Processing : 14389008.txt\n",
      "\n",
      "Processing : 14389159.txt\n",
      "\n",
      "Processing : 14389185.txt\n",
      "\n",
      "Processing : 14389220.txt\n",
      "\n",
      "Processing : 14389382.txt\n",
      "\n",
      "Processing : 14392043.txt\n",
      "\n",
      "Processing : 14392151.txt\n",
      "\n",
      "Processing : 14394291.txt\n",
      "\n",
      "Processing : 14394643.txt\n",
      "\n",
      "Processing : 14395405.txt\n",
      "\n",
      "Processing : 14395695.txt\n",
      "\n",
      "Processing : 14395704.txt\n",
      "\n",
      "Processing : 14396486.txt\n",
      "\n",
      "Processing : 14397937.txt\n",
      "\n",
      "Processing : 14426843.txt\n",
      "\n",
      "Processing : 14435710.txt\n",
      "\n",
      "Processing : 14445422.txt\n",
      "\n",
      "Processing : 14445546.txt\n",
      "\n",
      "Processing : 14449751.txt\n",
      "\n",
      "Processing : 14453157.txt\n",
      "\n",
      "Processing : 14453752.txt\n",
      "\n",
      "Processing : 14454508.txt\n",
      "\n",
      "Processing : 14475693.txt\n",
      "\n",
      "Processing : 1454933.txt\n",
      "\n",
      "Processing : 1605069.txt\n",
      "\n",
      "Processing : 163229.txt\n",
      "\n",
      "Processing : 16334.txt\n",
      "\n",
      "Processing : 16381.txt\n",
      "\n",
      "Processing : 16387.txt\n",
      "\n",
      "Processing : 1659738.txt\n",
      "\n",
      "Processing : 16706.txt\n",
      "\n",
      "Processing : 173979.txt\n",
      "\n",
      "Processing : 178204.txt\n",
      "\n",
      "Processing : 178963.txt\n",
      "\n",
      "Processing : 1810492.txt\n",
      "\n",
      "Processing : 183.txt\n",
      "\n",
      "Processing : 1833006.txt\n",
      "\n",
      "Processing : 1833066.txt\n",
      "\n",
      "Processing : 1839820.txt\n",
      "\n",
      "Processing : 1840610.txt\n",
      "\n",
      "Processing : 1845167.txt\n",
      "\n",
      "Processing : 1848435.txt\n",
      "\n",
      "Processing : 1880971.txt\n",
      "\n",
      "Processing : 1888132.txt\n",
      "\n",
      "Processing : 204579.txt\n",
      "\n",
      "Processing : 204586.txt\n",
      "\n",
      "Processing : 21580.txt\n",
      "\n",
      "Processing : 21587.txt\n",
      "\n",
      "Processing : 217637.txt\n",
      "\n",
      "Processing : 230498.txt\n",
      "\n",
      "Processing : 238458.txt\n",
      "\n",
      "Processing : 239996.txt\n",
      "\n",
      "Processing : 246813.txt\n",
      "\n",
      "Processing : 247234.txt\n",
      "\n",
      "Processing : 251906.txt\n",
      "\n",
      "Processing : 251909.txt\n",
      "\n",
      "Processing : 254542.txt\n",
      "\n",
      "Processing : 254899.txt\n",
      "\n",
      "Processing : 259989.txt\n",
      "\n",
      "Processing : 259993.txt\n",
      "\n",
      "Processing : 275063.txt\n",
      "\n",
      "Processing : 278400.txt\n",
      "\n",
      "Processing : 281466.txt\n",
      "\n",
      "Processing : 281471.txt\n",
      "\n",
      "Processing : 285729.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in key_files:\n",
    "    print(f'Processing : {file}\\n')\n",
    "    temp = []\n",
    "    f = open(path+'/keys/'+file)\n",
    "    for line in f.readlines():\n",
    "        temp.append(line.strip())\n",
    "    file = int(file.split('.')[0])\n",
    "    keywords_data[file] = \",\".join(temp)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lesbian-officer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10023569: 'multi-service application,resource provisioning',\n",
       " 10030413: 'association rule mining,collaborative filtering,latent topic models,recommender systems',\n",
       " 10048650: 'distributed systems,reputation,security and protection,service-oriented architectures,trust management',\n",
       " 10057110: 'patterns,rich internet applications',\n",
       " 10057112: 'component model,graphical user interfaces,interaction styles,spreadsheet-based mashup patterns,spreadsheets,web data mashups',\n",
       " 100585: 'blogs,graph theory,information propagation,learning,memes,topic characterization,topic structure,viral propagation,viruses',\n",
       " 100717: 'b2b,business-to-business,change response,content analysis and indexing,continual query,information quality,semantic web',\n",
       " 10071923: 'optimization,qos,service composition,web services',\n",
       " 10074391: 'active learning,keyword generation,on-line information services,sponsored search',\n",
       " 1008874: 'distributed systems,information search and retrieval,peer collaborative search,small world,topical crawlers',\n",
       " 10119: 'automated reasoning,daml,distributed systems,ontologies,representations,semantic web,web service composition,web services',\n",
       " 101488: 'interoperability,learning object repositories',\n",
       " 10153001: 'recommender systems,social media,social recommendation',\n",
       " 10172411: 'content match,contextual advertising,impression forecasting,information search and retrieval,online advertising,wand',\n",
       " 10178588: 'high dimensional,information search and retrieval,latent,missing value,text mining,transfer learning',\n",
       " 1021018: 'behavioral variability,web search',\n",
       " 10210475: 'application servers,dynamic placement,startup performance,web hosting',\n",
       " 10239456: 'dht,ir,p2p,query-driven indexing,text retrieval',\n",
       " 1025710: 'browsing,clustering,document preparation,hypermedia generation,hypertext/hypermedia,knowledge representation formalisms and methods,media archives,multimedia information systems,rdf,search,semantic web',\n",
       " 10260371: 'web caching',\n",
       " 102655: 'assembly,computer-managed instruction,content management,data retrieval,information retrieval,instruction,learning object,linking,lom,metadata,organization,rdf,semantic web',\n",
       " 10267504: 'learning preferences,preference judgments',\n",
       " 10272014: 'information extraction,markov logic networks,miscellaneous,site-level knowledge,structured data,web forums',\n",
       " 10274995: 'context-aware search,variable length hidden markov model',\n",
       " 10278781: 'aspect,browser,coding tools and techniques,ecmascript,javascript,membrane,reference monitor,remote object,same origin policy,security and protection,web security,wrapper',\n",
       " 1029161: 'number-theoretic computations',\n",
       " 10323033: 'abstraction,refinement,service composition',\n",
       " 1033807: 'access control policies,fault model,mutation testing,test generation,testing tools',\n",
       " 10348116: 'xml query,xml schema evolution',\n",
       " 10358404: 'control,news personalization,open user model,trust,user profile',\n",
       " 10365987: 'adaptive user interfaces,tracing,tree edit distance,tree pruning,web blocks',\n",
       " 10377441: 'load balancing,multi-attribute,p2p,range queries',\n",
       " 10378963: 'named entity,ontology,query logs',\n",
       " 10389546: 'cache,client-side,data base,database,general,web',\n",
       " 10414735: 'enterprise search,faceted search,information search and retrieval,social search',\n",
       " 10460295: 'service adaptation,service composition',\n",
       " 10460575: 'flickr,geo-tags,mechanical turk,media applications,orienteering problem,rich media,travel itinerary',\n",
       " 10460577: 'flickr,geographical gazetteer,georeferencing,image mining,text mining,tourist sites,visit times',\n",
       " 10461284: 'expected weighted hoeffding distance,ranking,search algorithm dissimilarity',\n",
       " 10464296: 'delay-tolerant networking,developing regions,general,wiki',\n",
       " 10477594: 'advertising,approximation algorithms,auctions,externalities,general',\n",
       " 104778: \"change in pagerank,information search and retrieval,miscellaneous,pagerank,random surfer model,search engine's impact,web evolution\",\n",
       " 1050874: 'decision tree,information retrieval,layout analysis,machine learning,support vector machine,table detection',\n",
       " 10514579: 'active learning,classification,statistical language models,vandalism,wikipedia',\n",
       " 10524156: 'data api network,mashup navigation,miscellaneous,semantic,social',\n",
       " 10525045: 'end-user programming,mashup,nested table,reusable software,spreadsheet',\n",
       " 10525233: 'applications,http,mutual authentication,web systems',\n",
       " 10563630: 'earthquake,event detection,location estimation,social sensor,twitter',\n",
       " 10565631: 'distributed,peer-to-peer,reasoning,self-organisation',\n",
       " 105768: 'generation of web content,level of detail,metadata,video stream,web browser from video streams and their metadat',\n",
       " 10581378: 'community structure,conductance,flow-based methods,graph partitioning,spectral methods',\n",
       " 10582583: 'credibility,dispute,web',\n",
       " 10582584: 'annotation,argumentation,cscw,decision support,miscellaneous,sensemaking,web',\n",
       " 10589971: 'on-line information services,social networks,trust',\n",
       " 1059530: 'assistment,computer-assisted instruction,intelligent tutoring system,learning,mcas,predict',\n",
       " 10599039: 'game theory,privacy,social networks',\n",
       " 10603174: 'online social networks,parallelization,software architectures,web crawler,web spider',\n",
       " 10606830: 'active learning,social network privacy,usability',\n",
       " 10631238: 'anomaly detection,drive-by-download attacks,security and protection,web client exploits',\n",
       " 10631834: 'plsa,question answering,question recommendation',\n",
       " 10633510: 'document summarization,ranking,tag',\n",
       " 10633621: 'data extraction,miscellaneous',\n",
       " 10643874: 'adversarial interface design,design principles,evil interfaces,malicious interfaces,miscellaneous,web usability guidelines',\n",
       " 10659466: 'miscellaneous,ontologies,scalable sense integration,semantic web',\n",
       " 10754076: \"bibliometrics,context,gleason's theorem,recommender systems\",\n",
       " 1077174: 'network architecture and design',\n",
       " 10772351: 'cosine similarity,emotional valence detection,machine-generated content',\n",
       " 10777275: 'media fragments,video accessibility,video url',\n",
       " 10795381: 'data exploration,faceted search,information search and retrieval,wikipedia',\n",
       " 10814222: 'dynamic networks,re-publication',\n",
       " 10844303: 'query refinements,random walks',\n",
       " 1084549: 'patching,peer-to-peer networks,performance evaluation,video on-demand service',\n",
       " 10901561: 'aggregated search,rdfa,semantic web,web of data',\n",
       " 10924972: 'basket recommendation,markov chain,matrix factorization',\n",
       " 10946910: 'information search and retrieval,language models,search query processing,very large-scale experiments,web n-gram',\n",
       " 10950133: 'browser,cross-site scripting,filter,unauthorized access,web,xss',\n",
       " 11006514: 'documentid reassignment,index compression',\n",
       " 11015761: 'document overlap,miscellaneous,shingling',\n",
       " 11026417: 'machine learning,natural language processing,reputation,wikipedia vandalism detection',\n",
       " 11034828: 'content analysis and indexing,folksonomy,social tagging,web 2.0',\n",
       " 11052867: 'index structures,linked data,rdf querying',\n",
       " 1107868: 'folksonomies,groupme!,miscellaneous,ranking,search,social media',\n",
       " 11081679: 'content redundancy,information extraction,miscellaneous',\n",
       " 11108002: 'attention economics,game theory,quality of online content,user generated content',\n",
       " 11134936: 'collective knowledge,folksonomies',\n",
       " 11134938: 'collective knowledge,folksonomies,social information processing,taxonomies',\n",
       " 11204787: 'content analysis and indexing,data quality,temporal coherence,web archiving',\n",
       " 11212270: 'markov models,ranking prediction',\n",
       " 11227602: 'homophily,miscellaneous,randomization,social influence,social networks',\n",
       " 11275612: 'entity relation discovery,link,web table',\n",
       " 11275613: 'clustering,data record extraction,information extraction',\n",
       " 11277694: 'link recommendation,random walk',\n",
       " 11338040: 'database applications,domain adaptation,feature alignment,learning,opinion mining,sentiment classification,transfer learning',\n",
       " 11338703: 'design tools and techniques,entity matching,linked data,semantic web',\n",
       " 11394824: 'email profiles,reply probability,reply time',\n",
       " 11403481: 'flickr,geolocation,graph mining,photo collections,trip planning',\n",
       " 11418806: 'diversity,information search and retrieval,intents,subtopics',\n",
       " 1142421: 'automatic speech recognition,key-phrase extraction,media archiving,multimedia,natural language processing,probabilistic algorithms,semantic annotation,semantic web,topical segmentation,web search',\n",
       " 11485543: 'evaluation,object retrieval,semantic search',\n",
       " 11498123: 'data mining,time management,web log analysis',\n",
       " 1150148: 'ontologies,semantic web,semantic web challenge groups,user interfaces',\n",
       " 11556066: 'attribute grammar,box model,css,font,graphical user interfaces,html,layout,mobile,multicore,selector',\n",
       " 11570239: 'blocking,broken links,information search and retrieval,instance matching,link integrity,linked data',\n",
       " 11616857: 'boinc,citizen science,crowdsourcing,motivations,online communities,seti@home,volunteer computing',\n",
       " 11619605: 'cross-site scripting,security and protection,web browser,web security',\n",
       " 11620085: 'band-width,caching,developing regions,ethiopia,latency,limited connectivity,personal data,prefetching,sms,www access',\n",
       " 11624616: 'ad auctions,clickthrough rate,conversion rate,miscellaneous,online advertising,pagerank,sponsored search,user behavior models',\n",
       " 11628441: 'graph regularization,learning,miscellaneous,review helpfulness,review quality,social network',\n",
       " 11631340: 'content filtering,content promotion,miscellaneous,social and behavioral sciences,social networks',\n",
       " 11631791: 'click logs,implicit feedback,presentation bias',\n",
       " 11633625: 'html5,javascript,json,object persistence,object-oriented programming,web storage',\n",
       " 11638887: 'information search and retrieval,query ambiguity,result diversity,search diversity',\n",
       " 11639816: 'collaborative filtering,database applications,graphical models,mood,music,recommendations,sessions,social media,taste',\n",
       " 11643476: 'all-pay auction,cheap talk,contest,crowdsourcing,electronic markets,entry deterrence,miscellaneous,reputation',\n",
       " 11657105: 'browser extensions,implicit measures,performance evaluation,web search',\n",
       " 11659004: 'communications applications,communities,endorsement social networks',\n",
       " 1167068: 'components,framework,integration,knowledge representation formalisms and methods,semantic web,virtual environments',\n",
       " 1168781: 'client-server distributed systems,communications applications,ict,mobile computing,mobile phones,paper user interface,rural development',\n",
       " 11701167: 'data extraction,information search and retrieval,web crawler,web query,xml',\n",
       " 11703421: 'chatroulette,obscene content detection,online video chat',\n",
       " 11708130: 'decentralized search,evaluation,folksonomies',\n",
       " 11714006: 'information search and retrieval,keyword querying,semantic web,semantic wikis,visual query languages,wiki',\n",
       " 11749957: 'database selection,information search and retrieval,lowest common ancestor,xml',\n",
       " 11756683: 'boostrapping algorithm,natural language processing,ner,weakly-supervised learning',\n",
       " 11759742: 'information search and retrieval,rated aspect summarization,rating prediction,short comments',\n",
       " 11782911: 'information diffusion,miscellaneous,social contagion,social media',\n",
       " 11785: 'named graphs,semantic web,trust mechanisms,trust policies',\n",
       " 11788753: 'a/b testing,bucket testing,miscellaneous,random walks,social networks',\n",
       " 11792499: 'collaborative filtering,location and activity recommendations',\n",
       " 11796084: 'deep web,source selection,sourcerank,web integration',\n",
       " 11798933: 'breakpoints,css,debugging,dynamic,firebug,html,javascript,web',\n",
       " 11806298: 'effective diameter,neighbourhood function,probabilistic algorithms,probabilistic counters,shortest paths',\n",
       " 11807460: 'drive-by download exploits,efficient web page filtering,invasive software,malicious web page analysis',\n",
       " 11811654: 'general,pricing mechanisms,recommendations,shapley value',\n",
       " 11820735: 'content analysis and indexing,entity extraction,entity ranking,entity search,information search and retrieval,structured database search',\n",
       " 1182832: 'information access,semantic annotation,semantic web,semantic wikis,wikis',\n",
       " 11835370: 'burstiness,multi-tiered systems,performance evaluation,resource allocation',\n",
       " 11836329: 'information extraction,web search',\n",
       " 11839803: 'general,hierarchy,measure,social networks',\n",
       " 11840142: 'analogical reasoning,community question answering',\n",
       " 11840967: 'probabilistic topic model,recommendation,travelogue mining',\n",
       " 11841922: 'pseudo-relevance feedback,query suggestion,random walk',\n",
       " 11848994: 'entity pages,parallel paths,semi-structured data,web structure mining',\n",
       " 11850677: 'click data,image re-ranking,image search,information search and retrieval',\n",
       " 11858468: 'general,online social networks,short urls,twitter',\n",
       " 11859214: 'web information integration,web lists,web mining',\n",
       " 11867634: 'document routing,index compression,index partitioning,inverted index,miscellaneous,online algorithm',\n",
       " 11876537: 'browser extension,computer uses in education,connectivity,hypertext/hypermedia,web acceleration',\n",
       " 11878695: 'nonnumerical algorithms and problems,population estimator,sampling,social network,undirected graph',\n",
       " 11881756: 'arduino,compass,geo-browsing,gps,interaction styles,maps,mobile photo browsing',\n",
       " 11931844: 'fastcgi,php,scripting,sendfile,web server,zero copy',\n",
       " 1194360: 'applications,cache consistency,change characteristics,collected content,object composition,object relationships,server invalidation,web caching',\n",
       " 1194809: 'back navigation,browsing,revisitation,usability study,web trails,web usage',\n",
       " 11981850: 'geo-tagged data,miscellaneous,place recommendation,user-generated content',\n",
       " 12003689: 'geolocation,propagation,social networks',\n",
       " 12008191: 'behavior targeting,contextual advertising,heat diffusion,influence propagation,online advertising,social network analysis',\n",
       " 12092784: 'graph partitioning,indexing,rdf,signature',\n",
       " 12102: 'ontology,semantic annotation of web services,semantic web services,web services discovery,wsdl',\n",
       " 12127726: 'general,wikipedia,wordnet',\n",
       " 12191750: 'general,geo-spatial,knowledge base,multilingual,ontology,temporal,textual',\n",
       " 1225527: 'labeled trees,xml compression and indexing',\n",
       " 1227358: 'annotations,annotea,fluid documents,rdf,web augmentation with open hypermedia,xlink,xpointer',\n",
       " 1229683: 'description logics,owl,structured objects',\n",
       " 1229684: 'knowledge representation formalisms and methods,owl,relational databases,semantic web',\n",
       " 1247804: 'rdf,semantic web,xml',\n",
       " 1247888: 'credibility,invasive software,p2p network,polling protocol,reputation,security and protection',\n",
       " 125048: 'knowledge representation formalisms and methods,provenance,rdf,semantic web,trust',\n",
       " 1267363: 'content distribution networks,data consistency,data dissemination,dynamic data,leases,pullc,push,scalability,world wide web',\n",
       " 1275176: 'query modification,rule extraction,sensitivity analysis,support vector machine',\n",
       " 12995751: 'general,influence,passivity',\n",
       " 13005831: 'query recommender systems,web search effectiveness',\n",
       " 13009292: 'discourse analysis,spoken dialogue systems,xpath',\n",
       " 13023730: 'anonymization,information search and retrieval,personalized search,privacy-preserving data publishing',\n",
       " 13058954: 'context-awareness,information search and retrieval,query auto-completion,query expansion',\n",
       " 13058990: \"hoeffding's inequality,query grammar,query structure,unsupervised query segmentation\",\n",
       " 13059029: 'citation network,general,information search and retrieval,language model,topic evolution,topology',\n",
       " 13059042: 'entity ranking,object ranking,semantic search,structured data',\n",
       " 13059044: 'query log analysis,search results diversification',\n",
       " 13059085: 'graph clustering,graph compression,social networks,web graphs',\n",
       " 13059173: 'automatic image annotation,image-text alignment,relevance re-ranking',\n",
       " 13059270: 'implementation,lsh,lsi,question authoring',\n",
       " 13059318: 'complex event processing,etalis,logic programming,rule systems,semantic web,streams',\n",
       " 13059382: 'auction,dynamics,miscellaneous,peer-to-peer lending service,user behavior',\n",
       " 13059388: 'stigmergy,systems and software,virtual pheromones,web collaboration',\n",
       " 13059401: 'keyword query interpretation,query history',\n",
       " 13059585: 'cyberlockers,file hosting services,rapidshare,web 2.0',\n",
       " 13059663: 'entity tagging,miscellaneous,tag association,tag discovery',\n",
       " 13059781: 'clustering,learning,link analysis,som,visualization,wikipedia',\n",
       " 13059898: 'information search and retrieval,query intent,retrieval model,web entity intent',\n",
       " 13059932: 'attribute discovery,collective information extraction,commerce search,information search and retrieval,integer linear program,natural language processing,weak supervision',\n",
       " 13060051: 'atom,feeds,location-based services,on-line information services,rss',\n",
       " 13060083: 'community detection,game theory,network formation,query incentive networks,social networks,viral marketing',\n",
       " 13060159: 'affinity propagation,k-means,short text,svd',\n",
       " 13060226: 'distributed computing,microcomputations,micropayments,monetization',\n",
       " 13060384: 'general,liquid architectures,programming models,rest,web services',\n",
       " 13060390: 'geographical topics,topic comparison,topic modeling',\n",
       " 13060588: 'deep web,source selection,sourcerank,web integration',\n",
       " 13060608: 'miscellaneous,opinion mining,optimization,sentiment analysis,sentiment lexicon',\n",
       " 13060664: 'content analysis and indexing,semantic analysis,semantic similarity,temporal dynamics,temporal semantics,word relatedness',\n",
       " 13060669: 'graph analysis,large-scale data collection,miscellaneous,network evolution,online social networks,user behavior',\n",
       " 13060679: 'click model,intent bias,intent diversity,search engine,user behavior',\n",
       " 13060796: 'approval voting,contextual opinion,grammatical tree derivation,information search and retrieval,sentence level',\n",
       " 13060825: 'ajax,automatic debugging,javascript,static analysis',\n",
       " 13060918: 'mindmap,miscellaneous,software engineering,user-driven composition,web application',\n",
       " 13061041: 'boosted regression trees,boosting,distributed computing,general,machine learning,parallel computing,ranking,web search',\n",
       " 13061054: 'social network',\n",
       " 13061186: 'adversarial data mining,review spam,spammer group detection',\n",
       " 13061206: 'linked data',\n",
       " 13061384: 'information search and retrieval,mobile queries,mobile search,mobile search query analysis,query categorization,query log analysis,voice queries',\n",
       " 13061550: 'consumer review organization,product aspect hierarchy',\n",
       " 13061552: 'influential mediator,social networks,viral marketing',\n",
       " 13061595: 'data mining,entity extraction,general,query logs',\n",
       " 13061601: 'content delivery networks,information propagation,on-line information services,online social networks',\n",
       " 13061675: 'classification,filtering,microblogging,social tv,twitter',\n",
       " 13061698: 'learning,learning to rank,relevance measures,web search',\n",
       " 13061707: 'social media analytics,social media credibility,twitter',\n",
       " 13061722: 'consumer surplus,information search and retrieval,product search,ranking,text mining,user-generated content,utility theory',\n",
       " 13061737: 'diversity,effectiveness measures,evaluation,measure informativeness,novelty',\n",
       " 13061804: 'a* search,query completion,spelling correction,transformation model',\n",
       " 13061836: 'data integration,enterprise data management,linked data,on-line information services,semantic link discovery',\n",
       " 13061839: 'classification,information diffusion,memes,microblogs,politics,social media,truthy,twitter',\n",
       " 13061918: 'akamai,cdn performance,content delivery networks',\n",
       " 13061924: 'citizen sensing,mobile development application,on-line information services,people-content-network view of social media,semantic social mashup,semantic social web,social development application,social media analysis,social signals,user generated content',\n",
       " 13061972: 'applications,internet usage,interviews,rural networks',\n",
       " 13061989: 'connection clarity,learning,novelty,post-click news recommendation,relatedness,relevance,transition smoothness',\n",
       " 13062025: 'information search and retrieval,metrics,offline prediction,online evaluation,randomization,user engagement',\n",
       " 13062026: 'cross-domain label propagation,heterogeneous knowledge propagation,text corpus and web images,translator function',\n",
       " 13062112: 'iptv,social data mining,social tv,speech interface,twitter,user interfaces',\n",
       " 13062121: 'data fusion,interoperability,object consolidation,object coreference,property combination,self-training',\n",
       " 13062191: 'query mining,query recommendation,query templates',\n",
       " 13062261: 'click model,learning to rank,ranking svm',\n",
       " 13062305: 'latent dirichlet allocation,sentiment mining,stop-words,text and language applications,topic models',\n",
       " 13062337: 'biomedical knowledge base,disease factors,general,ontology,relation extraction',\n",
       " 13062396: 'database applications,functional importance,network metrics',\n",
       " 13062406: \"affiliation networks,milgram's experiment,social networks\",\n",
       " 13062409: 'caching,delay tolerant networking,mobility,network architecture and design,simulation,systems and software',\n",
       " 13062436: 'ad auctions,adecn,electronic commerce,mechanism design,online advertising,sequential screening',\n",
       " 13062801: 'general,http,rest,web architecture,web services',\n",
       " 13062845: 'adaptive approach,correlation of publications,information search and retrieval,ontology-based',\n",
       " 13062852: 'information extraction,opinion mining,social media,twitter',\n",
       " 13062911: 'bengwal,blogs,crf,emotions,expression,holder,svm,syntactic argument structure,topic,tracking',\n",
       " 13062962: 'cloud,cluster,grid,grid service,resource management,trust management',\n",
       " 13063216: 'online shopping,product images,shopping search',\n",
       " 13063239: 'classification,information diffusion,information search and retrieval,microblogs,social media',\n",
       " 13063418: 'detection,drive-by download,invasive software,malware distribution network,signature generation',\n",
       " 13063481: 'arbitrary similarity measure,information storage and retrieval,iterative method,k-nearest neighbor graph',\n",
       " 13063484: 'argument,controversy,evaluation,wikipedia',\n",
       " 13063564: 'graph constraints,indexing,pub/sub',\n",
       " 13063596: 'entity-relation search,index design,information search and retrieval,systems and software,web-scale',\n",
       " 13063612: 'content quality models,information search and retrieval,page structure,search,segmentation',\n",
       " 13063679: 'knowledge bases,semantic search,summarization',\n",
       " 13063697: 'rewriting,sparql query,sparql views',\n",
       " 13063699: 'article generation,domain independent,miscellaneous,template selection,wikipedia',\n",
       " 13063758: 'optimization,personal photos,social networks,summarization',\n",
       " 13063830: 'entity completion,loss of pk-fk,web databases',\n",
       " 13063949: 'structural svm,transfer learning,video summarization',\n",
       " 13063954: 'advertising effectiveness,browsing behavior,causal inference,field experiments,selection bias',\n",
       " 13064078: 'information extraction,minimum description length,structural clustering',\n",
       " 13064160: 'compound splitting,multi-style language model,url segmentation,web scale word breaking,word segmentation',\n",
       " 13064222: 'game theory,generalized nash equilibrium,resource allocation',\n",
       " 13064271: 'methodologies,reasoning,requirements,self-tuning,survivability,value',\n",
       " 13064294: 'exploratory visualization,graphs,matrices,semantic web,user interfaces',\n",
       " 13064296: 'context,information spread,network science,social networks,structure',\n",
       " 13064323: 'diversity,ecommerce,relevance,search,trust,value',\n",
       " 13064431: 'incentive compatibility,individual rationality,nash equilibrium',\n",
       " 13064442: 'concepts,dispersion,education,information search and retrieval,readability,textbooks',\n",
       " 13064535: 'audio search,developing regions,information search and retrieval,literacy,mobile phone,multimedia information systems,spoken web,world wide telecom web',\n",
       " 13064551: 'information extraction,information search and retrieval,tag analysis,video search',\n",
       " 13064602: 'dynamic query suggestion,query context',\n",
       " 13064675: 'business intelligence,general,information extraction,twitter',\n",
       " 13064722: 'concept and image summarization,miscellaneous',\n",
       " 13064730: 'message-based interactions,synchronizability',\n",
       " 13087531: 'relation extraction,relational duality,relational similarity,web mining',\n",
       " 13089030: 'classification,communication networks,communications,information flow,twitter,two-step flow,user/machine systems',\n",
       " 13106832: 'data description languages,peer data management systems,semantic web,xml',\n",
       " 13109: 'adaptive hypermedia,content adaptation,mobile browser',\n",
       " 13152279: 'community detection,graph analysis,wikipedia',\n",
       " 13185533: 'data integration,fact-finders,graph algorithms,trust',\n",
       " 13190545: 'dirichlet processes,learning,online inference,topic models',\n",
       " 13193305: 'blogs,linked data,visualization,wikis',\n",
       " 13240315: 'folksonomies,pragmatics,semantics,tagging,user characteristics',\n",
       " 13251515: 'clustering,miscellaneous,search logs,wrapper induction',\n",
       " 13386371: 'adaptation,business process,interoperability,web services',\n",
       " 13395437: 'homophily,miscellaneous,randomization,social influence,social networks',\n",
       " 13403801: 'content analysis and indexing,flickr,random walk,recommendation,search,tag ranking',\n",
       " 13501869: 'internet,keyword auctions,mechanism design,nonnumerical algorithms and problems',\n",
       " 13502389: 'evaluation,model,search shortcut',\n",
       " 13502390: 'keyword expansion,re-ranking,social network',\n",
       " 13502392: 'benchmark,browser,javascript,replay,web',\n",
       " 13502500: 'blind,screen reader,user models,visually impaired,web 2.0',\n",
       " 13502857: 'buzz,novelty,recommenders,serendipity',\n",
       " 13502912: 'behavioral targeting,click-through rate .,model validation and analysis,online advertising,user segmentation',\n",
       " 13503261: 'binary classifier,miscellaneous,skewed class distribution,stratified sampling,threshold selection,web-page classification',\n",
       " 13504156: 'games with a purpose,human computation,online games,rank aggregation,relevance,search engine',\n",
       " 13505677: 'content analysis and indexing,image retrieval,information network,information search and retrieval,ranking',\n",
       " 13506151: 'collaborative tagging,content analysis and indexing,high-level music descriptors,information search and retrieval,metadata enrichment,theme tag recommendations',\n",
       " 13507253: 'information search and retrieval,intermittent network,low bandwidth,web search,world wide web',\n",
       " 13507543: 'centroid,denormalized cosine measure,inner-class,inter-class,text classification',\n",
       " 13507714: 'graph classification,web 2.0 ir',\n",
       " 13509697: 'e-commerce,e-markets,electronic commerce,marketplaces,selling practices',\n",
       " 13510800: 'data release,differential privacy,query click graph,search logs,security, integrity, and protection',\n",
       " 13512950: 'adaptive bidding,concentration bounds,display advertising,general,guaranteed delivery,guess-then-double algorithms',\n",
       " 13513557: 'natural language processing,relational similarity,web mining',\n",
       " 13514543: 'automated reasoning,general,information extraction,ontology',\n",
       " 13514784: 'discovery,general,loosely coupled systems,rest,service management',\n",
       " 13515810: '2.0,communications applications,search,social,social networks,systems and software,user content,web2.0',\n",
       " 13516419: 'web-based personal health record',\n",
       " 13517074: 'markov logic,probabilistic reasoning,semantic web,uncertainty, fuzzy, and probabilistic reasoning',\n",
       " 13517135: 'automatic lexicon compilation,information search and retrieval,mining transliteration pairs,natural language processing,person name',\n",
       " 13518145: 'ad auctions,analysis of algorithms and problem complexity,bid optimization,optimal bidding,sponsored search',\n",
       " 13518419: 'advertising,bayesian inference,collaborative filtering,machine learning,online services,recommender system',\n",
       " 13518634: 'geographical relevance,human computation,image annotation,image search',\n",
       " 13518670: 'categorization,language model,link analysis,machine learning,search engines,web mining',\n",
       " 13519640: 'click through rate,cpc,ctr,gbdt,gradient boosted decision trees,information search and retrieval,jobs,learning,linear regression,prediction,treenet',\n",
       " 13520597: 'concept networks,cross-reference,information search and retrieval,knowledge based search,query categorization,unsupervised learning,web mining',\n",
       " 13520879: 'distributed computing,dsn,mobile communication,p2p',\n",
       " 13521214: 'keyword extraction,miscellaneous,online advertising,social snippets',\n",
       " 13521570: 'general,link prediction,negative edge,slashdot zoo,social network',\n",
       " 13522059: 'derivatives,incomplete markets,services mashups,wavelets,web services',\n",
       " 13522731: 'query relaxation,query results ranking,top-k.,web database',\n",
       " 13523265: 'image ranking,image summarization,information search and retrieval,pagerank',\n",
       " 13525236: 'image spam,local invariant features,pyramid match kernel',\n",
       " 13526048: 'information retrieval,ranking,topic initiator,web mining',\n",
       " 13526508: 'crawling,document age,freshness,latency,metrics,monitoring,search engine',\n",
       " 13526671: 'business modeling,business process modeling language,rdf,semantic web,software',\n",
       " 13526814: 'flickr,geotag,information search and retrieval,object recognition,representative images',\n",
       " 13526913: 'co-browsing,collaboration,collaborative computing,shared browsing,web4ce',\n",
       " 13527964: 'communication,drag and drop,portal,start page,widget',\n",
       " 13528329: 'e-fcm algorithm,instance selection,security and protection,tcm-knn algorithm,web server anomaly detection',\n",
       " 13528523: 'information search and retrieval,multi-view clustering,reranking,web image retrieval',\n",
       " 13528887: 'crawling,metrics,quality,search engines,sitemaps',\n",
       " 13529859: 'data extraction,form filling,general,web applications',\n",
       " 13530223: 'blogs,burst analysis,hot topics,information retrieval,keyword correlation,miscellaneous,social media,text mining',\n",
       " 13530633: 'compression,double-pareto,index size,information search and retrieval,power law',\n",
       " 13530974: 'information search and retrieval,reuse detection,weblogs',\n",
       " 13531284: 'context,semantics,similarity,web services',\n",
       " 13532199: 'faceted search,hybrid query,information search and retrieval,inverted index,ranking',\n",
       " 13533362: 'content recommendation,ctr positional correlation,on-line information services',\n",
       " 13533459: 'benchmark,openrulebench,performance evaluation,rule systems,semantic web',\n",
       " 13533717: 'graph mining,map/reduce,parallel data mining,web usage mining,web user modeling',\n",
       " 13533950: 'general,mash-up,pipes,rdf,semantic web',\n",
       " 13534577: 'blog mining,crosslanguage,miscellaneous,trend visualisation',\n",
       " 13534613: 'betweenness centrality,clustering coefficient,degree distribution,diameter,shortest path',\n",
       " 13535450: 'information search and retrieval,search engine,soft 404,spam,url redirection',\n",
       " 13536580: 'caching,content-match,miscellaneous,nearest-neighbor',\n",
       " 13536686: 'browser,client-side programming,css,dom,events,general,html,javascript,language classifications,mash-up,script,scripting,stylesheets,xhtml,xml,xquery',\n",
       " 13536936: 'context,semantic portal,semantic web services,soa',\n",
       " 13537427: 'information search and retrieval,query log,search result ranking,social tagging',\n",
       " 13537435: 'blog,community,content hole search,miscellaneous,sns',\n",
       " 13538141: 'entity extraction,similarity measure,synonym generation,web search',\n",
       " 13538176: 'context,machine learning,process models,user interface management systems,web transaction',\n",
       " 13538488: 'information search and retrieval,learning,long tail theory,rare item detection,rareness measure',\n",
       " 13539372: 'harmonic function,random walk,search,semi-supervised learning',\n",
       " 13540032: 'online community,rating,social networks,trust',\n",
       " 13540651: 'hidden web,systems and software,web data extraction,web form extraction,web form mapping',\n",
       " 13542825: 'content quality,delicious,social bookmarking',\n",
       " 13543051: 'coauthorship networks,collaboration social networks,computer science,miscellaneous',\n",
       " 13543053: 'content analysis and indexing,flickr,image clustering,on-line information services,visual diversity',\n",
       " 13543554: 'clustering,general,name disambiguation,tagging',\n",
       " 13543822: 'host graph,nation graph,web graph,web structure',\n",
       " 13544213: 'learning,social network,staring people discovery',\n",
       " 13544630: 'genetic algorithm.,software reliability growth model,weighted combination',\n",
       " 13545447: 'anomaly detection,clustering,intrusion detection,security and protection',\n",
       " 13601457: 'name alias extraction,semantic web,web mining',\n",
       " 13621384: 'algorithm,graph theory,hypertext/hypermedia,learning',\n",
       " 13638975: 'cascades,competition,epidemics,winner-takes-all',\n",
       " 13714941: 'authenticity,merkle hash tree,web content distribution',\n",
       " 13753027: 'bayesian generative models,hierarchical clustering,learning,stochastic block models,topic models',\n",
       " 13756266: 'clustering,fact finding,trust',\n",
       " 13778833: 'computer uses in education,document classification,focused crawling,hypertext/hypermedia,offline,web portal',\n",
       " 13785303: 'communications applications,facebook,on-line information services,online social networks,privacy,social applications',\n",
       " 13788626: 'web robot population',\n",
       " 13788838: 'fake review detection,group opinion spam,opinion spam,social and behavioral sciences',\n",
       " 13801324: 'collusionrank,link farming,pagerank,security and protection,spam,twitter',\n",
       " 13819642: 'models,quality,wikipedia',\n",
       " 13820155: 'approximation algorithms,facility dispersion,information search and retrieval,novelty,relevance',\n",
       " 13833456: 'generative models,social rating networks,temporal dynamics,user behavioral modeling',\n",
       " 13875907: 'bittorrent,distributed systems,incentives,peer-to-peer,private communities,resale value,share ratio enforcement',\n",
       " 13896241: '3g networks,periodic transfers,periodicity detection,radio resource optimization,rrc state machine,smartphone applications',\n",
       " 13904998: 'hashing,information search and retrieval,learning to rank,nearest neighbor search',\n",
       " 13982483: 'auto-completions,data mining,estimation,,impressionrank,information search and retrieval,popular keyword extraction,search engines,suggestions',\n",
       " 14005107: 'web measurement,web mining,web site classification',\n",
       " 14005258: 'browsing,human computation,information networks,wikipedia,wikispeedia',\n",
       " 14007948: 'assisted browsing,information search and retrieval,link suggestion,quick links,tail sites,website clustering',\n",
       " 14026193: 'editors,expertise,web usage,wikipedia',\n",
       " 14027370: 'a/b testing,bucket testing,network bucket testing,nonnumerical algorithms and problems,social networks',\n",
       " 14045611: 'scheduling,task assignment,team formation',\n",
       " 14122983: 'content analysis and indexing,general,information,information search and retrieval,metro maps,summarization',\n",
       " 14123171: 'actions,active objects,entity-centric search,query log mining,web search',\n",
       " 14128558: 'activity prediction,miscellaneous,proximity,social media,social networks',\n",
       " 14146914: 'approximation algorithms,budget allocation,general,influence models',\n",
       " 14149355: 'nonnumerical algorithms and problems,revenue,sponsored search auctions',\n",
       " 14169498: 'information search and retrieval,recommender systems,social tagging',\n",
       " 14173486: 'domain adaptation,relation extraction,web mining',\n",
       " 14185350: 'applications,applications,behavioral analysis,predictive behavioral models',\n",
       " 14186402: 'betweenness centrality,centrality approximation,closeness centrality,graph theory,large networks,shortest paths',\n",
       " 14200731: 'bidder optimality,competitive equilibrium,envy freeness,expressiveness,general auction mechanism,gsp,nonnumerical algorithms and problems,vcg',\n",
       " 14209920: 'miscellaneous,music information retrieval,recommender systems',\n",
       " 14237951: 'social honeypots,social media,spam',\n",
       " 14249: 'bandwidth optimization,information storage and retrieval,parallel web crawlers',\n",
       " 14255423: 'crowdsourcing,crowdturfing,spam,sybils',\n",
       " 14265822: 'hashing,similarity estimation',\n",
       " 14316716: 'contest design,crowdsourcing,game theory,mechanism design,social computing,user generated content',\n",
       " 14316718: 'game theory,implementation,online q&amp;a forums,social computing,user generated content',\n",
       " 14322308: 'collaborative filtering,machine learning,social networks',\n",
       " 14347434: 'archiving,crawling,extraction,hypertext/hypermedia,web application,xpath',\n",
       " 14348154: 'cross-lingual information retrieval,information search and retrieval,machine translation,text classification,web search',\n",
       " 14353464: 'clickthrough data,information search and retrieval,latent semantic analysis,multilingual ir,topic models,translation model,web search',\n",
       " 14371347: 'heterogeneous databases,information integration,probabilistic model,semi-markov',\n",
       " 14372945: 'ad exchanges,adaptive bidding,applications,bidding agents,general',\n",
       " 14373644: 'crowdsourcing,human computation,max algorithms,miscellaneous,miscellaneous,plurality voting,vote aggregation,worker models',\n",
       " 14373837: 'causality,social influence,tie strength',\n",
       " 14374135: 'ad-hoc entity identification,entity extraction,information search and retrieval,mentionrank,named entity disambiguation,targeted disambiguation',\n",
       " 14375309: 'graph theory,link analysis,link prediction,network analysis',\n",
       " 14375496: 'information search and retrieval,partitioned multi-indexing,real time,scalable,social search',\n",
       " 14375712: 'applications and expert systems,future prediction,news prediction,web knowledge for future prediction',\n",
       " 14375718: 'css,debugging,style sheets,web development',\n",
       " 14376091: 'content analysis and indexing,general,geolocation,graphical model,language model,latent variable inference,topic models,twitter,user profiling',\n",
       " 14376543: 'causality,entropy,point processes,prediction,social networks,spam',\n",
       " 14377120: 'accommodation,coordination,dependence,language,linguistic convergence,linguistic style,online communities,power,relations,social and behavioral sciences,social status',\n",
       " 14377272: 'crowd sourcing,ecology,photo collections,social media',\n",
       " 14377562: 'multi-objective optimization,on-line information services,qos-aware service composition,robustness,service computing',\n",
       " 14380234: 'automatic question answering,community-based question answering,question-answering systems',\n",
       " 14380564: 'information search and retrieval,query log mining,query suggestion,search user interface,web search',\n",
       " 14382235: 'browsing behavior,general,markov chains,user models',\n",
       " 14383928: 'applications,electronic publishing,face recognition,machine learning,text analysis,web search',\n",
       " 14385094: 'community detection,distance metric learning,incomplete information networks',\n",
       " 14386517: 'linked data,navigation,semantic web,systems and software,web of data',\n",
       " 14386992: 'bag semantics,counting complexity,property paths,sparql 1.1',\n",
       " 14387519: 'learning to rank,relevance and freshness modeling,temporal features',\n",
       " 14387989: 'active learning,hodge decomposition,miscellaneous,multi-objective ranking',\n",
       " 14388642: 'graphical models,intent analysis,query templates',\n",
       " 14388703: 'behavior analysis,social networks,social signals,social ties',\n",
       " 14388913: 'crowdsourcing,data description languages,databases,games,probabilistic',\n",
       " 14388926: 'distributed algorithms,graph querying,graph simulation',\n",
       " 14389008: 'mapreduce,scalability,topic models',\n",
       " 14389159: 'betweenness centrality,update algorithm',\n",
       " 14389185: 'data translation,direct mapping,owl,rdb2rdf,rdf,relational databases,semantic web,sparql,sql',\n",
       " 14389220: 'credit networks,electronic commerce,empirical game-theoretic simulations,strategic network formation,trust',\n",
       " 14389382: 'crowd sourcing,exploratory search,information seeking,multi-domain search,search engine,search service,social network',\n",
       " 14392043: 'automation,cloud computing,computer-aided software engineering,criteria,decision support,decision support,factors,migration process,selection algorithm,service selection',\n",
       " 14392151: 'cloud,network,optimization,qos,service composition,web services',\n",
       " 14394291: 'caching,content integrity,security and protection,web security',\n",
       " 14394643: 'content spread,recommendation,social networks',\n",
       " 14395405: 'image search reranking,information search and retrieval,multimedia information systems,opinion mining,sentiment analysis,user comments,visual aesthetics modeling',\n",
       " 14395695: 'post-click search behavior,relevance estimation',\n",
       " 14395704: '3d graphics,3d web,hypertext,user interface,user interfaces',\n",
       " 14396486: 'entity disambiguation,miscellaneous,topic models',\n",
       " 14397937: 'fraudulence detection,learning,multiple instance learning,online auction,online feature selection,online modeling',\n",
       " 14426843: 'd2rq,linked data,rdb-to-rdf mapping,rdf,relational database,semantic web,sparql,sparql/update',\n",
       " 14435710: 'entity linking,fact integration,information search and retrieval,knowledge base,semantic knowledge,wikipedia',\n",
       " 14445422: 'information search and retrieval,log analysis,search log mining,task evaluation,task trail',\n",
       " 14445546: 'android,navigation,path planning,route search',\n",
       " 14449751: 'collaborative filtering,learning,meta search,preference aggregation',\n",
       " 14453157: 'diversification,evaluation,information search and retrieval,intents,metrics,novelty,redundancy',\n",
       " 14453752: 'natural language patterns,question answering,semantic web,sparql',\n",
       " 14454508: 'crowdsourcing,entity linking,linked data,probabilistic reasoning',\n",
       " 14475693: 'collaborative network,information flow,social routing',\n",
       " 1454933: 'model-theoretic semantics,representation,semantic web',\n",
       " 1605069: 'distributed enforcement,distributed systems,law governed interaction,online auctions,software architectures',\n",
       " 163229: 'attention,collections,information gathering and management,transclusions,web-based interaction design',\n",
       " 16334: 'information search and retrieval',\n",
       " 16381: 'automated semantic tagging,data mining,information retrieval,large text datasets,software architectures,text analytics',\n",
       " 16387: 'ontologies,semantic web,web services',\n",
       " 1659738: 'document object model,focused crawling,hypertext/hypermedia,reinforcement learning',\n",
       " 16706: 'identifier resolution,mobile computing,nomadic computing,physical hyperlinks,ubiquitous computing',\n",
       " 173979: 'answer extraction,answer selection,information retrieval,miscellaneous,natural language processing,query modulation,question answering,search engines',\n",
       " 178204: 'availability,data replication,distributed objects,edge services,wide area networks',\n",
       " 178963: 'learning to tag,multi-modality rankboost,social tagging,tag recommendation',\n",
       " 1810492: 'agents,awareness,collaboration,context,critical mass,opportunistic communication',\n",
       " 183: 'distributed eigenvector computation,peer-to-peer,reputation',\n",
       " 1833006: 'automatic tuning,gradient method,importance sampling,simulated annealing,system configuration',\n",
       " 1833066: 'digital ink,freehand writing,inkml,random access',\n",
       " 1839820: 'daml+oil,daml-s,semantic web,tcoz',\n",
       " 1840610: 'general,streaming media,workload characterization',\n",
       " 1845167: 'gnutella,peer-to-peer,search engine,world wide web',\n",
       " 1848435: 'document structure,rdf,semantics,style,xhtml+smil,xslt',\n",
       " 1880971: 'document and text editing,information extraction,metadata,natural language processing,semantic annotation,semantic web',\n",
       " 1888132: 'b2b,e-business,e-catalog,e-procurement,pricing,xml',\n",
       " 204579: \"aliasing,applications,cache hierarchies,caching,dtd,duplicate suppression,duplicate transfer detection,http,hypertext transfer protocol,performance analysis,redundant transfers,resource modification,world wide web,www,zipf's law\",\n",
       " 204586: 'aliasing,applications,caching,duplicate suppression,dynamic content,http,hypertext transfer protocol,privacy,proxy,redundant transfers,resource modification,scalability,world wide web,www',\n",
       " 21580: 'jena,owl,rdf,rdql,semantic web,software architectures',\n",
       " 21587: 'generation,grammar,rdf,universal algebra,unparsing,xml',\n",
       " 217637: 'adaptation,learning repositories,ontologies,p2p,personalization,standards,web services',\n",
       " 230498: 'document classification,opinion mining',\n",
       " 238458: 'password security,website user authentication',\n",
       " 239996: 'database applications,expressions and their representation,variable binding dependency,xquery containment',\n",
       " 246813: 'knowledge representation formalisms and methods,ontology,semantic web,uncertainty',\n",
       " 247234: 'index sizes,information search and retrieval,search engines,size of the web',\n",
       " 251906: 'accessibility,automatic presentation generation,digital talking books,ditabbu,document preparation,hypermedia,hypertext/hypermedia,multimodality,user interfaces',\n",
       " 251909: 'accessibility,semantic web,visual impairment,web engineering',\n",
       " 254542: 'bioinformatics,domain ontology,ontology evaluation,ontology learning,owl-s,semantic web,web services',\n",
       " 254899: 'information flow,invasive software,noninterference,program security,security vulnerabilities,type systems,unauthorized access,web application security',\n",
       " 259989: 'mixed-initiative interaction,out-of-turn interaction,program transformations,web dialogs',\n",
       " 259993: 'dynamic service coordination,dynamic web content,patterns,scalable component-based software systems,semantic interoperability,web information management systems',\n",
       " 275063: 'data streams,information search and retrieval,link-analysis,probabilistic algorithms,scalability,similarity search',\n",
       " 278400: 'appel,hippocratic databases,p3p,preference,privacy-aware data management,xpath,xpref',\n",
       " 281466: 'document preparation,java,language constructs and features,language design,xml',\n",
       " 281471: 'automata,sax,xml parsers',\n",
       " 285729: 'link structure,pagerank,personalized search,search,search in context,web graph'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-yahoo",
   "metadata": {},
   "source": [
    "### Generating dataframe for the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "modified-farmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_no</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183</td>\n",
       "      <td>The Eigentrust algorithm for reputation manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10119</td>\n",
       "      <td>Simulation , verification and automated compos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11785</td>\n",
       "      <td>Using context - and content-based trust polici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12102</td>\n",
       "      <td>Meteor-s web service annotation framework The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13109</td>\n",
       "      <td>Detecting web page structure for adaptive view...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>14449751</td>\n",
       "      <td>A flexible generative model for preference agg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>14453157</td>\n",
       "      <td>Evaluation with informational and navigational...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>14453752</td>\n",
       "      <td>Template-based question answering over RDF dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>14454508</td>\n",
       "      <td>ZenCrowd : leveraging probabilistic reasoning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>14475693</td>\n",
       "      <td>Understanding task-driven information flow in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Doc_no                                           Abstract\n",
       "0         183  The Eigentrust algorithm for reputation manage...\n",
       "1       10119  Simulation , verification and automated compos...\n",
       "2       11785  Using context - and content-based trust polici...\n",
       "3       12102  Meteor-s web service annotation framework The ...\n",
       "4       13109  Detecting web page structure for adaptive view...\n",
       "..        ...                                                ...\n",
       "495  14449751  A flexible generative model for preference agg...\n",
       "496  14453157  Evaluation with informational and navigational...\n",
       "497  14453752  Template-based question answering over RDF dat...\n",
       "498  14454508  ZenCrowd : leveraging probabilistic reasoning ...\n",
       "499  14475693  Understanding task-driven information flow in ...\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df = pd.DataFrame(text_data.items(), columns=['Doc_no','Abstract'])\n",
    "texts_df.sort_values(by=['Doc_no'], inplace=True)\n",
    "texts_df.reset_index(drop=True, inplace=True)\n",
    "texts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-thousand",
   "metadata": {},
   "source": [
    "### Generating dataframe for the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hundred-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_no</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183</td>\n",
       "      <td>distributed eigenvector computation,peer-to-pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10119</td>\n",
       "      <td>automated reasoning,daml,distributed systems,o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11785</td>\n",
       "      <td>named graphs,semantic web,trust mechanisms,tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12102</td>\n",
       "      <td>ontology,semantic annotation of web services,s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13109</td>\n",
       "      <td>adaptive hypermedia,content adaptation,mobile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>14449751</td>\n",
       "      <td>collaborative filtering,learning,meta search,p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>14453157</td>\n",
       "      <td>diversification,evaluation,information search ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>14453752</td>\n",
       "      <td>natural language patterns,question answering,s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>14454508</td>\n",
       "      <td>crowdsourcing,entity linking,linked data,proba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>14475693</td>\n",
       "      <td>collaborative network,information flow,social ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Doc_no                                           Keywords\n",
       "0         183  distributed eigenvector computation,peer-to-pe...\n",
       "1       10119  automated reasoning,daml,distributed systems,o...\n",
       "2       11785  named graphs,semantic web,trust mechanisms,tru...\n",
       "3       12102  ontology,semantic annotation of web services,s...\n",
       "4       13109  adaptive hypermedia,content adaptation,mobile ...\n",
       "..        ...                                                ...\n",
       "495  14449751  collaborative filtering,learning,meta search,p...\n",
       "496  14453157  diversification,evaluation,information search ...\n",
       "497  14453752  natural language patterns,question answering,s...\n",
       "498  14454508  crowdsourcing,entity linking,linked data,proba...\n",
       "499  14475693  collaborative network,information flow,social ...\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_df = pd.DataFrame(keywords_data.items(), columns=['Doc_no','Keywords'])\n",
    "keys_df.sort_values(by=['Doc_no'], inplace=True)\n",
    "keys_df.reset_index(drop=True, inplace=True)\n",
    "keys_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-persian",
   "metadata": {},
   "source": [
    "### Merging two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pediatric-liver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_no</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>183</td>\n",
       "      <td>The Eigentrust algorithm for reputation manage...</td>\n",
       "      <td>distributed eigenvector computation,peer-to-pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10119</td>\n",
       "      <td>Simulation , verification and automated compos...</td>\n",
       "      <td>automated reasoning,daml,distributed systems,o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11785</td>\n",
       "      <td>Using context - and content-based trust polici...</td>\n",
       "      <td>named graphs,semantic web,trust mechanisms,tru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12102</td>\n",
       "      <td>Meteor-s web service annotation framework The ...</td>\n",
       "      <td>ontology,semantic annotation of web services,s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13109</td>\n",
       "      <td>Detecting web page structure for adaptive view...</td>\n",
       "      <td>adaptive hypermedia,content adaptation,mobile ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>14449751</td>\n",
       "      <td>A flexible generative model for preference agg...</td>\n",
       "      <td>collaborative filtering,learning,meta search,p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>14453157</td>\n",
       "      <td>Evaluation with informational and navigational...</td>\n",
       "      <td>diversification,evaluation,information search ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>14453752</td>\n",
       "      <td>Template-based question answering over RDF dat...</td>\n",
       "      <td>natural language patterns,question answering,s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>14454508</td>\n",
       "      <td>ZenCrowd : leveraging probabilistic reasoning ...</td>\n",
       "      <td>crowdsourcing,entity linking,linked data,proba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>14475693</td>\n",
       "      <td>Understanding task-driven information flow in ...</td>\n",
       "      <td>collaborative network,information flow,social ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Doc_no                                           Abstract  \\\n",
       "0         183  The Eigentrust algorithm for reputation manage...   \n",
       "1       10119  Simulation , verification and automated compos...   \n",
       "2       11785  Using context - and content-based trust polici...   \n",
       "3       12102  Meteor-s web service annotation framework The ...   \n",
       "4       13109  Detecting web page structure for adaptive view...   \n",
       "..        ...                                                ...   \n",
       "495  14449751  A flexible generative model for preference agg...   \n",
       "496  14453157  Evaluation with informational and navigational...   \n",
       "497  14453752  Template-based question answering over RDF dat...   \n",
       "498  14454508  ZenCrowd : leveraging probabilistic reasoning ...   \n",
       "499  14475693  Understanding task-driven information flow in ...   \n",
       "\n",
       "                                              Keywords  \n",
       "0    distributed eigenvector computation,peer-to-pe...  \n",
       "1    automated reasoning,daml,distributed systems,o...  \n",
       "2    named graphs,semantic web,trust mechanisms,tru...  \n",
       "3    ontology,semantic annotation of web services,s...  \n",
       "4    adaptive hypermedia,content adaptation,mobile ...  \n",
       "..                                                 ...  \n",
       "495  collaborative filtering,learning,meta search,p...  \n",
       "496  diversification,evaluation,information search ...  \n",
       "497  natural language patterns,question answering,s...  \n",
       "498  crowdsourcing,entity linking,linked data,proba...  \n",
       "499  collaborative network,information flow,social ...  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.merge(texts_df, keys_df, on=\"Doc_no\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tutorial-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(data, '../../Data/WWW.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
