{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incoming-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential modules for the algorithms\n",
    "from rake_nltk import Rake\n",
    "import yake\n",
    "from keybert import KeyBERT\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from numpy import array, log\n",
    "\n",
    "# data processing\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-third",
   "metadata": {},
   "source": [
    "### Importing the dataset\n",
    "- The dataset is pre-processed from the \"Pre-Processing-SemEval2017.ipynb\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "played-massage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_no</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>poor oxidation behavior major barrier increase...</td>\n",
       "      <td>alloys,alloys with substantially improved oxid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>key problem inspector access data small inspec...</td>\n",
       "      <td>block,build a model of the smallest thicknesse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>situ oxidation experiment carried mm diameter ...</td>\n",
       "      <td>3mm diameter discs,accelerating voltage of 30k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>study outline trial transient response analysi...</td>\n",
       "      <td>assessment of the corrosion condition,assess t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104</td>\n",
       "      <td>result type oxidation test combined study tabl...</td>\n",
       "      <td>adjusted to accommodate buoyancy effects,alumi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Doc_no                                           Abstract  \\\n",
       "0     100  poor oxidation behavior major barrier increase...   \n",
       "1     101  key problem inspector access data small inspec...   \n",
       "2     102  situ oxidation experiment carried mm diameter ...   \n",
       "3     103  study outline trial transient response analysi...   \n",
       "4     104  result type oxidation test combined study tabl...   \n",
       "\n",
       "                                            Keywords  \n",
       "0  alloys,alloys with substantially improved oxid...  \n",
       "1  block,build a model of the smallest thicknesse...  \n",
       "2  3mm diameter discs,accelerating voltage of 30k...  \n",
       "3  assessment of the corrosion condition,assess t...  \n",
       "4  adjusted to accommodate buoyancy effects,alumi...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('Data/Processed_SemEval.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "durable-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = data['Abstract'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecological-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_no = data['Doc_no'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-situation",
   "metadata": {},
   "source": [
    "### RAKE Algorithm\n",
    "- The following code block uses the rake_nltk module to extract ketwords from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "italian-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the text from the datafame and returns keywords\n",
    "def get_rake_keywords(docs):\n",
    "    r = Rake(min_length=2, max_length=3)\n",
    "    sr_no = 1\n",
    "    results = []\n",
    "    for row in docs:\n",
    "        #print(f'Processing : {sr_no}')\n",
    "        r.extract_keywords_from_text(row)\n",
    "        keywords = r.get_ranked_phrases()\n",
    "        keywords = keywords[:7]\n",
    "        keywords = \",\".join(keywords)\n",
    "        results.append(keywords)\n",
    "        sr_no += 1\n",
    "    rake_df = pd.DataFrame(zip(docs,results),columns=['Abstract','Extracted_Keywords'])\n",
    "    rake_df.insert(loc=0, column=\"Doc_no\", value=doc_no)\n",
    "    return rake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "advised-holmes",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (493) does not match length of index (500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c784231a8ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrake_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/Hulth2003_data.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrake_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrake_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrake_pred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rake_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrake_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-e78272751367>\u001b[0m in \u001b[0;36mget_rake_keywords\u001b[0;34m(docs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msr_no\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mrake_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abstract'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Extracted_Keywords'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mrake_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Doc_no\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrake_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   3760\u001b[0m             )\n\u001b[1;32m   3761\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3762\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3763\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_duplicates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3899\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \"\"\"\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    752\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (493) does not match length of index (500)"
     ]
    }
   ],
   "source": [
    "# Since Rake algorithm did not work well with pre-processed data, we applied the abstract from the \n",
    "rake_df = pd.read_pickle('Data/SemEval_data.pkl')\n",
    "rake_docs = rake_df['Abstract'].tolist()\n",
    "rake_pred_df = get_rake_keywords(rake_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(rake_pred_df, 'hulth_results/rake_pred_df.pkl')\n",
    "rake_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-pixel",
   "metadata": {},
   "source": [
    "### YAKE Algorithm\n",
    "- The following code block uses the yake module to get keywords from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in the text from the datafame and returns keywords\n",
    "def get_yake_keywords(docs):\n",
    "    kw_extractor = yake.KeywordExtractor(n=2, top=7)\n",
    "    results = []\n",
    "    for row in docs:\n",
    "        keywords = kw_extractor.extract_keywords(row)\n",
    "        keys = []\n",
    "        for val in keywords:\n",
    "            keys.append(val[0])\n",
    "        results.append(\",\".join(keys))\n",
    "    yake_df = pd.DataFrame(zip(docs,results),columns=['Abstract','Extracted_Keywords'])\n",
    "    yake_df.insert(loc=0, column=\"Doc_no\", value=doc_no)\n",
    "    return yake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "yake_pred_df = get_yake_keywords(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(yake_pred_df, 'hulth_results/yake_pred_df.pkl')\n",
    "yake_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-italian",
   "metadata": {},
   "source": [
    "## TF-IDF \n",
    "- The following code block uses the TF-IDF method from the Scikit-Learn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_keywords(docs):\n",
    "    results = []\n",
    "    for row in docs:\n",
    "        vectorizer = CountVectorizer(ngram_range = (2,3))\n",
    "        X1 = vectorizer.fit_transform([row]) \n",
    "        features = (vectorizer.get_feature_names())\n",
    "        vectorizer = TfidfVectorizer(ngram_range = (2,3))\n",
    "        X2 = vectorizer.fit_transform([row])\n",
    "        scores = (X2.toarray())\n",
    "        sums = X2.sum(axis = 0)\n",
    "        data1 = []\n",
    "        for col, term in enumerate(features):\n",
    "            data1.append( (term, sums[0,col] ))\n",
    "        ranking = pd.DataFrame(data1, columns = ['term','rank'])\n",
    "        words = (ranking.sort_values('rank', ascending = False))\n",
    "        ex_keywords = []\n",
    "        for term in words['term'][:7]:\n",
    "            ex_keywords.append(term)\n",
    "        results.append(\",\".join(ex_keywords))\n",
    "    tf_idf_df = pd.DataFrame(zip(docs,results),columns=['Abstract','Extracted_Keywords'])\n",
    "    tf_idf_df.insert(loc=0, column=\"Doc_no\", value=doc_no)\n",
    "    return tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_pred_df = get_tf_idf_keywords(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(tf_idf_pred_df, 'hulth_results/tf_idf_pred_df.pkl')\n",
    "tf_idf_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-armstrong",
   "metadata": {},
   "source": [
    "### KeyBert Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keybert_keywords(docs):\n",
    "    model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "    results = []\n",
    "    doc_no = 1\n",
    "    for row in docs:\n",
    "        keywords = model.extract_keywords(row, keyphrase_ngram_range=(2, 3), stop_words=None)\n",
    "        temp_keys = []\n",
    "        for val in keywords:\n",
    "            temp_keys.append(val[0])\n",
    "        results.append(\",\".join(temp_keys))\n",
    "        doc_no += 1\n",
    "    key_bert_df = pd.DataFrame(zip(docs,results),columns=['Abstract','Extracted_Keywords'])\n",
    "    key_bert_df.insert(loc=0, column=\"Doc_no\", value=doc_no)\n",
    "    return key_bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_bert_pred_df = get_keybert_keywords(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(key_bert_pred_df, 'hulth_results/key_bert_pred_df.pkl')\n",
    "key_bert_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-mortality",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_keywords(docs):\n",
    "    results = []\n",
    "    for row in docs:\n",
    "        vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "        tf = vectorizer.fit_transform([row])\n",
    "        tf_feature_names = vectorizer.get_feature_names()\n",
    "        number_of_topics = 1\n",
    "        model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
    "        model.fit(tf)\n",
    "        no_top_words = 7\n",
    "        result_dict = display_topics(model, tf_feature_names, no_top_words)\n",
    "        kw = result_dict['Topic 0 words']\n",
    "        results.append(\",\".join(kw))\n",
    "    lda = pd.DataFrame(zip(docs,results),columns=['Abstract','Extracted_Keywords'])\n",
    "    lda.insert(loc=0, column=\"Doc_no\", value=doc_no)\n",
    "    return lda  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_pred_df = get_lda_keywords(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(lda_pred_df, 'hulth_results/lda_pred_df.pkl')\n",
    "lda_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-might",
   "metadata": {},
   "source": [
    "### PositionRank Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textrank_keywords(docs):\n",
    "    results = []\n",
    "    for row in docs:\n",
    "        # load a spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        # add PyTextRank to the spaCy pipeline\n",
    "        nlp.add_pipe(\"positionrank\") #positionrank\n",
    "        doc = nlp(row)\n",
    "\n",
    "        # examine the top-ranked phrases in the document\n",
    "        limit = len(doc._.phrases)\n",
    "        kw = []\n",
    "        if(limit > 4):\n",
    "            for phrase in doc._.phrases:\n",
    "                if(len(phrase.text.split()) <= 4 and len(phrase.text.split()) > 1):\n",
    "                    kw.append(phrase.text)\n",
    "            kw = \",\".join(kw)\n",
    "            results.append(kw)\n",
    "        else:\n",
    "            for phrase in doc._.phrases:\n",
    "                kw.append(phrase.text)\n",
    "            kw = \",\".join(kw)\n",
    "            results.append(kw)\n",
    "    pos_rank_df = pd.DataFrame(zip(docs,results),columns=['Abstract','Extracted_Keywords'])\n",
    "    pos_rank_df.insert(loc=0, column=\"Doc_no\", value=doc_no)\n",
    "    return pos_rank_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rank_pred_df = get_textrank_keywords(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(pos_rank_pred_df, 'hulth_results/pos_rank_pred_df.pkl')\n",
    "pos_rank_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-consciousness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
